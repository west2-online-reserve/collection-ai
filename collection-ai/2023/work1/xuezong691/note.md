# 这是学习pytorch实现神经网络识别MNIST的笔记

>总的来说，我先是学习了一下基本的神经网络框架，以及了解了一些其他的网络如生成对抗，卷积，resnet等，**之所以是了解是因为暂时还没有特别理解** :smiley:，再跟着一篇csdn的博客在虚拟环境miniconda里搭建了一遍模型，学习过程一直穿插着学习李沐老师的深度学习课程。

接下来我来说说我自己对神经网络基本框架的理解

- 数据预处理\输入层：将数据转化为向量形式，以便其稍后构建为一个可计算模型。
- 计算层：全连接层，卷积层等矩阵变换或计算输入的数据，是计算模型的一部分。
- 激活层：由于像全连接层这样的计算就相当于一种初等矩阵变换，所以添加激活层如relu使这一次的计算有意义化。计算层与激活层多次配合构建一个计算模型。
- softmax：我的理解很朴素，就是将计算模型出来的结果的每个维度概率化处理。在沐神的课程了解到许多的框架都需要softmax为算法处理结果，更别提我要做的MNIST只是一个十种结果的分类问题了。
- 损失函数：将通过softmax得到的结果与答案进行计算，得到误差。
- 优化：由损失函数对计算模型参数求导再求均值*就是pytorch的梯度累积*得到平均梯度，将之前计算模型中的参数减去平均梯度乘learning rate从而更新计算模型，这样可以使损失函数的值减小，达到训练模型的效果。

不得不说pytorch真的太  :cow:了，把各种难搞的部分都封装好了。

我稍微解释一下我的代码，由于感觉作者用的两层卷积层太牛b了~~其实是看不懂~~，我用两层全连接写了一个计算模型，竟然也可以达到98%的正确率:flushed:
