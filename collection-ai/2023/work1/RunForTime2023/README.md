# 历程

一开始先去啃李沐的《动手学深度学习》，发现它的文字读起来有些不通顺，看得难受。

磕磕绊绊看了前两章，看得云里雾里，最后不得已半路弃坑。

眼瞅着交作业的时间要到了。只得先“临时抱佛脚”，等代码弄出来了再回头看原理去。

在B站找了个up主的视频，跟着他手敲了一遍。跑的时候发现，自个电脑配置太烂了（没有GPU，CPU有点老，内存小，核显还跟废物一样），训练一个mnist跑完居然要近半小时...

~~交之前看了下别人的PR，发现有个同学跟我看的是一个视频。。。代码几乎相同。。。~~

# 模型理解

用的应该是卷积神经网络（CNN）。

由于mnist数据集中每张图均为灰度图，像素是28*28，故输入通道仅1个。然后将输入数据进行卷积运算（核的大小为5），并采用激活函数ReLU()获得隐藏层数据。

然后，这里使用了大于1的步幅以减少冗余信息。之后进行第二次卷积运算并激活。

再然后用全连接层并激活，重复2次，然后使用softmax()计算每个数字可能的概率。

该模型通过将原始图片数据正则化来防止过拟合，利用训练集训练10轮，通过计算交叉熵损失（作为损失函数）和梯度并反向传播来使得训练的精度更高。

其他内容在代码的注释中有所说明。~~目前自己连基本概念都看不太懂，只能先这么胡言乱语了，后续可能会传一份更新的README文档~~
