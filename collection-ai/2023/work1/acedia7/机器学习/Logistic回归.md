### 引入

对于分类问题而言，线性回归并不是一个比较好的方法，它的拟合直线以及预测值都有比较大的偏差，所以引入Logistic回归来处理分类问题

其实质是一种分类算法

![](D:\python\机器学习\3\1.png)

边界划分----》决策边界

高阶多项式的参数选择？

根据数据自动拟合参数

非凸函数的特性使得原本的平方差 的代价函数并不适用

新的代价函数：

![](D:\python\机器学习\3\2.png)

合并代价函数，推出梯度下降

![](D:\python\机器学习\3\3.png)

![](D:\python\机器学习\3\4.png)





# 过度拟合问题——Logistic  &  线性回归



高偏差与高方差

高方差也认为是过度拟合————无法应用到新的样本



正则化

避免过度拟合

在代价函数中加入惩罚项，使得参数θ尽可能地小，曲线平滑，使得其拟合度恰好



修改代价函数，加入正则项



![](D:\python\机器学习\3\5.png)

正则化参数λ的选择  



![](D:\python\机器学习\3\6.png)

梯度下降法优化



### softmax初认识(参考文章链接：https://blog.csdn.net/m0_54634272/article/details/128721623)

在我已知Logistic(Sigmoid)函数基础上，看到softmax函数感觉迷茫，上网找了一篇文章，才逐渐分清二者区别
Sigmoid =多标签分类问题=多个正确答案=非独占输出

Sigmoid函数是一种logistic函数，它将任意的值转换到 [ 0 , 1 ] [0,1][0,1]之间，如图1所示，函数表达式为：


![](D:\python\机器学习\3\7.png)

![](D:\python\机器学习\3\8.png)

优点：

1. Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作输出层。

2. 连续函数，便于求导。

   

   缺点：

1. 最明显的就是饱和性，从上图也不难看出其两侧导数逐渐趋近于0，容易造成梯度消失。
2. 激活函数的偏移现象。Sigmoid函数的输出值均大于0，使得输出不是0的均值，这会导致后一层的神经元将得到上一层非0均值的信号作为输入，这会对梯度产生影响。 ————>没搞懂，绷

3. 计算复杂度高，因为Sigmoid函数是指数形式。



softmax函数

Softmax =**多类别分类问题**=**只有一个正确答案**=互斥输出

**Softmax**函数，又称归一化指数函数，函数表达式为：

![](D:\python\机器学习\3\9.png)

Softmax函数是二分类函数Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。

Softmax直白来说就是将原来输出是3,1,-3通过Softmax函数一作用，就映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标。------->解释的太清晰了，悟了





所以用numpy实现softmax函数简单思路：

已知一向量V

根据计算的公式可知我只需求出每一个元素exp(xi)，然后再求和，最后对（exp(xi)/sum（exp(xi)）求和即可

实现代码则如提交作业



PyTorch 中自带的 Softmax 函数

```python
import torch
import torch.nn as nn
x = torch.tensor([2.0, 1.0, 0.1])
outputs = torch.softmax(x, dim=0)  # dim=0，表示处理的是第 1 维的数据
print('torch 版 softmax 的输入 :', x)
print('torch 版 softmax 的输出 :', outputs)
print('torch 版 softmax 的输出之和:', outputs.sum())
```

