# 单变量线性回归

### 模型描述

监督学习   &&  回归问题 

data set    训练集   

符号：

m         训练样本数量

x           输入变量

y           输出变量

(x,y)      一个训练样本

(x^(i),y^(i))           第i个训练样本

h           假设函数——    x->h->y

<u>如何表示假设函数h?</u>

h_\theta(x) = \theta0 + \theta1(x)

<img src="\1\4.png" style="zoom:50%;" />

预测y是x的线性函数——单变量线性回归

### 代价函数

引入

<img src="\1\5.png" style="zoom:50%;" />

选择参数θi

使得x->h->y，h_θ(x)能比较好的拟合

为了减小误差，我们要让(h_θ(x)-y)^2尽可能地小，也就是预测值与实际值之差的平方尽可能地小

此处代价函数(cost function)也被称为平方误差函数或平方误差函数

平方误差函数对于多数问题尤其是回归问题而言是一个合理的选择

<img src="\1\6.png" style="zoom:90%;" />

<img src="\1\7.png" style="zoom:90%;" />

简化此假设函数使得代价函数J更好地可视化



<img src="\1\8.png" style="zoom:90%;" />

<img src="\1\9.png" style="zoom:90%;" />

等高图像展示代价函数

<img src="D:\python\机器学习\1\10.png" style="zoom:90%;" />

J-->h

<u>我们需要一个高速的算法自动算出最小的代价并拟合假设函数？</u>

梯度下降

用梯度下降方法最小化任意函数J

——**不停地改变θ0  θ1使得J（θ0，θ1）最小**

梯度下降特点：初始值不同可能得到不同的局部最优解

符号：   =:      表示赋值

<img src="\1\11.png" style="zoom:100%;" />

α：学习效率——步长>0

α过小：下降太慢

α过大：下降太快——无法收敛甚至发散

由于导数项的变化   α与导数项的乘积逐渐减小，则梯度下降速度减缓，步伐减缓，最后到最低点

可以用来计算各种函数的代价函数

*梯度下降 + 代价函数 --> 线性回归*

![](\1\12.png)

![](\1\13.png)

问题：容易陷入局部最优？

我们需要得到凸函数——弓形函数：全局最优解，无局部最优

***Batch 梯度下降***     ：全览了整个训练集

正规方程组方法，梯度下降更适合大数据集





### 补充学习

梯度下降（gradient descent）在机器学习中应用十分的广泛，不论是在线性回归还是Logistic回归中，它的主要目的是通过迭代找到目标函数的最小值，或者收敛到最小值。

梯度实际上就是多变量微分的一般化。

![](\1\14.png)

（就是高数里的梯度算子）



##### 代码实现

1、场景分析

下面我们将用python实现一个简单的梯度下降算法。场景是一个简单的线性回归的例子：假设现在我们有一系列的点，如下图所示：

![](\1\15.png)

我们可以给每一个点x增加一维，这一维的值固定为1，这一维将会乘到Θ0上。这样就方便我们统一矩阵化的计算

```python
from numpy import *

# 数据集大小 即20个数据点
m = 20
# x的坐标以及对应的矩阵
X0 = ones((m, 1))  # 生成一个m行1列的向量，也就是x0，全是1
X1 = arange(1, m+1).reshape(m, 1)  # 生成一个m行1列的向量，也就是x1，从1到m
X = hstack((X0, X1))  # 按照列堆叠形成数组，其实就是样本数据
# 对应的y坐标
y = np.array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m, 1)
# 学习率
alpha = 0.01
# 定义代价函数
def cost_function(theta, X, Y):
    diff = dot(X, theta) - Y  # dot() 数组需要像矩阵那样相乘，就需要用到dot()
    return (1/(2*m)) * dot(diff.transpose(), diff)
# 返回的是代价函数J

# 定义代价函数对应的梯度函数
def gradient_function(theta, X, Y):
    diff = dot(X, theta) - Y
    return (1/m) * dot(X.transpose(), diff)
# 梯度下降迭代
def gradient_descent(X, Y, alpha):
    theta = array([1, 1]).reshape(2, 1)
    gradient = gradient_function(theta, X, Y)
    while not all(abs(gradient) <= 1e-5):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, Y)
    return theta


optimal = gradient_descent(X, Y, alpha)
print('optimal:', optimal)
print('cost function:', cost_function(optimal, X, Y)[0][0])

# 通过matplotlib画出函数
def plot(X, Y, theta):
    import matplotlib.pyplot as plt
    ax = plt.subplot(111)  # 这是我改的
    ax.scatter(X, Y, s=30, c="red", marker="s")
    plt.xlabel("X")
    plt.ylabel("Y")
    x = arange(0, 21, 0.2)  # x的范围
    y = theta[0] + theta[1]*x
    ax.plot(x, y)
    plt.show()


plot(X1, Y, optimal)


```

