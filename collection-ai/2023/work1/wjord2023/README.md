# README

##### 考核任务完成情况

- 完成了numpy实现softmax函数和mnist手写数字辨识任务，也做了用numpy实现手写数字辨识的考核，但确实感觉有难度暂时未能完成，也提交了代码（但这过程使得我对卷积神经网络有了更为深刻的理解）
- mnist手写数字辨识任务使用了三层的神经网络，效果还不错，可以在未见过的测试集上达到98.56%的正确率
- numpy实现手写数字辨识那题完成了conv，max_pooling，relu，backward_propagation等函数的实现，但是不知道如何把这些方法串在一起用来训练
- 编译器我是使用安装了github copilot的pycharm来编写jupyter notebook，所有使用了一定的ai工具辅助（尤其是numpy实现手写数字那题，问了gpt许多问题来使得我可以理解代码）

##### 学习笔记说明

- 我参加了java和ai方向的两项考核，原先是想把java作为主语言，python作为辅助语言进行学习，但在这一段时期的深度学习的学习中，我感觉学习人工智能是非常让人兴奋的（再加上java对数据库进行增删改查操作确实枯燥），我想我应该做出一定的改变，以后会把python和ai学习作为主力方向
- 深度学习以及Pytorch的课程不如java那么多，要找到适合自己的课程更是困难，这段时间我也是试听了许多的课，包括各种号称全站第一课，最后决定台大李宏毅的课程最为适合我，也最为能让我理解人工智能的底层原理。在代码实现方面，我听了up主蓝斯诺特的pytorch2快速入门并跟着敲了一遍代码（代码也进行了上传），此外觉得吴恩达以及沐神的课也不错，光听快速入门肯定不足，后面应该会多听沐神的课来更深入学习代码实现
- 深度学习的课主要是在一些水课以及一些琐碎时间上听的，因此没有特别去记笔记，就截了许多的图，感觉把截图全布拼起来作为笔记我也不可能去看，所以我通过了我的理解进行了复述
- 目前线性代数和微积分知识的缺漏对我进一步理解深度学习造成了印象（最近在听逻辑回归的时候明显感受到缺陷），后面可能会先偏向花一定时间完成线代和高数的学习

##### 学习笔记

所谓机器学习，就是用机器去寻找一个函数，而机器学习的过程就是尝试去拟合这一个函数

在机器学习过程主要分作三个步骤，第一步确定一个函数具有不确定的参数（Function with Unknown Parameters），第二步确定一个loss函数以判断模型的好坏（Define loss from Training Data），第三步对模型进行优化（Optimization）

###### 第一步 寻找函数

根据数据集寻找可能的映射关系，比如预测天气就是要寻找当天天气与前一天各项天气指标的关系，要进行图像识别就是要寻找画面的图像与每个像素的关系

机器学习可以分做以下几类，不同类所需的数据集类型不同：

1. **监督学习**：给每个数据都打上标签让机器学习数据和标签的映射关系（这样的数据集一般很贵）
2. **无监督学习**：单纯把一堆数据丢给机器，让机器自己去理解数据之间的关联性，主要应该会用在生成式ai上吧（我个觉得，就像给你看了大量大师的画你也可以知道怎么画一张好画）（chatGPT就是先进行了大量的无监督学习再通过监督学习和强化学习的微调实现的）
3. **强化学习**：对机器产生结果的好坏进行评估，通过好坏进行参数的调整（评估不一定是自己评估，可以让ai和自己进行大量的博弈比如自己和自己打无数把dot2，根据输赢进行评估）（这样做就不需要数据集）

在确立好关系后会对所需的参数进行随机的初始化

###### 第二步 定义loss

及算出预测结果和真实结果之间的差值，给予模型反馈，使得其有方向调整参数

常见的loss函数有MSELoss(平方差的和)，CrossEntropy(交叉熵)

###### 第三步 优化模型

**梯度下降算法**：算出最好模型即相当与在参数为各种可能时所构成的loss函数上找到一个最小的点，在参数非常多的情况下要穷举所有可能是不现实的，所以我们需要找到一条下山最快的路，于是我们根据山坡的坡度即斜率也就是导数，和我们内心自己估计的学习速率来决定我们从那个方向下山，从这个方向要走多少的路。

梯度下降算法可能会遇到鞍点（critical minimum)和局部最小值（local minimum）但好在这个概率是非常低的，因为一个二维曲线的最低点放到三维曲面上可能只会是一个鞍点，所以在参数如此庞大可能上万上亿维的高维空间内出现local minimum的可能性是非常小的

**反向传播**：由于对如此复杂的神经网络直接进行求导是先当困难的，所以有了反向传播（实际也就是链式法则罢了）

我觉得3B1B的解释特别好，就是当遇到一个loss反馈后每个输出结果对应的神经元都希望能够降低loss即调整他们之前的参数使得他们的值可以更趋向期望的值，于是他们给出他们的导数告诉前面的神经元调整方向。而前面的神经元也会希望做出相应的调整，于是他们告诉前面的神经元他们的导数....

**激活函数**：参数和输出结果的映射是相当复杂的，于是我们用sigmoid和relu这样像是函数的叠加去逼近











