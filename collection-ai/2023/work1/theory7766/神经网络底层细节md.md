##训练神经网络

- 定义模型net：
- 初始化模型参数
- 定义损失函数loss
- 定义优化器updater
- 读取数据集->训练train

##模型预测和评估
在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。 在接下来的实验中，我们将使用精度（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。

##线性神经网络
###线性回归 y=XW+b

### softmax回归（分类问题）

- softmax运算获取一个向量并将其映射为概率
- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数

细节：

1.对于分类问题，采用独热编码（one-hot encoding).独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。

2. softmax运算：
o=Wx+b,y_hat=softmax(o)=>

yjHat= exp(oj)/sum_1^k(exp(ok))

此处，所有的0<=yj_hat<=1.即可将yHat视为一个正确的概率分布。即argmax(yjHat)=argmax(ojHat)

3.损失函数

3.1 对数似然

 softmax函数给出了一个向量yHat
， 我们可以将其视为“对给定任意输入的每个类的条件概率”。根据最大似然估计，我们最大化P(Y|X)，相当于最小化负对数似然。其中，对于任何标签y和模型预测yHat，损失函数为：
$l(y,yHat)=-sum_(j=1)^q yj*log(yjHat)$即交叉熵函数。由于独热编码的存在，除了一个项之外的所有项j都消失了。由于所有yjHat都是预测的概率，所以它的对数永远不会大于0。

	#使用交叉熵损失函数：
	def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

3.2 softmax及其导数

根据计算，我们会发现softmax导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异即梯度是观测值y和估计值yHat之间的差异。

4.前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。

反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。

在训练深度学习模型时，前向传播和反向传播是相互依赖的。


5.激活函数：一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。类似于人类大脑中基于神经元的模型，激活函数最终决定了要发射给下一个神经元的内容。

加入非线性激励函数后，神经网络就有可能学习到平滑的曲线来分割平面，而不是用复杂的线性组合逼近平滑曲线来分割平面，使神经网络的表示能力更强了，能够更好的拟合目标函数。 这就是为什么我们要有非线性的激活函数的原因。

常见激活函数：

- Sigmoid：σ(x)=1/(1+exp(-x))【它能够把输入的连续实值变换为0和1之间的输出】
- tanh：f(x)=tanh(x)=2/(1+exp(-2x))-1
- ReLU:x>=0,σ(x)=max(0,x);x<0,σ(x)=0
- Leaky ReLU:f(x)=max(0.01x,x)
- softmax:f(x)=exp(zi)/∑(j=1)^K exp(zj)
