1.对于stdconv2d
2.对于groupnorm及其它归一化方法
3.对于Attention机制
4.对于classtoken
5.对于pos_embed
6.对于GELU和其它激活函数的比较
关于caltech101

1.对于stdconv2d
(1)stdconv2d和conv2d的区别在于它们的实现方式和计算方式不同。stdconv2d是一种基于矩阵乘法的卷积计算方式，而conv2d则是一种基于滑动窗口的卷积计算方式。stdconv2d的计算速度相对较快，但需要较大的内存空间;而conv2d则需要较小的内存空间，但计算速度相对较慢。在实际应用中，需要根据具体的场景和需求选择合适的卷积计算方式。
(2在深度学习中，Convolutional Neural Networks(CNN)是一种用于图像识别、物体检测、语音识别、自然语言处理等任务的常用神经网络。在CNN中，卷积操作是一种重要的数学运算。常见的卷积操作包括两种，一种是标准卷积(stdconv2d)，另一种是二维卷积)



2.对于groupnorm及其它归一化方法
pytorch中BatchNorm、LayerNorm、InstanceNorm、GroupNorm区别
BN，LN，IN，GN从学术化上解释差异：
BatchNorm：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布
LayerNorm：channel方向做归一化，算CHW的均值，主要对RNN作用明显；
InstanceNorm：一个channel内做归一化，算H*W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。
SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。


3.对于Attention机制
  将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。
  然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。
  然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。


4.对于classtoken
classtoken部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。    
在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。
此时生成一个classtoken，将classtoken堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。
在特征提取的过程中，classtoken会与图片特征进行特征的交互。最终分类时，我们取出classtoken的特征，利用全连接分类。


5.对于pos_embed
为网络提取到的特征添加上位置信息。
以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上classtoken后就是197, 768
此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。



6.对于GELU和其它激活函数的比较
相对于 Sigmoid 和 Tanh 激活函数，ReLU 和 GeLU 更为准确和高效，因为它们在神经网络中的梯度消失问题上表现更好。梯度消失通常发生在深层神经网络中，意味着梯度的值在反向传播过程中逐渐变小，导致网络梯度无法更新，从而影响网络的训练效果。而 ReLU 和 GeLU 几乎没有梯度消失的现象，可以更好地支持深层神经网络的训练和优化。
而 ReLU 和 GeLU 的区别在于形状和计算效率。ReLU 是一个非常简单的函数，仅仅是输入为负数时返回0，而输入为正数时返回自身，从而仅包含了一次分段线性变换。但是，ReLU 函数存在一个问题，就是在输入为负数时，输出恒为0，这个问题可能会导致神经元死亡，从而降低模型的表达能力。GeLU 函数则是一个连续的 S 形曲线，介于 Sigmoid 和 ReLU 之间，形状比 ReLU 更为平滑，可以在一定程度上缓解神经元死亡的问题。不过，由于 GeLU 函数中包含了指数运算等复杂计算，所以在实际应用中通常比 ReLU 慢。
总之，ReLU 和 GeLU 都是常用的激活函数，它们各有优缺点，并适用于不同类型的神经网络和机器学习问题。一般来说，ReLU 更适合使用在卷积神经网络（CNN）中，而 GeLU 更适用于全连接网络（FNN）。



关于caltech101:
属于 101 个类别的物体的图片。每个类别大约 40 到 800 张图像。大多数类别大约有 50 张图像。2003 年 9 月由 Fei-Fei Li、Marco Andreetto 和 Marc 'Aurelio Ranzato 收集。每张图像的大小约为 300 x 200 像素。
我们仔细点击了这些图片中每个物体的轮廓，这些都包含在“Annotations.tar”下。还有一个 matlab 脚本可以查看 annotaitons，'show_annotations.m'。
加州理工学院 101 数据集总共包含 9146 张图像，分为 101 个不同的对象类别，以及一个额外的背景/杂波类别。
每个对象类别平均包含 40 到 800 张图像。常见和流行的类别（例如人脸）往往比较少使用的类别具有更多的图像。每张图像的尺寸约为 300x200 像素。定向物体（如飞机和摩托车）的图像被镜像为左右对齐，垂直方向的结构（如建筑物）被旋转为偏离轴线。

