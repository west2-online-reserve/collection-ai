## 网络层

Conv2D：通常用于二维卷积层的构建。底层实现利用了二维互相关运算（因为二维互相关运算与卷积运算类似，为了得到卷积运算的输出，只需要将核数组左右翻转并上下翻转，然后再与输入数组做互相关运算即可）

BatchNormalization：对一个批量内的数据正则化/归一化处理。目的是防止模型过拟合。

LSTM：长短期记忆门。它有3个门：输入门、遗忘门、输出门。输入门决定采用多少（来自候选记忆元的）新信息，遗忘门决定有多少过去的信息会留存至今使用，而输出门决定有哪些信息会传递到下一时间步以及它们的权重。

GRU：门控循环单元。它有2个门：重置门、更新门。重置门决定如何将新的输入信息糅合进前面保存的信息，而更新门决定有多少过去的信息会留存到当前时间步使用。

Dropout：暂退法。即训练时，在计算后续层之前向网络的每一层注入噪声。或者说，在一定概率下舍弃部分神经节点。

## 损失函数篇

CrossEntropyLoss()：交叉熵损失函数。它适用于多分类问题。计算公式为：$H(q,p)=-\sum q_i\log{p_i}$

MSE()：均方损失。它一般用于线性回归问题。计算公式为：$l(x,\widehat{y})=\frac{1}{2}(y-\widehat{y})^2$

## 优化方法

SGD：随机梯度下降。它每次使用单个样本（作为全局的近似）来更新梯度。相较于标准梯度下降，它的速度更快。

Adam：它是一种自适应优化，默认初始学习率为0.001，在训练时会动态调整学习率，使梯度快速收敛至局部/全局最优。在许多场景下Adam是个优秀的优化器，但在少数情况下训练模型的泛化准确率可能不如SGD。

## 激活函数

Sigmoid()：它常见于生物学（即物种繁殖曲线），表达式为$f(x)=\frac{1}{1+e^{-x}}$。值域为(0,1)。在ReLU()出现之前，普遍以它作为激活函数。

Tanh()：它的表达式为$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$。Tanh()的值域为(-1,1)，形状与Sigmoid()相似。

ReLU()：一种较良好的激活函数。它的表达式为$f(x)=max(0,x)$。它避免了Sigmoid()可能导致的梯度消失问题。

LeakyReLU()：x>0时与ReLU()相同。当x<0时，LeakyReLU()的导出值为λx（其中λ∈(0,1)）。在某些情况下，LeakyReLU()可能表现得比ReLU()更优秀。

## 部分神经网络

AlexNet：它使用了八层卷积神经网络。它接受一个三通道的、大小为224×224的图片（尺寸不同的原始图片需先缩放、剪切）。AlexNet前四层是两个“卷积层+最大汇聚层”的组合，然后是连续三层带填充（padding）的卷积层，之后又是一层最大汇聚层，最后三层为全连接层。与LeNet5相比，它的网络深度上升，且使用ReLU()而非Sigmoid()作为激活函数。

ResNet：它沿袭了VGG的结构。不同的是，ResNet在每个卷积层后添加了BatchNorm层。
