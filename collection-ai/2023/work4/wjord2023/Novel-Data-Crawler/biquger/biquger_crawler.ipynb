{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56375b18ff52542f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "i = 0\n",
    "book_url_list = []\n",
    "while True:\n",
    "    url = f'https://m.bqg90.com/json?sortid=0&page={i}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200 and response.text:\n",
    "        response = json.loads(response.text)\n",
    "        for item in response:\n",
    "            book_url_list.append('https://m.bqg90.com' + item['url_list'])\n",
    "        print(f'第{i}页爬取成功')\n",
    "        i += 1\n",
    "    else:\n",
    "        print(f'在爬取第{i}页时出现错误, 错误码: {response.status_code}')\n",
    "        break\n",
    "book_url_list[:5], print(len(book_url_list))\n",
    "\n",
    "book_url_list = list(set(book_url_list))\n",
    "\n",
    "with open('book_url_list.pkl', 'wb') as f:\n",
    "    pickle.dump(book_url_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91954cca39c3d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "book_url_list = list(set(book_url_list))\n",
    "with open('book_url_list.pkl', 'wb') as f:\n",
    "    pickle.dump(book_url_list, f)\n",
    "with open('book_url_list.pkl', 'rb') as f:\n",
    "    book_url_list = pickle.load(f)\n",
    "book_url_list[:5], len(set(book_url_list)), len(book_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9158fff62acf3bd8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 2587/3750 [24:18<10:55,  1.77it/s] \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import tqdm\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from lxml import etree\n",
    "import threading\n",
    "\n",
    "# 创建锁\n",
    "lock = threading.Lock()\n",
    "\n",
    "def get_chapter(book_url, headers, pbar):\n",
    "    response = requests.get(book_url, headers=headers)\n",
    "    # response = etree.HTML(response.text)\n",
    "    selector = Selector(text=response.text)\n",
    "    # 获取书籍类型\n",
    "    book_type = selector.xpath('//div[2]//dd[1]/span[2]/text()').extract_first()\n",
    "    # 如果获取不到说明网页不存在\n",
    "    if book_type is None:\n",
    "        return None  \n",
    "    book_type = book_type.split('：')[1]\n",
    "    # 获取书籍名\n",
    "    book_name = selector.xpath('//div[1]/div[2]/dl/dt/text()').extract_first()\n",
    "    book_detail_url = book_url + 'list.html'\n",
    "    response = requests.get(book_detail_url, headers=headers)\n",
    "    selector = Selector(text=response.text)\n",
    "    \n",
    "    # 用于存放章节信息\n",
    "    chapter_json_list = []\n",
    "    # 添加章节信息\n",
    "    i = 2\n",
    "    while True:\n",
    "        xpath_str = f'//dl/dd[{i}]/a/text()'\n",
    "        chapter_name_text = selector.xpath(xpath_str).extract_first()\n",
    "        if chapter_name_text:\n",
    "            i += 1\n",
    "            # 去掉不要的章节\n",
    "            if chapter_name_text.startswith('第') and '章' in chapter_name_text:\n",
    "                # 如果没有空格, 添加空格\n",
    "                if ' ' not in chapter_name_text:\n",
    "                    index = chapter_name_text.index('章')\n",
    "                    chapter_name_text = chapter_name_text[:index + 1] + ' ' + chapter_name_text[index + 1:]\n",
    "                # 添加章节名，并去除无用的信息\n",
    "                # 去除章节名中的'第..章'\n",
    "                chapter_name_text_clean = chapter_name_text.split(' ')[1]\n",
    "                # # 去除章节名中的'【】'和'（）'和'()',可能会去除全部的章节名，但是我的训练集可以允许为空的章节名\n",
    "                # chapter_name_text_clean = chapter_name_text_clean.split('【')[0].split('（')[0].split('(')[0]\n",
    "                # chapter_name_text_clean = chapter_name_text_clean.split('】')[-1].split('）')[-1].split(')')[-1]\n",
    "                        \n",
    "                # 添加章节链接\n",
    "                chapter_url_end = selector.xpath(\n",
    "                    f'//dl/dd[{i}]/a/@href').extract_first()\n",
    "                if chapter_url_end:\n",
    "                    chapter_url = 'https://m.bqg90.com/' + chapter_url_end\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                chapter_json_list.append({\n",
    "                    'book_name': book_name,\n",
    "                    'book_type': book_type,\n",
    "                    'chapter_name': chapter_name_text_clean,\n",
    "                    'chapter_index': int(chapter_name_text.split(' ')[0][1:-1]),\n",
    "                    'chapter_url': chapter_url})\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    with lock:        \n",
    "        with open('get_chapter_progress.txt', 'w') as f:    \n",
    "            f.write(book_url)\n",
    "            \n",
    "        with open('chapter_json.json', 'a', encoding='utf-8') as f:\n",
    "            for chapter_json in chapter_json_list:\n",
    "                f.write(json.dumps(chapter_json, ensure_ascii=False) + '\\n')\n",
    "        pbar.update(1)\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "with open('book_url_list.pkl', 'rb') as f:\n",
    "    book_url_list = pickle.load(f)\n",
    "\n",
    "if os.path.exists('get_chapter_progress.txt'):\n",
    "    with open('get_chapter_progress.txt', 'r') as f:\n",
    "        start_url = f.read().strip()\n",
    "else:\n",
    "    start_url = None\n",
    "\n",
    "if start_url:\n",
    "    start_index = book_url_list.index(start_url) + 1\n",
    "    book_url_list = book_url_list[start_index:10000]\n",
    "\n",
    "with tqdm.tqdm(total=len(book_url_list)) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "        for book_url in book_url_list:\n",
    "            executor.submit(get_chapter, book_url, headers, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd300e8170650538",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-12T12:43:09.870030Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 181079/285392 [9:47:52<5:41:15,  5.09it/s]    "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import tqdm\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def get_book_text(chapter_data, headers, pbar):\n",
    "    response = requests.get(chapter_data['chapter_url'], headers=headers)\n",
    "    selector = Selector(text=response.text)\n",
    "    paragraph_list = selector.xpath('//*[@id=\"chaptercontent\"]/text()').extract()\n",
    "    if paragraph_list:\n",
    "        text = ''\n",
    "        for paragraph in paragraph_list:\n",
    "            if paragraph.endswith('。'):\n",
    "                text += paragraph + '\\n'\n",
    "        data_json = {\n",
    "            'book_name': chapter_data['book_name'],\n",
    "            'book_type': chapter_data['book_type'],\n",
    "            'chapter_name': chapter_data['chapter_name'],\n",
    "            'chapter_index': chapter_data['chapter_index'],\n",
    "            'book_text': text.replace('\\t', '').replace('\\r', '')\n",
    "        }\n",
    "            \n",
    "        with lock:\n",
    "            with open('get_book_text_progress.txt', 'r') as f:\n",
    "                index = int(f.read().strip())\n",
    "            with open('get_book_text_progress.txt', 'w') as f:\n",
    "                f.write(str(index + 1))\n",
    "            with open('book_text.json', 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(data_json, ensure_ascii=False) + '\\n')\n",
    "            pbar.update(1)\n",
    "\n",
    "if os.path.exists('get_book_text_progress.txt'):\n",
    "    with open('get_book_text_progress.txt', 'r') as f:\n",
    "        start_index = int(f.read().strip())\n",
    "else:\n",
    "    with open('get_book_text_progress.txt', 'w') as f:\n",
    "        f.write('0')\n",
    "    start_index = 0\n",
    "    \n",
    "with open('chapter_json.json', 'r', encoding='utf-8') as f:\n",
    "    chapter_data_list = [json.loads(line) for line in f]\n",
    "    \n",
    "chapter_data_list = chapter_data_list[start_index:300000]\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "with tqdm.tqdm(total=len(chapter_data_list)) as pbar:\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        for chapter_data in chapter_data_list:\n",
    "            executor.submit(get_book_text, chapter_data, headers, pbar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
