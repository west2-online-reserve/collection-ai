### 爬取小说

> 爬取的小说网站为[笔趣看](https://www.biqukan.cc/)

#### 基础知识

网页HTML一般由三部分组成 ：

* HTML（超文本标记语言）
* CSS（层叠样式表）
* JavaScript（活动脚本语言）

**HTML标签**

```python
<html>...</html>    # 表示标记中间的元素是网页
<body>...</body>    # 表示用户可见的内容
<div>...</div>      # 表示框架
<p>...</p>          # 表示段落
<li>...</li>        # 表示列表
<img>...</img>      # 表示图片
<h1>...</h1>        # 表示标题
<a href="">...</a>  # 表示超链接
```

**CSS样式**

CSS 表示样式，如代码中`<style type="text/css" media="screen">` 表示将在下面引用一个 CSS，并在 CSS 中定义了对应的样式。

因为CSS只是样式，相当于化妆品，我们爬虫关注的是核心数据，一般不关心网页的CSS，所以此部分简短介绍。

**JavaScript脚本**

JavaScript 表示功能脚本。交互的内容和各种特效都在 JavaScript 中，JavaScript 描述了网站中的各种功能。

如果把网页比喻为人体，那么 HTML 是人的骨架，并且定义了人的嘴巴、眼睛、耳朵等要长在哪里；CSS 表示人的外观细节，如嘴巴长什么样子，眼睛是双眼皮还是单眼皮，是大眼睛还是小眼睛，皮肤是黑色的还是白色的等；JavaScript 表示人的技能，如跳舞、唱歌或演奏乐器等。

至此，我们基本知道了一个网页的核心结构，理论上我们就可以编写一个网页了。

#### 爬取章节四大步骤

* 发送请求
* 解析页面
* 定位标签
* 保存数据

> 抓取一个实际网站内容的**准备工作**如下(抛开正式的写代码工作):
>
> 1. 浏览器打开网站，简单观察页面信息，是否呈现了所需的内容；
> 2. 浏览器右键查看源代码，在源代码中是否能找到到想爬取的内容；
> 3. 打开浏览器控制台，观察网络请求；
>
> 为什么这几部非常重要了？原因有如下几点:
>
> 1. 有些网站内容保护比较好，浏览器打开后不一定呈现你想要抓取的内容，比如微博未登陆状态 打开 和 已登陆状态 打开呈现的内容是不一样的，此时需要一些小技巧才能模拟浏览器访问站点；
> 2. 目前有些网站采用了前后分离的技术（某种意义上更容易处理），页面呈现的内容并不是放在HTML源码中的，此时需要我们进一步分析网页的内容是通过什么机制返回的；
> 3. 观察网络请求，可以让我们看到此网站请求时所需要传递的参数，方便我们依葫芦画瓢，发送网络请求。

#### 发送请求

```python
import requests

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    # 输出返回的HTML源代码
    print(req.text)
    # 输出网站本身内容的编码方式
    print(req.apparent_encoding)
    # 输出requests转换内容的编码方式
    print(req.encoding)
    
    
    #有可能发生网页返回的编码方式和requests库的编码不同 这边有两个方法解决
    import requests

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    # 约定编码gbk，和网页返回编码保持一致
    req.encoding = 'gbk'

    print(req.text)
    
    
    #或者把返回的内容做编码转换
    import requests

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    # 将返回的内容，decode 为unicode
    print(req.content.decode('gbk'))
```

#### 网页解析&定位标签

网页解析可直接使用`BeautifulSoup`库，先将上一步中获取网页内容作为参数初始化一个`BeautifulSoup`对象变量，代码如下:

```python
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    html_content = req.content.decode('gbk')
    bs = BeautifulSoup(html_content, "lxml")
    # 输出HTML 代码的美化形式
    print(bs.prettify())
```

截取HTML核心片段:

```python
<ul class="mulu_list">
    <li class="fenjuan">《凡人修仙之仙界篇》正文</li>
    <li><a href="17748489.html">仙界篇外传一</a></li>
    <li><a href="17748490.html">仙界篇外传二</a></li>
    <li><a href="17748491.html">第一章 狐女</a></li>
    <li><a href="17748492.html">第二章 石头哥哥</a></li>
    <li><a href="17748493.html">第三章 远去</a></li>
    <li><a href="17751178.html">第四章 相依</a></li>
    ...
    <li class="fenjuan">《凡人修仙之仙界篇》大道归一</li>
    <li><a href="35076658.html">第一千二百四十三章 祸福相依</a></li>
    <li><a href="35077267.html">第一千二百四十四章 迷阵重重</a></li>
    <li><a href="35081021.html">第一千二百四十五章 二金之争</a></li>
    ...
    <li></li>
    <li></li>
    <div class="clear"></div>
</ul>
```

观察此源码片段，发现章节信息在一个`ul`标签里面，并且`class="mulu_list"`。有了这两个**定位**信息，基本足够解析出我们想要的这个片段了。

我们通过代码`ul_tags = bs.find_all('ul', class_='mulu_list')`，就能解析出上面的html代码片段，但是有一个细节，因为有可能**多个**`<ul class="mulu_list">`这样的标签，所以`BeautifulSoup`我们尽量用`.find_all()`方法来获取，此时返回的是一个**列表**

完整代码如下:

```python
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    html_content = req.content.decode('gbk')
    bs = BeautifulSoup(html_content, "lxml")
    # 输出HTML 代码的美化形式
    # print(bs.prettify())
    ul_tags = bs.find_all('ul', class_='mulu_list')

    # 因为url_tags 返回是列表，所以我们需要通过下标访问
    print(ul_tags[0].prettify())
```

很顺利，接下来再匹配每一个`<a>`标签，并提取章节名和章节链接。

如果我们使用`Beautiful Soup`匹配到了下面这个`<a>`标签，如何提取它的`href`属性和`<a>`标签里存放的章节名呢？

```
<a href="17748491.html">第一章 狐女</a>
```

方法很简单，对`Beautiful Soup`返回的匹配结果`a`，使用`a.get('href')`方法就能获取`href`的属性值，使用`a.string`就能获取章节名，编写代码如下：

```python
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    html_content = req.content.decode('gbk')
    bs = BeautifulSoup(html_content, "lxml")
    # 输出HTML 代码的美化形式
    # print(bs.prettify())
    ul_tags = bs.find_all('ul', class_='mulu_list')

    # 因为url_tags 返回是列表，所以我们需要通过下标访问
    a_tags = ul_tags[0].find_all('a')
    for each in a_tags:
        print(each.string, target + each.get('href'))

```

#### 保存数据

通过前面讲解，我们获取了[《凡人修仙之仙界篇》](https://www.biqukan.cc/article/30067/) 的所有章节信息，但这些信息都在内存里面，程序退出后就丢失了。 所以为了避免每次抓取的时候都要获取一次章节信息，我们可以将章节信息保存文件中，此处我们直接将章节信息保存到`csv`文件中。

```python
import csv
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    chapter_csv_file = './link.csv'
    target = 'https://www.biqukan.cc/article/30067/'
    req = requests.get(url=target)

    html_content = req.content.decode('gbk')
    bs = BeautifulSoup(html_content, "lxml")
    # 输出HTML 代码的美化形式
    # print(bs.prettify())
    ul_tags = bs.find_all('ul', class_='mulu_list')

    # 因为url_tags 返回是列表，所以我们需要通过下标访问
    a_tags = ul_tags[0].find_all('a')
    csv_datas = []
    for each in a_tags:
        print(each.string, target + each.get('href'))
        csv_datas.append({
            'name': each.string,
            'url': target + each.get('href')
        })

    # 写入csv文件开始
    with open(chapter_csv_file, 'w') as output_csv:
        fields = ['name', 'url']
        output_writer = csv.DictWriter(output_csv, fieldnames=fields)
        output_writer.writeheader()
        for item in csv_datas:
            output_writer.writerow(item)

```

#### 爬取特定章节内容

**发送请求**

```python
import requests

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/17748491.html'
    req = requests.get(url=target)

    print(req.content.decode('gbk'))
```

**网页解析及定位标签**

直接看下单个章节的内容的HMTL标签是什么。 ![img](https://cdn.py2fun.com/course/spider/03/img/2023-04-01-16-09-00.png)

发现内容都在`<div id="htmlContent" class="chapter-content">`下面。

细心的朋友可能已经发现，除了`div`字样外，还有`id`和`class`。`id`和`class`就是`div`标签的属性，`content`和`showtxt`是属性值，一个属性对应一个属性值。这东西有什么用？它是用来区分不同的div标签的，因为div标签可以有很多，我们怎么加以区分不同的div标签呢？就是通过不同的属性值。

仔细观察我们会发现这样一个事实：`class`属性为`chapter-content`的`div`标签，独一份！这个标签里面存放的内容，是我们关心的正文部分。

知道这个信息，我们就可以使用`Beautiful Soup`提取我们想要的内容了，编写代码如下：

```python
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/17748491.html'
    req = requests.get(url=target)
    html = req.content.decode('gbk')

    bf = BeautifulSoup(html, 'lxml')
    # 使用find
    text_tag = bf.find('div', id='htmlContent', class_='chapter-content')
    print(text_tag.text)

```

可以看到，我们已经顺利匹配到我们关心的正文内容，但输出都在一行，没有换行处理？我们继续编写代码：

```python
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/17748491.html'
    req = requests.get(url=target)
    html = req.content.decode('gbk')

    bf = BeautifulSoup(html, 'lxml')
    text_tag = bf.find('div', id='htmlContent', class_='chapter-content')

    # 处理换行
    print(text_tag.text.replace("    ", "\n"))
```

**保存数据**

抓取了小说正文内容，那么我们就可以将其保存到`text`文件中，以后读小说再也不用打开浏览器，直接本地打开记事本即可。

```python
import requests
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # 凡人修仙之仙界篇 对应的网络连接 为 target
    target = 'https://www.biqukan.cc/article/30067/17748491.html'
    req = requests.get(url=target)
    html = req.content.decode('gbk')

    bf = BeautifulSoup(html, 'lxml')
    text_tag = bf.find('div', id='htmlContent', class_='chapter-content')

    # 处理换行
    content = text_tag.text.replace("    ", "\n")

    # 保持到本地文件
    with open('./content.txt','w') as pf:
        pf.write(content)
```

#### 完整代码

```python
from bs4 import BeautifulSoup
import requests, sys
import os
import csv


class SpiderBiqukan(object):
    """
    抓取小说站点 https://www.biqukan.cc/
    以凡人修仙之仙界篇 https://www.biqukan.cc/article/30067/作为示例
    """

    def __init__(self, article_id: int):
        """
        :param article_id: 传入30067，表示抓取 凡人修仙之仙界
        """

        self.target = f'https://www.biqukan.cc/article/{article_id}/'
        self.chapter_csv_dir = f'./{article_id}'  # 保存的信息的目录

        self.chapter_csv_file = os.path.join(self.chapter_csv_dir, 'chapter.csv')
        self.all_content_file = os.path.join(self.chapter_csv_dir, 'all_content.txt')

        # 初始化保持目录
        self.init_save_dir()

    def init_save_dir(self):
        """
        初始化保存抓取信息的目录
        """
        if not os.path.exists(self.chapter_csv_dir):
            try:
                os.mkdir(self.chapter_csv_dir)
            except Exception as e:
                pass

    def spider_chapters(self):
        """
        抓取小说的章节信息并返回
        [
            {
                'name': 章节名，
                'url': 章节对应的链接
            }
        ]
        """
        # 章节信息的保存地址

        req = requests.get(url=self.target)
        html_content = req.content.decode('gbk')

        bs = BeautifulSoup(html_content, "lxml")
        # <div class="listmain">
        ul_tags = bs.find_all('ul', class_='mulu_list')

        # 因为url_tags 返回是列表，所以我们需要通过下标访问
        a_tags = ul_tags[0].find_all('a')
        csv_datas = []
        for each in a_tags:
            print(each.string, self.target + each.get('href'))
            csv_datas.append({
                'name': each.string,
                'url': self.target + each.get('href')
            })
        return csv_datas

    def spider_content(self, target):
        """
        抓取某章节内容
        """
        req = requests.get(url=target)
        try:
            html = req.content.decode('gbk')
            # 防止转码失败，部分文章中有特殊字符
        except Exception as e:
            html = req.text

        bf = BeautifulSoup(html, 'lxml')
        text_tag = bf.find('div', id='htmlContent', class_='chapter-content')
        # 处理换行
        content = text_tag.text.replace("    ", "\n")
        return content

    def write_csv(self, fields, csv_datas):
        """
        章节信息写csv文件
        """
        with open(self.chapter_csv_file, 'w') as output_csv:
            # fields = ['name', 'url']
            output_writer = csv.DictWriter(output_csv, fieldnames=fields)
            output_writer.writeheader()
            for item in csv_datas:
                output_writer.writerow(item)

    def write_content(self, name, text):
        """
        所有章节内容写入 self.all_content_file 文件中
        """
        write_flag = True
        with open(self.all_content_file, 'a', encoding='utf-8') as f:
            f.write(name + '\n')
            f.writelines(text)
            f.write('\n\n')


if __name__ == "__main__":
    # 传入30067 代表 凡人修仙之仙界 小说的ID
    spider = SpiderBiqukan(30067)
    print('开始抓取章节信息:')
    csv_datas = spider.spider_chapters()
    print('保存章节信息到:%s' % (spider.chapter_csv_file))
    spider.write_csv(['name', 'url'], csv_datas)

    print('开始抓取具体章节内容:')
    for (i, item) in enumerate(csv_datas):
        print('正在抓取:', item['name'], item['url'])
        content = spider.spider_content(item['url'])
        # spider.write_csv('all_content.txt')
        spider.write_content(item['name'], content)

        print("  已下载:%.3f%%" % float(i / len(csv_datas)) + '\r')

    print('下载完成')
```

#### 爬取小说实战

爬取了一个小说网站[20小说网](https://www.20xs.org/)

```python
import os,tqdm
import requests
from bs4 import BeautifulSoup

# 网站地址和书本ID
web = 'https://www.20xs.org'
while True:
    input_id = input('请输入想查询的图书id:')
    target = web + '/' + input_id

    # 获取网页内容
    try:
        req = requests.get(url=target)
        req.raise_for_status()  # 检查请求是否成功
    except requests.exceptions.RequestException as e:
        print(f"请求出错: {e},原因可能是不在的图书id")
        exit()

    b = BeautifulSoup(req.text, 'lxml')

    # 获取书名、作者、书籍描述、章节数
    meta_tag = b.find("meta", {"property": "og:novel:book_name"})
    author = b.find('meta',{"property":"og:novel:author"})['content']
    description = b.find('div',id='intro').find('p').string
    number = len(b.find_all('div', id='list')[0].find_all('a'))
    if meta_tag:
        book_name = meta_tag['content'] 
        break
    else:
        print("图书id错误,请重新输入")


#爬取询问
print(f'书名:{book_name}\n作者:{author}\n书籍描述:{description}\n章节数:{number}')
choice = input('是否进行爬取:(y/n):')
if choice == 'y':
    # 查找章节列表
    div_tag = b.find_all('div', id='list')
    if not div_tag:
        print("无法找到章节列表")
        exit()

    a_tags = div_tag[0].find_all('a')

    # 创建文件夹
    folder_path = os.path.join('爬取的东西', '小说', book_name)
    os.makedirs(folder_path, exist_ok=True)

    # 定义文件路径
    file_path = os.path.join(folder_path, f'{book_name}.txt')

    # 将章节列表写入文件
    with open(file_path, 'w', encoding='utf-8') as fp:
        for each in a_tags:
            # 写入章节标题和链接
            chapter_url = web + each.get('href')
            fp.write(each.string + ' ' + chapter_url + '\n')

    # 逐个章节抓取内容
    with open(file_path, 'a', encoding='utf-8') as fp:
        for i, each in tqdm.tqdm(enumerate(a_tags), total=len(a_tags)):
            chapter_url = web + each.get('href')
            try:
                chapter_req = requests.get(url=chapter_url)
                chapter_req.raise_for_status()  # 检查请求是否成功
            except requests.exceptions.RequestException as e:
                print(f"获取章节 {each.string} 时出错: {e}")
                continue

            b = BeautifulSoup(chapter_req.content, 'lxml')
            div_tags = b.find_all('div', id='content')
            if not div_tags:
                print(f"未找到章节内容 {each.string}")
                continue
            
            # 获取章节段落并写入文件
            p_tags = div_tags[0].find_all('p')
            fp.write(f'\n\n章节：{each.string}\n\n')
            for p in p_tags:
                if p.string:  # 避免写入 None
                    fp.write(p.string + '\n')

    print(f"采集完成，内容已保存至文件夹：{folder_path}")

```

### 爬取壁纸

#### 前期准备

##### 网站分析

我们先打开[手机壁纸](http://sj.zol.com.cn/bizhi/1080x1920/)观察网站，可以总结出三个点：

1. 站点不需要登录也可以访问；
2. 手机壁纸有多页，抓取后续的页面需要进行翻页动作；
3. 某一页的地址格式为`http://sj.zol.com.cn/bizhi/1080x1920/N.html`，`N.html` 表示第N页，可以替换为`1,2,3,4,...`等。

我们继续通过浏览器审查下元素，观察下图片详情的地址: <img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-15-37-45.png" style="zoom: 25%;" /> 

会发现图片的`a`标签地址格式为:

```html
 <a 
    class="pic" 
    href="/bizhi/detail_10981_121155.html"
    target="_blank" 
    hidefocus="true" 
    title = "中国最美公路227国道沿途风光"
 >
 ...
 </a>
```



<img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-15-45-20.png" alt="img" style="zoom:25%;" /> 

其中有一个重要的信息`class="pic"`，如果仔细观察HMTL源码可以判断有属性`class="pic"` 的`a`标签就是图片的超链接。

有了这个信息，那么我们就可以通过手机壁纸列表页中源码解析出所有的图片详情地址。

##### 详情页分析

刚刚我们分析完了首页，现在我们来看看每一个图集是什么结构，我们先打开图集[中国最美公路227国道沿途风光](http://sj.zol.com.cn/bizhi/detail_10981_121155.html)观察下详情： <img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-16-05-16.png" alt="img" style="zoom: 25%;" /> 

通过截图，我们可以大概得出：

1. 详情页里面也可以进行翻页动作，并且是一个图集；
2. 图集有标题，比如 《中国最美公路227国道沿途风光》
3. 图集的标题中，包含了图集的总数以及当前图片的序号

继续审查下元素，观察下一页的a标签元素:

 <img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-16-20-06.png" alt="img" style="zoom:25%;" /> 

发现下一个图片的`a`标签格式为:

```html
 <a 
    id="pageNext" 
    class ="next" 
    href="/bizhi/detail_10981_121153.html" title="点击浏览下一张壁纸，支持'→'翻页"
 >
 ...
 </a>
```



其中`class = "next"`可以精确定位出下一图片的详情页地址，但是在html的标签中，如果有id和class同时存在，那么我们推荐使用id来定位元素，因为通常情况下，id的值是唯一的

此图集有10张图，我们直接到最后一张图。

 <img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-16-25-21.png" alt="img" style="zoom:25%;" /> 

发现即便是最后一张图，但下一页依然可以点击，点击后你会发现，又返回到第一张图。

完整操作视频如下:

通过详情页分析，抛出了以下问题:

1. 如何获取图集的所有图片的详情页地址；
2. 翻页是循环的，如何避免循环抓取；

##### 图片地址分析

以图集[中国最美公路227国道沿途风光](http://sj.zol.com.cn/bizhi/detail_10981_121155.html)第一张图审查下元素： <img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-16-37-27.png" alt="img" style="zoom:25%;" /> 

发现图片的`img`标签格式如下:

```html
<img 

  id="bigImg" 

  src="https://sjbz-fd.zol-img.com.cn/t_s320x510c5/g6/M00/0E/07/ChMkKV-NICeIOAUKAEr-VrVkwWAAAD5UQDbYv8ASv5u449.jpg" 

  width="320" 

  height="510" 

  alt=""

/>
```

其中`src`属性就是图片的真实地址，最终目标就是通过`requests`库请求图片真实的地址并保存到本地。并且`id="bigImg"` 可以精确定位出图片的`img`标签。

##### 总结

通过网站分析我们基本可以确认，如果要抓取手机壁纸所有的图集，并且将每个图集的图片也保存下来，需要以下几个步骤： <img src="https://cdn.py2fun.com/course/spider/04/img/2020-10-20-17-03-51.png" alt="img" style="zoom: 50%;" />

接下来的小节中主要分为如下步骤进行讲解：

1. 抓取列表页，解析图集地址；
2. 抓取图集并保存图片；
3. 使用多进程，提高抓取速度。

#### 抓取列表页地址和图集地址

##### 列表地址

前一节中我们分析了[手机壁纸](http://sj.zol.com.cn/bizhi/1080x1920/)的列表页结构。

得出了如下结论:

1. 站点不需要登录也可以访问；
2. 手机壁纸有多页，抓取后续的页面需要进行翻页动作；
3. 某一页的地址格式为`http://sj.zol.com.cn/bizhi/1080x1920/N.html`，`N.html` 表示第N页，可以替换为`1,2,3,4,...`等。
4. 通过`class="pic"`可以精确获取图集的详情地址

```python
 <a 
    class="pic" 
    href="/bizhi/detail_10981_121155.html"
    target="_blank" 
    hidefocus="true" 
    title = "中国最美公路227国道沿途风光"
 >
 ...
 </a>
```

下面代码中我们主要实现了`get_page_urls`，生成抓取`start_page`到`end_page`的列表页地址并保存到`self.page_urls`中，然后输出:

```python
class SpiderZol(object):
    """
    抓取手机壁纸 http://sj.zol.com.cn/bizhi/1080x1920/
    """

    def __init__(self, start_page=1, end_page=5):
        # 手机壁纸网址
        self.target = 'http://sj.zol.com.cn/bizhi/1080x1920'
        # 列表的地址列表
        self.page_urls = []
        # 需要抓取的列表页起始 及 结束
        self.start_page = start_page
        self.end_page = end_page

    def get_page_urls(self):
        """
        某一页的地址格式为
        http://sj.zol.com.cn/bizhi/1080x1920/N.html  N为具体那一页

        start_page: 开始的页数
        end_page: 结束的页数
        """

        self.page_urls = []
        for i in range(self.start_page, self.end_page + 1):
            self.page_urls.append("{}/{}.html".format(self.target,i))

        print("抓取的列表页如下:")
        print("\n".join(self.page_urls))

    def go(self):
        self.get_page_urls()

if __name__ == '__main__':
    spider = SpiderZol(1,5)
    spider.go()
```

##### 解析图集地址

通过第一步，我们获取了列表页的地址，接下来我们增加函数`get_image_urls`请求每一页的数据，并解析出图集的名字及地址。

需要注意的是，在使用`bs.find_all()`函数的时候，若要筛选`class`属性，因为`class`在python中属于关键词，需要用`class_`代替。

核心代码为如下:

```python
import requests
from bs4 import BeautifulSoup

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
}

page_url = "http://sj.zol.com.cn/bizhi/1080x1920/1.html"
r = requests.get(page_url, headers=headers)
bs = BeautifulSoup(r.text, "lxml")

# 通过 <a class="pic"> 来定位解析
for tag in bs.find_all("a", class_="pic"):
    image_name = tag.text.strip()  # 获取图集名
    image_url = tag.get("href", "")  # 获取href属性

    # 将数据放入对象变量image_urls 中
    print("解析出图集:《{}》 地址为:{}".format(image_name, image_url))
```

##### 整合代码

```python
import requests
from bs4 import BeautifulSoup


class SpiderZol(object):
    """
    抓取手机壁纸 http://sj.zol.com.cn/bizhi/1080x1920/
    """

    def __init__(self, start_page=1, end_page=5):
        self.headers = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        }
        # 手机壁纸网址
        self.target = "http://sj.zol.com.cn/bizhi/1080x1920"
        # 列表的地址列表
        self.page_urls = []
        self.image_urls = []
        # 需要抓取的列表页起始 及 结束
        self.start_page = start_page
        self.end_page = end_page

    def get_page_urls(self):
        """
        某一页的地址格式为
        http://sj.zol.com.cn/bizhi/1080x1920/N.html  N为具体那一页

        start_page: 开始的页数
        end_page: 结束的页数
        """

        self.page_urls = []
        for i in range(self.start_page, self.end_page + 1):
            self.page_urls.append("{}/{}.html".format(self.target, i))

        print("抓取的列表页如下:")
        print("\n".join(self.page_urls))

    def get_image_urls(self, page_url):
        """
        抓取列表页如
        http://sj.zol.com.cn/bizhi/1080x1920/1.html
        解析出其中所有的图集地址
         <a
            class="pic"
            href="/bizhi/detail_10981_121155.html"
            target="_blank"
            hidefocus="true"
            title = "中国最美公路227国道沿途风光"
        >
        """
        r = requests.get(page_url, headers=self.headers)
        bs = BeautifulSoup(r.text, "lxml")

        # 通过 <a class="pic"> 来定位解析
        for tag in bs.find_all("a", class_="pic"):
            image_name = tag.text.strip()  # 获取图集名
            image_url = tag.get("href", "")  # 获取href属性

            # 将数据放入对象变量image_urls 中
            print("解析出图集:《{}》 地址为:{}".format(image_name, image_url))
            self.image_urls.append({"image_name": image_name, "image_url": image_url})

    def go(self):
        self.get_page_urls()
        for page_url in self.page_urls:
            self.get_image_urls(page_url)


if __name__ == "__main__":
    spider = SpiderZol(1, 1)
    spider.go()
```

#### 抓取图集并保存

##### 抓取图集

前一节中我们抓取了列表页并解析出了图集的地址，接下来我们抓取某个特定的图集并保存。

我们本节主要聚焦在请求图集并下载保存所有图片。

下文以图集[中国最美公路227国道沿途风光](http://sj.zol.com.cn/bizhi/detail_10981_121155.html)为例进行讲解。

直接用`requests`库请求网页，`BeautifulSoup`库解析`img`标签数据，获取真实的图片地址。 ![img](https://cdn.py2fun.com/course/spider/04/img/2020-10-20-19-45-23.png)

```python
import requests
from bs4 import BeautifulSoup

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
}
r = requests.get("http://sj.zol.com.cn/bizhi/detail_10981_121155.html", headers=headers)
bs = BeautifulSoup(r.text, "lxml")

# 通过 <img id="bigImg"> 来定位解析出图片的真实地址
"""
image 标签
<img
    id="bigImg"
    src="https://sjbz-fd.zol-img.com.cn/t_s320x510c5/g6/M00/0E/07/ChMkKV-NICeIOAUKAEr-VrVkwWAAAD5UQDbYv8ASv5u449.jpg"
    width="320"
    height="510"
    alt=""
/>
"""
for tag in bs.find_all("img", id="bigImg"):
    # 解析出图片地址，并请求下载保存数据
    print(tag.get("src"))
```

##### 保存图片到文件

第一步已经获取了图片的[真实地址](https://sjbz-fd.zol-img.com.cn/t_s320x510c5/g6/M00/0E/07/ChMkKV-NICeIOAUKAEr-VrVkwWAAAD5UQDbYv8ASv5u449.jpg)，假设现在已经有了。

图片本质是也是一个网页，我们依然可以用`requests`库直接请求，然后通过二进制模式打开保存到某一个文件，代码如下:

```python
import requests
req = requests.get('https://sjbz-fd.zol-img.com.cn/t_s320x510c5/g6/M00/0E/07/ChMkKV-NICeIOAUKAEr-VrVkwWAAAD5UQDbYv8ASv5u449.jpg')

# 图片非文本，需要以二进制模式b打开
with open('./tmp.jpg', "wb") as f:
    # 使用req.content 获取内容源数据
    f.write(req.content)
    print(type(req.content))
    print(type(req.text))
    print("保存成功")
```

此处有一个细节我们写入文件时用的是`req.content`其类型为`bytes`，如果改为`f.write(req.text)`将会报错，原因为图片是一个二进制文件非文本文件，所以需要使用`req.content`，并且`open()`文件时传入的模式为`wb`表示以二进制+可写模式打开文件。

##### 解析下一张图地址

前面两步我们解析出了当前页面的图片地址 并且保存了下来。

但是一个图集有多张图片，我们可以解析出下一个图片的详情地址，然后按照第一步、第二步进行处理。

所以核心问题变为如何解析下一页的地址:

![img](https://cdn.py2fun.com/course/spider/04/img/2020-10-20-19-53-05.png)

```python
import requests
from bs4 import BeautifulSoup

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
}
r = requests.get("http://sj.zol.com.cn/bizhi/detail_10981_121155.html", headers=headers)
bs = BeautifulSoup(r.text, "lxml")
"""
下一页a 标签
 <a
    id="pageNext"
    class ="next"
    href="/bizhi/detail_10981_121153.html" title="点击浏览下一张壁纸，支持'→'翻页"
>
"""
# 解析下一张图的递归抓取
for tag in bs.find_all("a", id="pageNext"):
    next_image_url = tag.get("href")
    print(next_image_url)
```

##### 整合代码

```python
import requests
from bs4 import BeautifulSoup
import os
import time


class SpiderZol(object):
    """
    抓取手机壁纸 http://sj.zol.com.cn/bizhi/1080x1920/
    """

    def __init__(self, start_page=1, end_page=5):
        # 请求的headers
        self.headers = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        }
        # 手机壁纸网址
        self.host = "http://sj.zol.com.cn"  # 站点host页
        self.target = "http://sj.zol.com.cn/bizhi/1080x1920"  # 手机壁纸页
        self.save_dir = "./zol_data"  # 保存的目录

        # 需要抓取的列表页起始 及 结束
        self.start_page = start_page
        self.end_page = end_page
        self.mkdir(self.save_dir)

    def spider_one_images(self, image_name, image_url, cur_image_no=1):
        """
        image_name: 图集名 可爱吃货油爆叽丁图片壁纸 (9张)
        image_url: 图集地址 http://sj.zol.com.cn/bizhi/detail_10981_121155.html
        """
        # 通过image_name 解析出 可爱吃货油爆叽丁图片壁纸 (9张)  当前图集有多少张图
        max_image_num = int(image_name.split(" ")[-1][1:-2])
        if cur_image_no > max_image_num:
            print("退出已到图集最后一张图")
            return

        # r = requests.get(image_url, headers=HEADERS)
        req = self.try_request_url(image_url)
        bs = BeautifulSoup(req.text, "lxml")

        print("第{}张图 地址:{}".format(cur_image_no, image_url))
        # 通过 <img id="bigImg"> 来定位解析出图片的真实地址
        for tag in bs.find_all("img", id="bigImg"):
            # 解析出图片地址，并请求下载保存数据
            self.save_image(image_name, tag.get("src"), cur_image_no)

        # 解析下一张图的递归抓取
        for tag in bs.find_all("a", id="pageNext"):
            next_image_url = tag.get("href")
            print(cur_image_no, next_image_url)
            self.spider_one_images(
                image_name, "{}{}".format(self.host, next_image_url), cur_image_no + 1
            )

    def save_image(self, image_name, image_src, image_no):
        """
        image_name: 图集名 如 可爱吃货油爆叽丁图片壁纸 (9张)
        image_src: 图片下载地址 如 https://b.zol-img.com.cn/sjbizhi/images/11/320x510/160308409840.JPG
        image_no: 当前图片的编号，第几张图
        """
        image_title = image_name.split(" (")[0]  # 去掉` (9张)`作为图集的title
        image_dir = os.path.join(self.save_dir, image_title)
        self.mkdir(image_dir)
        try:
            # 请求图片
            # req = requests.get(image_src)
            req = self.try_request_url(image_src)
            # 图片最终保存路径
            img_path = os.path.join(image_dir, "{}.jpg".format(image_no))

            # 图片非文本，需要以二进制模式b打开
            with open(img_path, "wb") as f:
                # 使用req.content 获取内容源数据
                f.write(req.content)
                print("{}/{}.jpg 保存成功".format(image_title, image_no))
        except:
            pass

    def try_request_url(self, url):
        try_num = 3
        i = 0
        req = None
        while i < try_num:
            req = requests.get(url, headers=self.headers)
            if req.status_code == 200:
                return req
            print(req.status_code, "尝试重新抓取", url)
            print(req.text)
            time.sleep(2)
            i += 1
        return req

    def mkdir(self, dir_path):
        """
        生成目录，如果已有则退出
        """
        if not os.path.exists(dir_path):
            try:
                os.mkdir(dir_path)
            except Exception as e:
                pass

    def go(self):
        image_name = "中国最美公路227国道沿途 (10张)"
        image_url = "/bizhi/detail_10981_121155.html"
        self.spider_one_images(image_name, "{}{}".format(self.host, image_url), 1)


if __name__ == "__main__":
    spider = SpiderZol(1, 1)
    spider.go()
```

我们将不同功能的代码库进行封装成了函数:

- `spider_one_images` 传入图片的详情页地址，并解析出图片真实地址调用`save_image`保存，然后解析下一张图的详情页地址，递归调用完成。
- `save_image` 请求并保存图片到本地文件
- `try_request_url` 封装了统一的请求函数，并且根据请求返回的状态码`status_code`做了至多尝试3次的逻辑（`status_code=200`则是正常请求，否则请求可能出错了）
- `mkdir` 创建目录

另外在这段代码中有一个**递归**的调用： ![img](https://cdn.py2fun.com/course/spider/04/img/2020-10-20-20-01-10.png)

此处的递归调用代码，需要注意退出条件，否则很容易出现死循环。 示例的代码中通过`cur_image_no`(表示图集的第几张图) 来控制的，如果超过了图集最大的图片数则退出。 ![img](https://cdn.py2fun.com/course/spider/04/img/2020-10-20-20-06-48.png)

其实递归调用也可以改为**for**循环，你可以本地尝试修改下代码，并改成循环调用的方式来完成。

#### 抓取的完整流程（单进程版）

```python
import requests
from bs4 import BeautifulSoup
import os
import time

import requests
from bs4 import BeautifulSoup
import os
import time


class SpiderZol(object):
    """
    抓取手机壁纸 http://sj.zol.com.cn/bizhi/1080x1920/
    """

    def __init__(self, start_page=1, end_page=5):
        # 请求的headers
        self.headers = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        }
        # 手机壁纸网址
        self.host = "http://sj.zol.com.cn"  # 站点host页
        self.target = "http://sj.zol.com.cn/bizhi/1080x1920"  # 手机壁纸页
        self.save_dir = "./zol_data"  # 保存的目录

        # 列表的地址列表
        self.page_urls = []
        self.image_urls = []
        # 需要抓取的列表页起始 及 结束
        self.start_page = start_page
        self.end_page = end_page
        self.mkdir(self.save_dir)

    def get_page_urls(self):
        """
        某一页的地址格式为
        http://sj.zol.com.cn/bizhi/1080x1920/N.html  N为具体那一页

        start_page: 开始的页数
        end_page: 结束的页数
        """

        self.page_urls = []
        for i in range(self.start_page, self.end_page + 1):
            self.page_urls.append("{}/{}.html".format(self.target, i))

        print("抓取的列表页如下:")
        print("\n".join(self.page_urls))

    def get_image_urls(self, page_url):
        """
        抓取列表页如
        http://sj.zol.com.cn/bizhi/1080x1920/1.html
        解析出其中所有的图集地址
         <a
            class="pic"
            href="/bizhi/detail_10981_121155.html"
            target="_blank"
            hidefocus="true"
            title = "中国最美公路227国道沿途风光"
        >
        """
        # r = requests.get(page_url)
        req = self.try_request_url(page_url)
        bs = BeautifulSoup(req.text, "lxml")

        # 通过 <a class="pic"> 来定位解析
        for tag in bs.find_all("a", class_="pic"):
            image_name = tag.text.strip()  # 获取图集名
            image_url = tag.get("href", "")  # 获取href属性

            # 将数据放入对象变量image_urls 中
            print("解析出图集:《{}》 地址为:{}".format(image_name, image_url))
            self.image_urls.append({"image_name": image_name, "image_url": image_url})

    def spider_one_images(self, image_name, image_url, cur_image_no=1):
        """
        image_name: 图集名 可爱吃货油爆叽丁图片壁纸 (9张)
        image_url: 图集地址 http://sj.zol.com.cn/bizhi/detail_10981_121155.html

        如http://sj.zol.com.cn/bizhi/detail_10981_121155.html
        image 标签
        <img
            id="bigImg"
            src="https://sjbz-fd.zol-img.com.cn/t_s320x510c5/g6/M00/0E/07/ChMkKV-NICeIOAUKAEr-VrVkwWAAAD5UQDbYv8ASv5u449.jpg"
            width="320"
            height="510"
            alt=""
        />
        下一页a 标签
         <a
            id="pageNext"
            class ="next"
            href="/bizhi/detail_10981_121153.html" title="点击浏览下一张壁纸，支持'→'翻页"
        >
        """
        # 通过image_name 解析出 可爱吃货油爆叽丁图片壁纸 (9张)  当前图集有多少张图
        max_image_num = int(image_name.split(" ")[-1][1:-2])
        if cur_image_no > max_image_num:
            print("退出已到图集最后一张图")
            return

        req = self.try_request_url(image_url)
        bs = BeautifulSoup(req.text, "lxml")

        print("第{}张图 地址:{}".format(cur_image_no, image_url))
        # 通过 <img id="bigImg"> 来定位解析出图片的真实地址
        for tag in bs.find_all("img", id="bigImg"):
            # 解析出图片地址，并请求下载保存数据
            self.save_image(image_name, tag.get("src"), cur_image_no)

        # 解析下一张图的递归抓取
        for tag in bs.find_all("a", id="pageNext"):
            next_image_url = tag.get("href")
            self.spider_one_images(
                image_name, "{}{}".format(self.host, next_image_url), cur_image_no + 1
            )

    def save_image(self, image_name, image_src, image_no):
        """
        image_name: 图集名 如 可爱吃货油爆叽丁图片壁纸 (9张)
        image_src: 图片下载地址 如 https://b.zol-img.com.cn/sjbizhi/images/11/320x510/160308409840.JPG
        image_no: 当前图片的编号，第几张图
        """
        image_title = image_name.split(" (")[0]  # 去掉` (9张)`作为图集的title
        image_dir = os.path.join(self.save_dir, image_title)
        self.mkdir(image_dir)
        try:
            # 请求图片
            req = self.try_request_url(image_src)
            # 图片最终保存路径
            img_path = os.path.join(image_dir, "{}.jpg".format(image_no))

            # 图片非文本，需要以二进制模式b打开
            with open(img_path, "wb") as f:
                # 使用req.content 获取内容源数据
                f.write(req.content)
                print("{}/{}.jpg 保存成功".format(image_title, image_no))
        except:
            pass

    def mkdir(self, dir_path):
        """
        生成目录，如果已有则退出
        """
        if not os.path.exists(dir_path):
            try:
                os.mkdir(dir_path)
            except Exception as e:
                pass

    def try_request_url(self, url):
        try_num = 3
        i = 0
        req = None
        while i < try_num:
            req = requests.get(url, headers=self.headers)
            if req.status_code == 200:
                return req
            print(req.status_code, "尝试重新抓取", url)
            print(req.text)
            time.sleep(2)
            i += 1
        return req

    def go(self):
        self.get_page_urls()
        for page_url in self.page_urls:
            self.get_image_urls(page_url)

        for item in self.image_urls:
            # item 为dict，各式为{'image_name': '中国最美公路227国道沿途 (10张)', 'image_url': '/bizhi/detail_10981_121155.html'}
            print("开始抓取图集:{}".format(item["image_name"]))
            self.spider_one_images(
                item["image_name"], "{}{}".format(self.host, item["image_url"]), 1
            )


if __name__ == "__main__":
    spider = SpiderZol(1, 1)  # 仅抓取第一页的图集
    spider.go()
```

#### 抓取的完整流程（多进程版）

```python
import requests
from multiprocessing import cpu_count, Pool
from bs4 import BeautifulSoup
import os
import time


class SpiderZol(object):
    """
    抓取手机壁纸 http://sj.zol.com.cn/bizhi/1080x1920/
    """

    def __init__(self, start_page=1, end_page=5):
        self.headers = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        }
        # 手机壁纸网址
        self.host = "http://sj.zol.com.cn"  # 站点host页
        self.target = "http://sj.zol.com.cn/bizhi/1080x1920"  # 手机壁纸页
        self.save_dir = "./zol_data"  # 保存的目录

        # 列表的地址列表
        self.page_urls = []
        self.image_urls = []
        # 需要抓取的列表页起始 及 结束
        self.start_page = start_page
        self.end_page = end_page
        self.mkdir(self.save_dir)

    def get_page_urls(self):
        """
        某一页的地址格式为
        http://sj.zol.com.cn/bizhi/1080x1920/N.html  N为具体那一页

        start_page: 开始的页数
        end_page: 结束的页数
        """

        self.page_urls = []
        for i in range(self.start_page, self.end_page + 1):
            self.page_urls.append("{}/{}.html".format(self.target, i))

        print("抓取的列表页如下:")
        print("\n".join(self.page_urls))

    def get_image_urls(self, page_url):
        """
        抓取列表页如
        http://sj.zol.com.cn/bizhi/1080x1920/1.html
        解析出其中所有的图集地址
         <a
            class="pic"
            href="/bizhi/detail_10981_121155.html"
            target="_blank"
            hidefocus="true"
            title = "中国最美公路227国道沿途风光"
        >
        """
        # r = requests.get(page_url)
        req = self.try_request_url(page_url)
        bs = BeautifulSoup(req.text, "lxml")

        # 通过 <a class="pic"> 来定位解析
        for tag in bs.find_all("a", class_="pic"):
            image_name = tag.text.strip()  # 获取图集名
            image_url = tag.get("href", "")  # 获取href属性

            # 将数据放入对象变量image_urls 中
            print("解析出图集:《{}》 地址为:{}".format(image_name, image_url))
            self.image_urls.append({"image_name": image_name, "image_url": image_url})

    def spider_one_images(self, image_name, image_url, cur_image_no=1):
        """
        image_name: 图集名 可爱吃货油爆叽丁图片壁纸 (9张)
        image_url: 图集地址 http://sj.zol.com.cn/bizhi/detail_10981_121155.html

        如http://sj.zol.com.cn/bizhi/detail_10981_121155.html
        image 标签
        <img
            id="bigImg"
            src="https://sjbz-fd.zol-img.com.cn/t_s320x510c5/g6/M00/0E/07/ChMkKV-NICeIOAUKAEr-VrVkwWAAAD5UQDbYv8ASv5u449.jpg"
            width="320"
            height="510"
            alt=""
        />
        下一页a 标签
         <a
            id="pageNext"
            class ="next"
            href="/bizhi/detail_10981_121153.html" title="点击浏览下一张壁纸，支持'→'翻页"
        >
        """
        # 通过image_name 解析出 可爱吃货油爆叽丁图片壁纸 (9张)  当前图集有多少张图
        max_image_num = int(image_name.split(" ")[-1][1:-2])
        if cur_image_no > max_image_num:
            print("退出已到图集最后一张图")
            return

        req = self.try_request_url(image_url)
        bs = BeautifulSoup(req.text, "lxml")

        print("第{}张图 地址:{}".format(cur_image_no, image_url))
        # 通过 <img id="bigImg"> 来定位解析出图片的真实地址
        for tag in bs.find_all("img", id="bigImg"):
            # 解析出图片地址，并请求下载保存数据
            self.save_image(image_name, tag.get("src"), cur_image_no)

        # 解析下一张图的递归抓取
        for tag in bs.find_all("a", id="pageNext"):
            next_image_url = tag.get("href")
            self.spider_one_images(
                image_name, "{}{}".format(self.host, next_image_url), cur_image_no + 1
            )

    def save_image(self, image_name, image_src, image_no):
        """
        image_name: 图集名 如 可爱吃货油爆叽丁图片壁纸 (9张)
        image_src: 图片下载地址 如 https://b.zol-img.com.cn/sjbizhi/images/11/320x510/160308409840.JPG
        image_no: 当前图片的编号，第几张图
        """
        image_title = image_name.split(" (")[0]  # 去掉` (9张)`作为图集的title
        image_dir = os.path.join(self.save_dir, image_title)
        self.mkdir(image_dir)
        try:
            # 请求图片
            req = self.try_request_url(image_src)
            # 图片最终保存路径
            img_path = os.path.join(image_dir, "{}.jpg".format(image_no))

            # 图片非文本，需要以二进制模式b打开
            with open(img_path, "wb") as f:
                # 使用req.content 获取内容源数据
                f.write(req.content)
                print("{}/{}.jpg 保存成功".format(image_title, image_no))
        except:
            pass

    def mkdir(self, dir_path):
        """
        生成目录，如果已有则退出
        """
        if not os.path.exists(dir_path):
            try:
                os.mkdir(dir_path)
            except Exception as e:
                pass

    def try_request_url(self, url):
        try_num = 3
        i = 0
        req = None
        while i < try_num:
            req = requests.get(url, headers=self.headers)
            if req.status_code == 200:
                return req
            print(req.status_code, "尝试重新抓取", url)
            print(req.text)
            time.sleep(2)
            i += 1
        return req

    def go(self):
        self.get_page_urls()
        for page_url in self.page_urls:
            self.get_image_urls(page_url)

        for item in self.image_urls:
            # item 为dict，各式为{'image_name': '中国最美公路227国道沿途 (10张)', 'image_url': '/bizhi/detail_10981_121155.html'}
            print("开始抓取图集:{}".format(item["image_name"]))
            self.spider_one_images(
                item["image_name"], "{}{}".format(self.host, item["image_url"]), 1
            )


def spider_func(page):
    print("进程ID:{}, 抓取第{}页".format(os.getpid(), page))
    spider = SpiderZol(page, page)  # 仅抓取第一页的图集
    spider.go()


if __name__ == "__main__":
    # 假设我们要抓取前10页的
    page_nums = list(range(1, 11))

    st_time = time.time()

    with Pool(cpu_count()) as p:
        p.map(spider_func, page_nums)

    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time - st_time))
```

#### 总结

通过本章的学习我们巩固:

- 通过浏览器观察网站，审查元素
- `requests`网页请求乱码的两种处理方式
- `BeautifulSoup` 如何根据网页的标签结合`.find_all()`方法查找元素
- 如何获取`a`标签的`href`及文字
- 如何获取`img`标签的`src`图片地址

并且get新的技能点:

- 抓取图片，并通过二进制模式打开文件保存图片
- 多进程模型编程`multiprocessing`及`Pool`库
- 如何单进程的爬虫代码修改完多进程的模式

此一章基本已经进入工程级别的爬虫实战了，如果掌握了，可以说抓取其他大型站点都可以手到擒来。

思考题: 如何借助本章的知识将之前抓取小说的爬虫代码 修改为 多进程的方式？

### 爬虫多进程

在真正进入多进程抓取，我们先简单介绍下Python中如何实现多进程主要的库`multiprocessing`。

Python 官方文档的介绍如下:

> multiprocessing 是一个支持使用与 threading 模块类似的 API 来产生进程的包。 multiprocessing 包同时提供了本地和远程并发操作，通过使用子进程而非线程有效地绕过了 全局解释器锁。 因此，multiprocessing 模块允许程序员充分利用给定机器上的多个处理器。 它在 Unix 和 Windows 上均可运行。 multiprocessing 模块还引入了在 threading 模块中没有的API。一个主要的例子就是 Pool 对象，它提供了一种快捷的方法，赋予函数并行化处理一系列输入值的能力，可以将输入数据分配给不同进程处理（数据并行）。下面的例子演示了在模块中定义此类函数的常见做法，以便子进程可以成功导入该模块。这个数据并行的基本例子使用了 Pool

#### 单进程示例

我们先从简单的例子说起，将一个单进程模型的代码，变为多进程的。

如下是一个单进程代码示例:

```python
from multiprocessing import cpu_count
import os
import time

def f(x):
    print("进程ID:{} 处理变量X: {}".format(os.getpid(),x))
    time.sleep(1) #主动sleep 1秒
    return x*x

if __name__ == '__main__':
    print('主进程ID:{} CPU 核数:{}'.format(os.getpid(),cpu_count()))
    # with Pool(cpu_count()) as p:
    #     print(p.map(f, [1, 2, 3, 4, 5, 6]))
    st_time = time.time()
    for i in range(1, cpu_count()+1):
        f(i)
    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time-st_time))
```

运行截图: ![img](https://cdn.py2fun.com/course/spider/04/img/2020-10-20-22-46-02.png)

其中有几个知识点:

1. `multiprocessing.cpu_count`方法可以获取机器的cpu核心，某种意义上cpu核心表示了最佳的并发进程数。
2. `os.getpid()`可以获取当前代码执行时的进程ID，此例因为就是单进程，所以`f(x)`函数输出的进程ID都是一样的。
3. 整体耗时16秒左右，因为笔者电脑是16核串行调用`f(x)`函数，并且每次调用都会主动`time.sleep(1)`，所以耗时大于16秒。

#### 多进程改造

接下来，我们通过`multiprocessing`库的提供的进程池`Pool`将代码改为多进程的，代码如下:

```python
from multiprocessing import Pool,cpu_count
import os
import time

def f(x):
    print("进程ID:{} 处理变量X: {}".format(os.getpid(),x))
    time.sleep(1) #主动sleep 1秒
    return x*x

if __name__ == '__main__':
    print('主进程ID:{} CPU 核数:{}'.format(os.getpid(),cpu_count()))
    st_time = time.time()
    arg_nums = list(range(1,cpu_count()+1))
    print(arg_nums)

    # 核心代码
    with Pool(cpu_count()) as p:
        p.map(f, arg_nums)

    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time-st_time))

```

运行截图如下: ![img](https://cdn.py2fun.com/course/spider/04/img/2020-10-20-22-54-50.png) 通过输出我们发现：

1. `f(x)`函数中打印的进程ID 都不一样，说明是多进程运行了；
2. 整体耗时从16s多变为了1s多，说明代码并发执行了；

其中最核心的是引入了`multiprocessing`模块的进程池`Pool`，并且通过如下代码让其发挥作用:

```python
    with Pool(cpu_count()) as p:
        p.map(f, arg_nums)
```

知识点如下:

1. `Pool(cpu_count())` 设置进程池的大小，我们此处直接传入了cpu的核数作为进程池大小；
2. `arg_nums`此处为一个列表，值为`[1, 2, 3,..., 16]`；
3. `with Pool(cpu_count()) as p`语句获取到了一个进程池的变量`p`（思考下 `p=Pool(cpu_count())`可以吗?）,通过上下文管理`with`语句可以让Python处理一些异常问题。
4. `p.map(f, arg_nums)` 此语句第一个参数`f` 表示需要并发调用的函数，`arg_nums` 表示需要逐个传入的参数，正确设置后`Pool`会自动取出`arg_nums`中的每一个值，并传入到`f`中。

Good Job! 有了上述知识基本可以改造前一节的代码了，让其变为多进程运行的。

如果你想进一步了解`multiprocessing`库，建议到[官方文档](https://docs.python.org/zh-cn/3/library/multiprocessing.html#multiprocessing.pool.Pool)进行查看。

#### 多进程模型

##### 前景要提

前一节简单介绍了`multiprocessing`库中`Pool`进程池，如何实现一个多进程的简单代码:

```python
from multiprocessing import Pool,cpu_count
import os
import time

def f(x):
    print("进程ID:{} 处理变量X: {}".format(os.getpid(),x))
    time.sleep(1) #主动sleep 1秒
    return x*x

if __name__ == '__main__':
    print('主进程ID:{} CPU 核数:{}'.format(os.getpid(),cpu_count()))
    st_time = time.time()
    arg_nums = list(range(1,cpu_count()+1))
    print(arg_nums)

    # 核心代码
    with Pool(cpu_count()) as p:
        p.map(f, arg_nums)

    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time-st_time))

```

假设我们有一个单进程的爬虫代码如下:

```python
import os
import time

class Spider(object):
    def __init__(self, urls):
        self.urls = urls

    def go(self):
        # 核心抓取流程
        print("进程ID:{}".format(os.getpid()))
        for url in self.urls:
            print('正在抓取:{}'.format(url))
            time.sleep(1)

if __name__ == '__main__':

    st_time = time.time()
    spider = Spider(['http://xxx.com','http://yyy.com','http://zzz.com','http://aaa.com'])
    spider.go()

    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time-st_time))
```

##### 多进程爬虫

如果要改为多进程的方式，一种很简单的思路就是抓取的网址列表分为多个，然后申请多个`Spider`对象进行抓取，比如下面的:

```python
import os
import time

class Spider(object):
    def __init__(self, urls):
        self.urls = urls

    def go(self):
        # 核心抓取流程
        print("进程ID:{}".format(os.getpid()))
        for url in self.urls:
            print('正在抓取:{}'.format(url))
            time.sleep(1)

if __name__ == '__main__':

    st_time = time.time()
    urls = ['http://xxx.com','http://yyy.com','http://zzz.com','http://aaa.com']

    # 爬虫1
    spider1 = Spider(urls[0:2])
    spider1.go()

    # 爬虫2
    spider2 = Spider(urls[2:4])
    spider2.go()

    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time-st_time))

```

我们申请了两个爬虫对象`spider1`和`spider2`，但本质上他们还是在一个进程里面运行的（进程ID一样）。

顺着这个思路，我们就可以引入`multiprocessing.Pool` 通过进程池的方式多进程运行，如下代码:

```python
from multiprocessing import cpu_count,Pool
import os
import time

class Spider(object):
    def __init__(self, urls):
        self.urls = urls

    def go(self):
        # 核心抓取流程
        print("进程ID:{}".format(os.getpid()))
        for url in self.urls:
            print('正在抓取:{}'.format(url))
            time.sleep(1)

def spider_func(urls):
    spider = Spider(urls)
    spider.go()

if __name__ == '__main__':

    st_time = time.time()
    urls = [
        ['http://xxx.com','http://yyy.com'],
        ['http://zzz.com','http://aaa.com']
    ]

    with Pool(cpu_count()) as p:
        p.map(spider_func,urls)

    et_time = time.time()
    print("结束了, 耗时:{}秒".format(et_time-st_time))

```

注意，多次运行后会发现输出可能乱序，但是时间从之前的4s变为了2s，并且进程ID不一样的，说明我们改造成功了！

##### 多进程和多线程的区别

> 多进程和多线程是并发编程的两种主要方式。它们的核心区别在于运行的方式、资源的分配和适用场景。以下是它们的详细对比：
>
> ---
>
> ## **1. 定义**
> - **多进程 (Multiprocessing)**：
>   - 每个进程有自己独立的内存空间和资源，运行在独立的地址空间中。
>   - 进程之间通过进程间通信（如管道、队列）来共享数据。
>
> - **多线程 (Multithreading)**：
>   - 多个线程共享一个进程的内存空间（地址空间）和资源。
>   - 线程之间可以直接访问共享的变量。
>
> ---
>
> ## **2. 内存和资源**
> - **多进程**：
>   - 独立的内存和资源，互不干扰。
>   - 数据需要显式传递，增加通信开销。
>   - 更加稳定，不容易因为一个进程崩溃影响整个程序。
>
> - **多线程**：
>   - 共享内存和资源，通信更高效。
>   - 容易出现数据竞争和死锁问题，需要使用同步机制（如锁）。
>
> ---
>
> ## **3. 执行模式**
> - **多进程**：
>   - 每个进程由操作系统调度，可以在多核 CPU 上并行运行。
>   - 适合 CPU 密集型任务（如矩阵计算、大量科学计算）。
>
> - **多线程**：
>   - 线程由进程内部调度，通常受限于 Python 的 GIL（全局解释器锁）。
>   - 在 Python 中，线程更适合 I/O 密集型任务（如网络请求、文件读写）。
>
> ---
>
> ## **4. 性能**
> - **多进程**：
>   - 开销较大，每个进程需要独立的内存和上下文切换。
>   - 启动较慢，但可以充分利用多核 CPU。
>
> - **多线程**：
>   - 启动和上下文切换的开销较小，效率更高。
>   - 因 GIL 的存在，Python 的线程在计算密集型任务中无法完全并行。
>
> ---
>
> ## **5. 适用场景**
> - **多进程**：
>   - CPU 密集型任务，例如：
>     - 数据科学中的大规模计算。
>     - 图像处理、视频编码。
>     - 深度学习训练。
>
> - **多线程**：
>   - I/O 密集型任务，例如：
>     - 网络爬虫。
>     - 数据库查询。
>     - 文件 I/O 操作。
>
> ---
>
> ## **6. 稳定性**
> - **多进程**：
>   - 单个进程崩溃不会影响其他进程。
>   - 程序更健壮，但调试和维护难度较大。
>
> - **多线程**：
>   - 一个线程崩溃可能会影响整个进程。
>   - 数据共享更容易出错，需要谨慎同步。
>
> ---
>
> ## **7. Python 中的限制**
> - **GIL (Global Interpreter Lock)**：
>   - GIL 是 Python 解释器的一个互斥锁，保证同一时间只有一个线程执行 Python 字节码。
>   - GIL 限制了 Python 多线程在计算密集型任务中的并发性能。
>   - 多进程不受 GIL 的限制，可以真正并行运行。
>
> ---
>
> ## **对比表**
>
> | **特性**        | **多进程**           | **多线程**         |
> | --------------- | -------------------- | ------------------ |
> | **内存空间**    | 独立的内存空间       | 共享的内存空间     |
> | **开销**        | 高启动和切换开销     | 低启动和切换开销   |
> | **资源共享**    | 数据需要通过通信共享 | 可直接访问共享资源 |
> | **适用场景**    | CPU 密集型任务       | I/O 密集型任务     |
> | **稳定性**      | 稳定，不易影响全局   | 易受其他线程影响   |
> | **受 GIL 限制** | 否                   | 是                 |
>
> ---
>
> ## **总结**
> - 如果你的任务是 **CPU 密集型**（需要高计算性能），选择 **多进程**。
> - 如果你的任务是 **I/O 密集型**（如网络请求、文件操作），选择 **多线程**。
> - 如果你使用的是 Python，GIL 是一个重要的因素，需要结合实际任务选择适当的并发模型。

### 爬取豆瓣

##### 网站分析

1.书籍标签页

[豆瓣所有书籍标签](https://book.douban.com/tag/?view=cloud)

 <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-12-04-21.png" alt="img" style="zoom:33%;" />

2.书籍列表页

[编程](https://book.douban.com/tag/编程)

 <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-18-19-53.png" alt="img" style="zoom:33%;" />

3. 书籍详情信息

[Python编程](https://book.douban.com/subject/26829016/) 

<img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-16-37-40.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-16-38-03.png" alt="img" style="zoom: 33%;" />

经过上面的分析，我们的抓取思路就很简单了:

1. 先抓取标签信息；
2. 根据标签信息抓取标签书籍列表信息；
3. 根据第2步的书籍列表信息 抓取数据详情信息；
4. 最终将书籍详情信息保存到本地csv文件中。

##### 爬取标签页

本小节抓取的网页为[豆瓣所有书籍标签](https://book.douban.com/tag/?view=cloud)

 <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-12-04-21.png" alt="img" style="zoom:33%;" />

目标将标签数据抓取出来，并写到到一个csv文件里面，格式如下:

```
name,num,href
小说,6695105,/tag/小说
历史,2866695,/tag/历史
...
```

首先浏览器打开右键，审查下元素如图: <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-12-05-23.png" alt="img" style="zoom:33%;" />

通过审查元素，我们发现，标签在源码的HTML代码类似如下:

```html
<table class="tagCol">
    <tbody>
        <tr>
        <td><a href="/tag/小说">小说</a><b>(6694973)</b></td>
        <td><a href="/tag/历史">历史</a><b>(2866618)</b></td>
        </tr>
        ...
    </tbody>
</table>
```

仔细观察我们发现:

1. 单个书籍标签元素类似`<a href="/tag/小说">小说</a><b>(6694973)</b>`;
2. 所有书籍标签位于`<table class="tagCol"></table>` 里面。

有了上述信息，基本就可以通过`BeautifulSoup`库提取出所有需要的数据了。

```python
import requests
import time
import csv
import re

class SpiderDoubanTag(object):

    def __init__(self,csv_file_path):

        self.target = 'https://book.douban.com/tag/?view=cloud'
        # 统一的请求头
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36',
        }
        self.all_tag_infos = []
        # 保存书籍信息的csv文件地址
        self.tag_csv_path = csv_file_path

    def spider_tag(self):
        req = self.try_request_url(self.target)
        html_content = req.text

        # <a href="/tag/哲学">哲学</a><b>(1624525)</b>
        rets = re.findall('<a href="(.*)">(.*)</a><b>(.*)</b>', html_content)
        for item in rets:
            # ('/tag/爱情', '爱情', '(1273547)')
            # print(item)
            tmp = {
                'name': item[0].strip(),
                'href': item[1].strip(),
                'num':  item[2].strip()[1:-1],
            }
            self.all_tag_infos.append(tmp)


    def try_request_url(self, url):
        """
        统一请求网页的函数，如果请求出错进行至多尝试3次的处理
        """
        try_num = 3
        i = 0
        req = None
        while i < try_num:
            req = requests.get(url, headers=self.headers)
            if req.status_code == 200:
                return req
            print(req.status_code,'尝试重新抓取', url)
            print(req.text)
            time.sleep(2)
            i += 1
        return req

    def save_to_csv(self):
        """
        保存到本地的csv文件中
        """
        csv_datas = self.all_tag_infos
        with open(self.tag_csv_path, 'w', encoding="utf-8") as output_csv:
            fields = [ 'name', 'num','href' ]
            output_writer = csv.DictWriter(output_csv, fieldnames=fields)
            output_writer.writeheader()
            for item in csv_datas:
                output_writer.writerow(item)

    def go(self):

        print("开始抓取")
        # 循环抓取每页数据 并且解析书籍信息
        self.spider_tag()

        print("共抓取书籍标签%s个" % (len(self.all_tag_infos)))
        # 保存数据到csv
        self.save_to_csv()
        print("结束抓取，数据保存到%s中" % self.tag_csv_path)



if __name__ == '__main__':

    spider = SpiderDoubanTag('./douban_tags.csv')
    spider.go()

```

##### 爬取书籍列表页

前一节我们抓取了豆瓣所有的书籍标签信息，解析来我们抓取某个标签下所有书本列表信息。 我们以抓取[**编程**](https://book.douban.com/tag/编程)标签下所有书本信息作为示例:

<img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-15-08-54.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-15-09-14.png" alt="img" style="zoom:33%;" />

目标是将列表页中所有的书本基础信息抓取下来，并保存到csv文件中，格式如下:

```html
book_id,book_title,rate_score,rate_nums,book_pub_info,book_desc,book_url,pic_url
26829016,Python编程,9.1,(3307人评价),[美] 埃里克·马瑟斯 / 袁国忠 / 人民邮电出版社 / 2016-7-1 / 89.00元,本书是一本针对所有层次的Python 读者而作的Python 入门书。全书分两部分：第一部分介绍用Python 编程所必须了解的基本概念，包括matplot...,https://book.douban.com/subject/26829016/,https://img9.doubanio.com/view/subject/s/public/s28891775.jpg
4822685,编码,9.3,(3405人评价),[美] Charles Petzold / 左飞、薛佟佟 / 电子工业出版社 / 2010 / 55.00元,本书讲述的是计算机工作原理。作者用丰富的想象和清晰的笔墨将看似繁杂的理论阐述得通俗易懂，你丝毫不会感到枯燥和生硬。更重要的是，你会因此而获得对计算机工作原理...,https://book.douban.com/subject/4822685/,https://img3.doubanio.com/view/subject/s/public/s27331702.jpg
...
```

还是先分析下列表页的页面结构信息: <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-15-12-35.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-15-13-25.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-15-16-42.png" alt="img" style="zoom:33%;" />

观察上图发现:

1. 单本书的信息都在`<li class="subject-item">`里面。
2. 该页面的书本基础信息有:标题、详情链接、出版社相关信息、评分、评分人数、书本介绍、书封面图地址。
3. 列表页是可以翻页的，翻页参数为`start`，其中第一页`start=0`，第二页`start=20`依次类推。

因为我们需要解析出书本的基础信息，相比前两个实战，解析元素的代码稍微要多点，但基本原理一样。

```python
import requests
from bs4 import BeautifulSoup, Tag
import time
import csv


class SpiderDoubanList(object):
    def __init__(self, tag_name, start_page, end_page, csv_file_path):
        self.page_url_fomart = "https://book.douban.com/tag/%s?start={}&type=T" % (
            tag_name
        )

        # 统一的请求头
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36",
        }
        # 抓取的起始页
        self.start_page = start_page
        self.end_page = end_page
        # 所有的书籍信息
        self.all_book_infos = []
        # 保存书籍信息的csv文件地址
        self.book_info_csv_path = csv_file_path

    def spider_one_page(self, page_num):
        """
        page_num: 传入抓取的页数，从1开始
        某一页的url格式: https://book.douban.com/tag/%E7%BC%96%E7%A8%8B?start=%s&type=T

        其中 page 和 参数start 映射关系如下:
        page=1 => start = 0
        page=2 => start = 20
        page=3 => start = 40
        ...
        page=N => start = (page-1)*20

        函数功能：
        1、请求某一页数据
        2、解析数据，生成书籍的基本信息，注意是多个书籍
        """
        print("开始抓取第{}页数据".format(page_num))
        page_url = self.page_url_fomart.format((page_num - 1) * 20)
        req = self.try_request_url(page_url)

        # 一般都用req.text 如果遇到编码问题可以考虑 req.content.decode('gbk') 之类的方式
        html_content = req.text
        book_infos = self.parse_books_info(html_content)

        print("结束抓取第{}页数据, 正确解析出{}书!\n".format(page_num, len(book_infos)))

        # 通过列表的.extend 加入到self.all_book_infos 中
        self.all_book_infos.extend(book_infos)

    def parse_books_info(self, html_content):
        """
        解析书籍信息
        """
        bs = BeautifulSoup(html_content, "lxml")

        sub_item_tags = bs.find_all("li", {"class": "subject-item"})
        # print(len(sub_items))
        book_infos = []
        for sub_item_tag in sub_item_tags:
            if not isinstance(sub_item_tag, Tag):
                continue

            cur_book_title = ""
            try:
                # 书籍的封面图片
                pic_url = (
                    sub_item_tag.find("div", {"class": "pic"})
                    .find("img")
                    .get("src", "")
                )

                # 解析出书的名字 + 详情链接 + 出版社信息等
                div_book_info_tag = sub_item_tag.find("div", {"class": "info"})
                book_url = (
                    div_book_info_tag.find("h2").find("a").get("href", "")
                )  # 书籍详情信息
                book_title = (
                    div_book_info_tag.find("h2").find("a").get("title", "")
                )  # 书籍标题
                cur_book_title = book_title  # 用于报错信息处理
                book_pub_info = div_book_info_tag.find(
                    "div", {"class": "pub"}
                ).text.strip()  # 书籍作者、出版社等信息

                # 解析出书的评价 评价分数 + 评价人数
                rate_score = (
                    div_book_info_tag.find("div", {"class": "star clearfix"})
                    .find("span", {"class": "rating_nums"})
                    .text.strip()
                )
                rate_nums = (
                    div_book_info_tag.find("div", {"class": "star clearfix"})
                    .find("span", {"class": "pl"})
                    .text.strip()
                )

                # 解析出书的简短简绍
                book_desc = div_book_info_tag.find("p").text.strip()

                # 变为字典 放入book_infos 列表中
                book_info = {
                    "pic_url": pic_url,
                    "book_url": book_url,
                    "book_title": book_title,
                    "book_pub_info": book_pub_info,
                    "rate_score": rate_score,
                    "rate_nums": rate_nums,
                    "book_desc": book_desc,
                    "book_id": book_url.split("subject/")[-1].replace("/", ""),
                }
                book_infos.append(book_info)
            except Exception as e:
                print("error for", cur_book_title)
                print(e)

        return book_infos

    def try_request_url(self, url):
        """
        统一请求网页的函数，如果请求出错进行至多尝试3次的处理
        """
        try_num = 3
        i = 0
        req = None
        while i < try_num:
            req = requests.get(url, headers=self.headers)
            if req.status_code == 200:
                return req
            print(req.status_code, "尝试重新抓取", url)
            print(req.text)
            time.sleep(2)
            i += 1
        return req

    def save_to_csv(self):
        """
        保存到本地的csv文件中
        """

        csv_datas = self.all_book_infos
        with open(self.book_info_csv_path, "w",encoding="utf-8") as output_csv:
            fields = [
                "book_id",
                "book_title",
                "rate_score",
                "rate_nums",
                "book_pub_info",
                "book_desc",
                "book_url",
                "pic_url",
            ]
            output_writer = csv.DictWriter(output_csv, fieldnames=fields)
            output_writer.writeheader()
            for item in csv_datas:
                output_writer.writerow(item)

    def go(self):
        # 循环抓取每页数据 并且解析书籍信息
        for page in range(self.start_page, self.end_page + 1):
            self.spider_one_page(page)

        # 保存数据到csv
        self.save_to_csv()


if __name__ == "__main__":
    # 示例代码仅抓取前5页的数据
    spider = SpiderDoubanList("编程", 1, 5, "./douban_book_list.csv")
    spider.go()
```

##### 爬取书籍详情

接下来我们以单本书[Python编程](https://book.douban.com/subject/26829016/)为例，再看下如何抓取豆瓣单本书的信息。

目标是将下图所示的信息抓取出来: <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-16-37-40.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-16-38-03.png" alt="img" style="zoom:33%;" /> 

相比书籍列表页面，本次我们主要新增了书籍的**价格信息、ISBN、内容简介、作者简介、目录**。

首先还是分析下抓取信息的页面结构: <img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-17-19-37.png" alt="img" style="zoom:33%;" /> ![img](https://cdn.py2fun.com/course/spider/05/img/2020-10-23-17-21-10.png)

观察上图可以发现:

1. 价格及ISBN 结构简单，可以通过正则表达式进行提取；
2. 内容简介在`<div class="indent" id="link-report">`这个div模块里面；
3. 作者简介在内容简介**同级的下一个**`<div class="indent ">`div模块里面；
4. 目录在`<div class="indent" id="dir_26829016_full">`div模块里面，其中`id="dir_26829016_full"` 为动态生成的，这里的`26829016`为书本的ID。

但是部分书籍，可能没有目录 或 作者简介，如下图:

<img src="https://cdn.py2fun.com/course/spider/05/img/2020-10-23-17-31-43.png" alt="img" style="zoom:33%;" />

针对这种缺失信息的情况，我们不再采用`try-except`的方式进行容错处理，而是采用代码判断的方式进行。

```python
from bs4 import BeautifulSoup,Tag
import requests
import re
import json
def parse_by_re( re_str, html_content):
    """
    通过正则表达式进行批评
    ## 解析定价 <span class="pl">定价:</span> 98.00元<br/>
    # 解析ISBN <span class="pl">ISBN:</span> 9787115517197<br/>
    """
    rets = re.findall(re_str,html_content)
    if len(rets) > 0:
        return rets[0].strip()
    return ""

def spider_one_book(book_info):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36',
    }
    html_content =  requests.get(book_info['book_url'], headers=headers).text

    ## 解析定价 <span class="pl">定价:</span> 98.00元<br/>
    price = parse_by_re('<span class="pl">定价:</span>(.*)<br/>', html_content)

    # 解析ISBN <span class="pl">ISBN:</span> 9787115517197<br/>
    isbn = parse_by_re('<span class="pl">ISBN:</span>(.*)<br/>', html_content)

    # 以下使用BeautifulSoup进行解析
    bs = BeautifulSoup(html_content, "lxml")

    # 解析内容简介
    # <div class="indent" id="link-report">
    content_summary_tag = bs.find('div', {'class': 'indent', 'id': 'link-report'})
    content_summary = content_summary_tag.text.strip()

    # 作者介绍
    # <div class="indent ">
    author_summary_tag = content_summary_tag.find_next('div', {'class': 'indent'})
    author_summary = author_summary_tag.text.strip() if author_summary_tag != None else ''

    ## 解析目录信息
    # <div class="indent" id="dir_26829016_full">
    catalog_tag = bs.find('div', {'class': 'indent', 'id': 'dir_%s_full' % book_info['book_id']})
    catalog = catalog_tag.text.strip() if catalog_tag != None else ''

    print(price)
    print(isbn)
    print(content_summary)
    print(author_summary)
    print(catalog)


spider_one_book({
    "book_id": "26829016",
    "book_title": "Python编程",
    "rate_score": "9.1",
    "rate_nums": "(3311人评价)",
    "book_pub_info": "[美] 埃里克·马瑟斯 / 袁国忠 / 人民邮电出版社 / 2016-7-1 / 89.00元",
    "book_desc": "本书是一本针对所有层次的Python 读者而作的Python 入门书。全书分两部分：第一部分介绍用Python 编程所必须了解的基本概念，包括matplot...",
    "book_url": "https://book.douban.com/subject/26829016/",
    "pic_url": "https://img9.doubanio.com/view/subject/s/public/s28891775.jpg"
})
```

此段代码中有几个重点：

1. 价格及ISBN 通过正则提取；
2. `content_summary_tag.find_next` 查找内容简介的下一个同级标签；
3. 作者信息 及 目录信息 判断了是否有标签，如果没有则返回空。

上面代码基本就是抓取单个书籍最核心的代码，接下来我们还是以面向对象的方式整理下，并且加入写入`csv`文件的代码:

```python
import requests
from bs4 import BeautifulSoup, Tag
import time
import re
import csv
import json


class SpiderDoubanBooks(object):
    """
    抓取书籍的详情页 并保存到 csv 文件中
    """

    def __init__(self, book_list_csv_path, books_csv_path):
        """
        spider_book_infos: 需要抓取的书籍列表信息
        boos_csv_path: 保存的书籍详情信息
        """
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36",
        }

        self.spider_book_infos = self.read_csv_file(book_list_csv_path)
        self.books_csv_path = books_csv_path
        self.all_book_infos = []

    def spider_one_book(self, book_info):
        """
        book_info : {
            'book_url': 'https://book.douban.com/subject/26829016/',，
            'book_id': '26829016'
        }
        """
        req = self.try_request_url(book_info["book_url"])

        # 一般都用req.text 如果遇到编码问题可以考虑 req.content.decode('gbk') 之类的方式
        html_content = req.text
        ret = self.parse_book_info(book_info["book_id"], html_content)
        book_info.update(ret)

        self.all_book_infos.append(book_info)

        # print('结束抓取第{}页数据, 正确解析出{}书!\n'.format(page_num,len(book_infos)))

    def parse_by_re(self, re_str, html_content):
        """
        通过正则表达式进行批评
        ## 解析定价 <span class="pl">定价:</span> 98.00元<br/>
        # 解析ISBN <span class="pl">ISBN:</span> 9787115517197<br/>
        """
        rets = re.findall(re_str, html_content)
        if len(rets) > 0:
            return rets[0].strip()
        return ""

    def parse_book_info(self, book_id, html_content):
        """
        解析某本书的信息
        """
        bs = BeautifulSoup(html_content, "lxml")

        ## 解析定价 <span class="pl">定价:</span> 98.00元<br/>
        price = self.parse_by_re('<span class="pl">定价:</span>(.*)<br/>', html_content)

        # 解析ISBN <span class="pl">ISBN:</span> 9787115517197<br/>
        isbn = self.parse_by_re('<span class="pl">ISBN:</span>(.*)<br/>', html_content)

        # 解析内容简绍
        # <div class="indent" id="link-report">
        content_summary_tag = bs.find("div", {"class": "indent", "id": "link-report"})
        content_summary = content_summary_tag.text.strip()

        # 作者介绍
        # <div class="indent ">
        author_summary_tag = content_summary_tag.find_next("div", {"class": "indent"})
        author_summary = (
            author_summary_tag.text.strip() if author_summary_tag != None else ""
        )

        ## 解析目录信息
        # <div class="indent" id="dir_26829016_full">
        catalog_tag = bs.find("div", {"class": "indent", "id": "dir_%s_full" % book_id})
        catalog = catalog_tag.text.strip() if catalog_tag != None else ""

        if author_summary == "" or catalog == "":
            print(book_id)

        return {
            "price": price,  # 价格
            "isbn": isbn,  # ISBN
            "content_summary": content_summary,  # 内容介绍
            "author_summary": author_summary,  # 作者介绍
            "catalog": catalog,  # 目录详情
        }

    def try_request_url(self, url):
        """
        统一请求网页的函数，如果请求出错进行至多尝试3次的处理
        """
        try_num = 3
        i = 0
        req = None
        while i < try_num:
            req = requests.get(url, headers=self.headers)
            if req.status_code == 200:
                return req
            print(req.status_code, "尝试重新抓取", url)
            print(req.text)
            time.sleep(2)
            i += 1
        return req

    def read_csv_file(self, csv_path):
        book_infos = []
        with open(csv_path, "r", encoding="utf-8") as pf:
            cool_csv_dict = csv.DictReader(pf)
            for row in cool_csv_dict:
                book_infos.append(row)
        return book_infos

    def save_to_csv(self):
        """
        保存到本地的csv文件中
        """

        csv_datas = self.all_book_infos
        with open(self.books_csv_path, "w", encoding="utf-8") as output_csv:
            # 需要写入的字段
            fields = [
                "book_id",
                "book_title",
                "price",
                "isbn",
                "rate_score",
                "rate_nums",
                "book_url",
                "pic_url",
                "book_pub_info",
                "book_desc",
                "content_summary",
                "author_summary",
                "catalog",
            ]
            output_writer = csv.DictWriter(output_csv, fieldnames=fields)
            output_writer.writeheader()
            # print(len(output_writer.fieldnames),output_writer.fieldnames)
            for item in csv_datas:
                # print(item)
                # print(item.get('rate_score',''))
                # print(item.get('isbn',''))
                output_writer.writerow(item)

    def go(self):
        # 循环抓取每页数据 并且解析书籍信息
        all_book_num = len(self.spider_book_infos)
        for i, book_info in enumerate(self.spider_book_infos):
            print(
                "进度:[{}/{}] 开始抓取书:{} {}".format(
                    i + 1, all_book_num, book_info["book_title"], book_info["book_url"]
                )
            )
            self.spider_one_book(book_info)

        # 将数据保存到csv文件中
        self.save_to_csv()

        print("结束抓取，书籍相信信息保存到了{}".format(self.books_csv_path))


if __name__ == "__main__":
    spider = SpiderDoubanBooks("./douban_book_list.csv", "./books_detail.csv")
    spider.go()
```

##### 总结

通过本章抓取豆瓣标签、标签列表页、书籍详情的学习我们收获了如下技能:

- `requests`请求携带`headers`参数以模拟浏览器请求；
- 通过正则匹配`re.findall()`来解析简单的元素数据。
- 对于部分页面不一定具备所有元素信息，此时可以加`try-except`进行容错处理；
- 可以判断`BeautifulSoup`解析返回的对象是否为`None`来判断是否存在元素；
- 如何读取`csv`文件，作为爬虫的输出。
- 将豆瓣爬虫分为三个步骤：抓取标签、抓取列表、抓取详情

至此，通过三个爬虫实例**抓取小说**、**抓取手机壁纸**、**抓取豆瓣书籍** 的介绍及编程实战，相信你基本已经掌握了爬虫实现的基本原理：

1. `requests`库模拟网页请求；
2. `re`模块 和 `BeautifulSoup` 解析网页 提前元素内容；
3. 如何合理的将爬虫分为多个阶段进行爬取；
4. 借助多进程提高爬虫的速度。

### 进阶知识

#### 反爬虫策略

通过前面的三个小爬虫实例**抓取小说**、**抓取手机壁纸**、**抓取豆瓣书籍**，我们基本掌握了如何实现一个爬虫，核心点如下:

- 借助浏览器审查网页元素，观察网页结构；
- 使用`requests`库请求网页，并且加入`headers`模拟浏览器发起请求；
- 结合`BeautifulSoup`或正则表达式模块`re`解析网页提取元素内容;
- 使用`csv`模块将爬虫数据保存到本地文件中；
- 使用多进程模块`multiprocessing`结合进程池`Pool`加速整个爬虫效率；

虽然使用上面的技能点可以抓取大部分网站，但现实世界是复杂的，特别是随着大家对数据隐私意识的加强，大部分网站都采用了一些**反爬虫策略**来应对爬虫的抓取。

本小节，主要介绍一些常见的网站**反爬虫**策略，只有知道了这些反爬虫策略我们才能更好的写出爬虫代码以逐一应对这些策略。

接下来，假设下你正在编写一个网站，但你不想让你团队辛苦生产的内容被爬虫白嫖，你会怎么做了？

停顿3秒，思考下。此时估计你会自然地想到下面几个策略：

1. 验证用户身份，核心数据仅能部分用户访问，如腾讯视频的VIP 会员才能看付费内容；
2. 限制请求频率，检测到异常访问的直接加入黑名单，或者返回`403 Forbidden` 错误等；
3. 检测请求是否为人类发起，加入各种图形验证码，如12306的火车票验证码；
4. 验证是否为正常浏览器访问的请求；
5. 部分数据返回加密的，即便抓取了也不能简单的解密；

其实上面的策略就是大部分网站使用反爬虫的策略，毕竟如果策略过于严格，很容易进行误判，影响正常用户访问。

总结起来，反爬虫策略主要三个方向:

- 基于**身份识别**进行反爬
- 基于**爬虫行为**进行反爬
- 基于**数据加密**进行反爬

下面，我们分别对三个方向**身份识别**、**爬虫行为**、**数据加密**，进行深入的介绍。

##### 通过身份识别来反爬虫

进行网络请求时，身份识别信息是在网络的网络请求的`headers`中，如下图: <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-15-45-18.png" alt="img" style="zoom:33%;" /> `headers`中有很多字段，这些字段都有可能会被对方服务器拿过来进行判断是否为爬虫。下面我们将一一介绍。

1. 通过headers中的User-Agent字段来反爬

- 反爬原理：爬虫默认情况下没有User-Agent，而是使用模块默认设置
- 解决方法：请求之前添加User-Agent即可；更好的方式是使用User-Agent池来解决（收集一堆User-Agent的方式，或者是随机生成User-Agent）

1. 通过referer字段或者是其他字段来反爬

- 反爬原理：爬虫默认情况下不会带上referer字段，服务器端通过判断请求发起的源头，以此判断请求是否合法
- 解决方法：添加referer字段

1. 通过cookie来反爬

- 反爬原因：通过检查cookies来查看发起请求的用户是否具备相应权限，以此来进行反爬
- 解决方案：进行模拟登陆，成功获取cookies之后在进行数据爬取

**通过请求参数来反爬**

请求参数的获取方法有很多，向服务器发送请求，很多时候需要携带请求参数，通常服务器端可以通过检查请求参数是否正确来判断是否为爬虫

1. 通过从html静态文件中获取请求数据(github登录数据)

- 反爬原因：通过增加获取请求参数的难度进行反爬
- 解决方案：仔细分析抓包得到的每一个包，搞清楚请求之间的联系

1. 通过发送请求获取请求数据

- 反爬原因：通过增加获取请求参数的难度进行反爬
- 解决方案：仔细分析抓包得到的每一个包，搞清楚请求之间的联系，搞清楚请求参数的来源

1. 通过js生成请求参数

- 反爬原理：js生成了请求参数
- 解决方法：分析js，观察加密的实现过程，通过js2py获取js的执行结果，或者使用selenium来实现

1. 通过验证码来反爬

- 反爬原理：对方服务器通过弹出验证码强制验证用户浏览行为
- 解决方法：打码平台或者是机器学习的方法识别验证码，其中打码平台廉价易用，更值得推荐

##### 通过爬虫行为来反爬虫

爬虫的行为与普通用户有着明显的区别，爬虫的请求频率与请求次数要远高于普通用户，此时一般通过如下手段反爬:

1. 通过请求ip/账号单位时间内总请求数量进行反爬

- 反爬原理：正常浏览器请求网站，速度不会太快，同一个ip/账号大量请求了对方服务器，有更大的可能性会被识别为爬虫
- 解决思路：对应的通过购买高质量的ip的方式能够解决问题/购买个多账号

2, 通过同一ip/账号请求之间的间隔进行反爬

- 反爬原理：正常人操作浏览器浏览网站，请求之间的时间间隔是随机的，而爬虫前后两个请求之间时间间隔通常比较固定同时时间间隔较短，因此可以用来做反爬
- 解决思路：请求之间进行随机等待，模拟真实用户操作，在添加时间间隔后，为了能够高速获取数据，尽量使用代理池，如果是账号，则将账号请求之间设置随机休眠

1. 通过对请求ip/账号每天请求次数设置阈值进行反爬

- 反爬原理：正常的浏览行为，其一天的请求次数是有限的，通常超过某一个值，服务器就会拒绝响应
- 解决思路：对应的通过购买高质量的ip的方法/多账号，同时设置请求间随机休眠

##### 基于数据加密进行反爬

通常的特殊化处理主要指的就是css数据偏移/自定义字体/数据加密/数据图片/特殊编码格式等，比如:

1. 自定义字体来反爬(下图来自猫眼电影电脑版):

<img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-15-55-12.png" alt="img" style="zoom:33%;" />

- 反爬思路: 使用自有字体文件
- 解决思路：切换到手机版/解析字体文件进行翻译

1. 通过css来反爬(下图来自猫眼去哪儿电脑版):

<img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-15-55-38.png" alt="img" style="zoom:33%;" />

- 反爬思路：源码数据不为真正数据，需要通过css位移才能产生真正数据
- 解决思路：计算css的偏移

1. 通过js动态生成数据进行反爬

- 反爬原理：通过js动态生成
- 解决思路：解析关键js，获得数据生成流程，模拟生成数据

1. 通过编码格式进行反爬

- 反爬原理: 不适用默认编码格式，在获取响应之后通常爬虫使用utf-8格式进行解码，此时解码结果将会是乱码或者报错
- 解决思路：根据源码进行多格式解码，或者真正的解码格式

#### 爬虫模拟登录

这一节，主要介绍爬虫模拟登陆的2种常用方法:

- `POST`请求：获取登录的URL并填写请求体参数，然后 POST 请求登录，相对麻烦；
- 添加`Cookies`：先登录将获取到的 Cookies 加入 Headers 中，最后用所以得网络请求都带上次cookie，这种最为方便；

##### POST模拟登陆

下面，我们以[豆瓣网](https://accounts.douban.com/passport/login)登录并访问[我的订单页](https://www.douban.com/mine/orders/)为例，用代码分别实现上述2种方法。

[豆瓣登陆页](https://accounts.douban.com/passport/login?source=group)，我们通过浏览器控制台，观察网络请求如图: <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-20-14-49.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-20-50-58.png" alt="img" style="zoom:33%;" /> <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-20-52-31.png" alt="img" style="zoom:33%;" />

这里**建议故意输入错误**的密码，这样就不会因为页面跳转而捕捉不到请求！通过上图得知:

1. 登陆url为`https://accounts.douban.com/j/mobile/login/basic` 且是一个`POST`类型请求；
2. `post`传递表单为`ck`、`remember`、`name`、`password`，其中用户名、密码是我们关注的。

有了上面信息，我们直接使用`requests`库发起`post`请求即可，代码如下:

```python
import requests
def login_douban(account, password):
    url = "https://accounts.douban.com/j/mobile/login/basic"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36",
        "Referer": "https://accounts.douban.com/passport/login_popup?login_source=anony",
    }
    data = {
        "ck": "",
        "name": account,
        "password": password,
        "remember": "true",
    }
    req = requests.post(url, headers=headers, data=data)
    print(req.text)

# 故意输错密码
login_douban('12','12')
```

运行后输出为:

```
{"status":"failed","message":"unmatch_name_password","description":"用户名或密码错误","payload":{}}
```

因为示例代码故意输出了信息，如果将你调用函数`login_douban`传入正确的用户名、密码后，会得到如下结果: <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-21-01-53.png" alt="img" style="zoom:33%;" />

说明登陆正确了，但有可能你会遇到另外一种情况（即便密码、用户名正确），返回为提示需要验证码: <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-21-03-44.png" alt="img" style="zoom:33%;" /> 

如果你在浏览器里点击登陆，对应的效果便是提示滑动验证码 <img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-21-06-49.png" alt="img" style="zoom:33%;" /> 

对于此种比较变态的二次验证，我们先不做处理(毕竟涉及到图形识别 及 滑动验证)。

> 据笔者经验，如果你本机运行遇到这个问题，可以在浏览器端手动退出豆瓣 然后再次登录，登录成功后 再重新运行程序会发现成功。

我们先假定，上面的登录程序不会触发二次验证，但是即便返回登录成功也只是第一步，因为我们最终想获取的是[我的订单页](https://www.douban.com/mine/orders/)。

这里我们直接引入一个新的知:**session会话**!

> `requests`库的`session`会话对象可以**跨请求**保持某些参数，说白了，就是比如你使用`session`成功的登录了某个网站，则在再次使用该`session`对象求求该网站的其他网页都会默认使用该`session`之前使用的`cookie`等参数。

> 尤其是在保持登陆状态时运用的最多，在某些网站抓取，或者app抓取时，有的需强制登陆，有的不登陆返回的数据就是假的或者说是不完整的数据，那我们不可能去做到每一次请求都要去登陆一下怎么办，就需要用到保持会话的功能了，我们可以只登陆一次，然后保持这种状态去做其他的或者更多的请求。

使用`requests.Session`可以很简单的实现多个请求之间保存`cookie`，完整代码如下:

```python
import requests

class DouBanLogin(object):
    def __init__(self, account, password):
        self.url = "https://accounts.douban.com/j/mobile/login/basic"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36",
            "Referer": "https://accounts.douban.com/passport/login_popup?login_source=anony",
        }
        self.data = {
            "ck": "",
            "name": account,
            "password": password,
            "remember": "true",
        }
        self.session = requests.Session()

    def get_cookie(self):
        """模拟登陆获取cookie"""
        html = self.session.post(
            url=self.url,
            headers=self.headers,
            data=self.data
        ).json()
        if html["status"] == "success":
            print("恭喜你，登陆成功")
        else:
            print("登录失败")
            print(html)

    def get_user_order(self):
        url = "https://www.douban.com/mine/orders/"
        req = self.session.get(url, headers=self.headers)
        print(req.status_code)
        print(req.text)

    def run(self):
        """运行程序"""
        self.get_cookie()
        self.get_user_order()


if __name__ == '__main__':
    # 请将用户名、密码换为你自己的豆瓣账号密码
    account = '12' #input("请输入你的账号:")
    password = '12' #input("请输入你的密码:")
    login = DouBanLogin(account, password)
    login.run()
```

如果一切顺利，上面代码输出为:

```
恭喜你，登陆成功
200
<!DOCTYPE html>
<html lang="zh-CN" class="ua-mac ua-webkit">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="renderer" content="webkit">
    <meta name="referrer" content="always">
    <meta name="google-site-verification" content="ok0wCgT20tBBgo9_zat2iAcimtN4Ftf5ccsh092Xeyw" />
    <title>
      我的订单
</title>
...
```

其中最核心的代码如图:

<img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-21-33-35.png" alt="img" style="zoom:33%;" />

1. 通过`requests.Session()`申请一个会话对象`self.session`;
2. 使用申请的`self.session`发起登录`post`请求;
3. 使用相同的`self.session`发起我的订单页`get`请求，这样如果第2步登录成功后，相同会话对象就可以保持`cookie`等信息，从而达到以登录状态访问订单页。

另外，上面代码中发起请求的对象变成了`requests.Session`对象，它和原来的`requests`对象发起请求方式一样，只不过它每次请求会自动带上`Cookie`，所以后面我们都用`requests.Session`对象来发起请求！

##### 设置Cookies模拟登录

之前的方法，需要去获取登录POST请求链接和参数，比较麻烦。

下面，可以尝试先**手动登录**，获取 Cookie，然后将该 Cookie 添加到 Headers 中去，之后用 GET 方法请求即可，过程简单很多。

获取`Cookie`比较简单，读者可以手动登录豆瓣后，打开控制台，观察网络请求，复制里面的`cookie`，如图:

<img src="https://cdn.py2fun.com/course/spider/06/img/2020-10-25-21-52-23.png" alt="img" style="zoom:33%;" />

有了`Cookie`后，就可以直接设置请求头`headers` 直接发起请求了，代码如下:

```python
import requests
def get_user_order_with_cookie():
    # 运行时请修改为你自己的cookie
    cookie = "请换成你自己的cookie"

    url = "https://www.douban.com/mine/orders/"

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.80 Safari/537.36",
        'Cookie': cookie
    }

    req  = requests.get(url, headers=headers)
    print(req.status_code)
    print(req.text)

get_user_order_with_cookie()
```

如果替换代码中的cookie为正确的值后，运行后部分输出为:

```
200
<!DOCTYPE html>
<html lang="zh-CN" class="ua-mac ua-webkit">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="renderer" content="webkit">
    <meta name="referrer" content="always">
    <meta name="google-site-verification" content="ok0wCgT20tBBgo9_zat2iAcimtN4Ftf5ccsh092Xeyw" />
    <title>
      我的订单
</title>
...
```

说明我们带上了登陆凭证cookie，并且正确获取了[我的订单](https://www.douban.com/mine/orders/)页面你的源码

可以看到，添加了`Cookie`后就不用再`POST`登录请求了，直接 `GET` 请求目标网页即可。

思考下：`Cookie`是不是一个网站用户唯一的登陆凭证了，有没有其他了？

> 答案：大部分网站采用`Cookie`作为用户登陆凭证，但随即技术的发展特别是App的流行，也有部分网站采用[`JWT`](https://jwt.io/)的方式进行验证，甚至一些专业的站点也自己的一套登陆验证算法，但核心浏览器发起请求时**肯定需要携带此用户标识**，其要么在请求参数里中，要么在请求`headers`里面的一个参数，出此无它了。

#### 爬虫代理

我们在使用爬虫的时候，经常遇到一种情况，刚开始的运行的时候，都如丝般顺滑，可能一杯茶的功夫，就被打脸了，可能会出现各种各样的限制，比如 `403 Forbidden` 、 `429 Too Many Request` 等等。

这时候，很有可能就是我们的 **IP 被限制**了。

出现以上问题一般是因为网站的安全限制或者是机房的安全限制，有时候是在服务器上做检测，有时候是在网关处做检测，一旦发现某个 IP 在单位时间内的访问次数超过了当前限定的某个阀值，就会直接拒绝服务，这种情况我们统称为：**封 IP** 。

对于上面这种情况难道我们就束手就擒吗？当然不！

代理就是为了解决这个问题的。其核心思路为:

> 请求中间增加的代理服务器做转发，本来请求是由 A 直接访问到服务器 C 的，如： A -> C ，加了代理 B 之后就变成了这个样子： A -> B -> C

所以设置爬虫代理，变为了如下几个核心问题:

1. 如何获取代理，毕竟你本地的电脑一时间只有一个IP；
2. 如何设置`requests`库请求网页使用代理；

##### 代理获取

在做实战之前，我们先了解下如何获取代理。

首先在百度上输入 “代理” 两个字进行查询，可以看到有很多提供代理服务的网站，当然哈，大多数都是收费的。但是其中不乏会有一部分免费的代理。

当然如果想要获得稳定的、网络延时低的代理服务，建议付费使用，毕竟用的少也花不了多少钞票。

代理的站点笔者就不列举了，是在太多，我们随便打开一个免费代理的网站： ![img](https://cdn.py2fun.com/course/spider/06/img/2020-10-26-16-30-31.png)

可以发现，代理好像分两种，一种是**高匿代理**，还有一种是**透明代理**，这两个有什么区别呢？

其实，除了高匿代理和透明代理以外，还有一种中间形态叫做**匿名代理**。

这几种代理之间的区别在于转发请求的头部参数不同。

**透明代理**

目标服务器可以知道我们使用了代理，并且也知道我们的真实 IP 。 透明代理访问目标服务器所带的 HTTP 头信息如下：

- REMOTE_ADDR = 代理服务器IP
- HTTP_VIA = 代理服务器IP
- HTTPXFORWARDED_FOR = 我们的真实IP

透明代理还是将我们的真实IP发送给了对方服务器，因此无法达到隐藏身份的目的

**匿名代理**

目标服务器可以知道我们使用了代理，但不知道我们的真实 IP 。匿名代理访问目标服务器所带的 HTTP 头信息如下：

- REMOTE_ADDR = 代理服务器IP
- HTTP_VIA = 代理服务器IP
- HTTPXFORWARDED_FOR = 代理服务器IP

匿名代理隐藏了我们的真实IP，但是向目标服务器透露了我们是使用代理服务器访问他们的。

**高匿代理**

目标服务器不知道我们使用了代理，更不知道我们的真实 IP 。高匿代理访问目标服务器所带的 HTTP 头信息如下：

- REMOTE_ADDR = 代理服务器IP
- HTTP_VIA 不显示
- HTTPXFORWARDED_FOR 不显示

高匿代理隐藏了我们的真实 IP ，同时目标服务器也不知道我们使用了代理，因此隐蔽度最高。 可以看到，处于中间态的匿名代理，事情做了一半没做完，反而是没什么用武之地的。

**总结下**:

看了上面三种代理方式，相信你心中有了答案，要用就用**高匿代理**代理。现在一般的代理网站提供都是**高匿代理**。 ![img](https://cdn.py2fun.com/course/spider/06/img/2020-10-26-16-38-54.png)

笔者收藏了几个免费代理站点:

1. http://www.66ip.cn/
2. http://www.ip3366.net/
3. https://www.kuaidaili.com/free/

##### 代理设置

上面我们看到了一些免费代理服务，接下来我们看一下`requests`如何设置代理。

我们先从一个[免费代理网站](http://www.66ip.cn/)随便获取一个代理地址: ![img](https://cdn.py2fun.com/course/spider/06/img/2023-04-02-12-42-31.png)

使用`requests`配置代码服务只需要类似传递`headers`一样传递一个`proxies`即可，具体代码如下:

```python
import requests

# 白嫖的免费代理
proxies = {
    'http': 'http://27.70.38.29:6666',
    'https': 'https://27.70.38.29:6666',
}
try:
    response = requests.get('https://www.baidu.com', proxies = proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error', e.args)
```

如果耐心等待，可能上面的代码能正确输出百度首页源代码。但大概率是下面的错误输出提示:

```
Error (MaxRetryError("HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x106b64320>: Failed to establish a new connection: [Errno 60] Operation timed out')))"),)
```

说明白嫖的免费代理不可靠，笔者用自己本机搭建的代理服务进行设置:

```python
import requests

# 笔者本机电脑配置的代理服务

proxies = {

  'http': 'http://127.0.0.1:7890',

  'https': 'https://127.0.0.1:7890',

}

try:

  response = requests.get('https://www.baidu.com', proxies = proxies)

  print(response.text)

except requests.exceptions.ConnectionError as e:

  print('Error', e.args)
```



运行后部分输出为:

```
<!DOCTYPE html>
<!--STATUS OK--><html> <head><meta http-equiv=content-type 
...
```

说明代理服务器已经生效了。

但是我们本机搭建的代理，本质上没有啥作用，但白嫖的代理往往一段时间后不可用了。此时如果想继续白嫖的话，需要抓取大量的免费代理，并且做可用性检测，构建一个代理IP池。

但是构建一个高可用的代理IP池是一个**工程活**(需要考虑很多细节)，推荐一个[开源的代理服务](https://github.com/Python3WebSpider/ProxyPool)，并提供了如下功能:

- 定时抓取免费代理网站，简易可扩展。
- 使用 Redis 对代理进行存储并对代理可用性进行排序。
- 定时测试和筛选，剔除不可用代理，留下可用代理。
- 提供代理 API，随机取用测试通过的可用代理。

如果有需要进行大规模抓取网站，大家可以去官网尝鲜体验下。我们这里就不细说了。

不过免费的代理服务器的稳定较差，如果需要稳定的 IP 建议自己购买可换 IP 的 VPS 服务器。在 VPS 上搭建一个代理服务器，定期拨号更换 IP。另外也有供应商在出售代理 IP 服务，一天可以使用数万的 IP 地址，对于绝大多数场景都是够用的。





