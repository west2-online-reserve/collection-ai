
# 爬虫

## 什么是爬虫

  通过编写程序，模拟浏览器上网，然后让其去抓取数据的过程。
## 爬虫在使用场景中的分类
1. 通用爬虫：
 
   抓取系统中的重要部分。抓取的是一整张数据。
2. 聚焦爬虫：
    
    是建立在通用爬虫基础之上。抓取的是页面中特定的内容。
3. 增量式爬虫：
    
    检测网站中数据更新情况。只会抓取网站中最新更新出来的数据。
## 爬虫的矛与盾：
   
   我想爬你，又不想让你爬我。
## 君子协定：

   robot.txt协议.规定网站中哪些可以爬取，哪些不可以爬取
## http协议：
 
   就是服务器和客户端进行数据交互的一种形式。
## 常用请求头信息：
1. User-Agent：请求载体身份标识
2. Connection：请求完毕的连接断开还是保持
## 常用响应头信息： 
1. Content-Type：服务器相应回客户端的数据类型
## https协议：
安全的超文本传输协议
## 加密方式：
1. 对称加密
2. 非对称加密
3. 证书密钥加密
## request模块
1. urlib模块，早期出现，被取缔
2. requests模块：python中原生的一款基于网络请求的模块。 作用：模拟浏览器发送请求。
## 如何使用：
1. 指定url：你想请求的网址
2. 发起请求
3. 获取响应数据
4. 持久化存储
## 任务
- 01：爬取搜狗页面数据
- 02：网页采集器
- 03：破解百度翻译
- 04: 爬取豆瓣电影
- 05：作业：爬取肯德基餐厅

## Task2-爬取福大教务处
- 问题：爬取打开中文乱码。解决：是编码格式问题，用 
   print(response.encoding)  # 查看网页返回的字符集类型
    print(response.apparent_encoding)  # 自动判断字符集类型
查看所需的字符集类型，然后在写入文件时改成相应的类型

## 数据解析分类：
1. 正则
2. bs4
3. xpath

## 数据解析原理概述：

* 解析的局部的文本内容都会在变迁之间或者标签对应的属性中进行存储

1. 进行指定标签的定位
2. 标签或者标签对应的属性中催出的数据值进行提取。
3. 在获取响应数据后进行数据解析，然后进行持久化存储


- xpath解析原理：
1. 实例化一个etree对象，且需要将被解析的页面源码数据加载到对象中
2. 调用etree对象中xpath方法结合着xpath表达式实现标签的定位和内容的捕获。

- 环境安装： pip install lxml
- 如何实例化一个etree对象： from lxml import etree
- 1. 将本地的html文档中的源码数据加载到etree对象中：
1. etree。parse（filePath）
2. 将互联网获取的源码加载到该对象中 ：etree.html（‘page_text’）
3. xpath（‘xpath表达式’）












