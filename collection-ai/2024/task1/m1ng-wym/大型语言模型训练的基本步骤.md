语言模型生成回答是在做一种“文字接龙”：未完成句子 --> 语言模型 --> 生成下一个token

下一个token = f(未完成句子) = ...a...b...c...d...   -->拥有数十亿个未知参数的函式，即模型：Transformer

通过把训练资料交给语言模型进行训练，可以将数十亿个参数找出来
(找参数的挑战：找到的参数可能只适用于特定资料，所以要换多组超参数来尝试)


训练步骤：


第一阶段：Pre-train（Foundation Model）：

    在网络上找到大量文字资料拿来学习“文字接龙”（因为这一阶段人工介入很少，所以叫做Self-supervised Learning 自督导式学习） 

    理论上任何文字资料都可以拿来学“文字接龙”，但要排除不良内容/大量重复的内容的干扰

    经过第一阶段，语言模型已经学了很多东西，但回答的正确率不高，此时需要下一阶段由人类进行的帮助



第二阶段：Instruction Fine-tuning（Alignment）：

    使用人力进行资料标注，给语言模型提供好的回答 （Supervised Learning 督导式学习）

    但如果只靠人力进行资料收集的话，受限于人力成本，效果并不好，所以，要将自督导式学习和督导式学习结合

    关键是以第一阶段的参数作为第二阶段的初始参数：
    先以网络上找到的大量文字资料生成初始参数，再通过instruction fine-tuning，以人类标注的少量资料对初始参数进行最佳化，产生不会和第一阶段初始参数差太多的参数



第三阶段：RLHF -- Reinforcement Learning from Human Feedback 增强式学习 （Alignment）：

    语言模型生成两个答案，若人类认为答案一比答案二更好，则语言模型会微调自己的参数，提高回答出答案一的机率，降低回答出答案二的机率

    RLHF和Instruction Fine-tuning相比：

    在RLHF中，人类更轻松（有时候人类写出正确答案不容易，但容易判断答案的好坏）

    在RLHF中，模型对生成结果做通盘考量（只问结果，不问过程），在Instruction Fine-tuning中，模型要学的就是怎么接下一个字，而对于生成结果没有通盘考量

    回馈模型（Reward Model）：可模仿人类喜好，就能从RLHF转变为RLAIF（但过度向虚拟人类学习是有害的）

    增强式学习面临的难题：
    ·面对safety和helpfulness的抉择
    ·人类自己都无法正确判断回答好坏的情况