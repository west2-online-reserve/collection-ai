标题,详情,回答1,回答2,回答3,回答4,回答5,回答6,回答7,回答8,回答9,回答10
你认为该图片是真实拍摄的还是由AI生成的？,请说明理由（素材来源于网络）,Martian 来自火星 3006 人赞同了该回答 我可以终结这个问题了： 一眼AI，而且绝对是字节家的seedream4.0，而且我还把提示词逆推了出来！ 大部分人可能看不出来，但是对我来说真的太熟悉了，因为豆包生成的随手拍风格的照片看的太多了，就是这个味。但是如果要让我总结到底是什么特征，我确实也说不清楚... 字节的seedream4.0（就是豆包内置的那个），我觉得是国内最最拟真大模型（没有之一），有时候真的能以假乱真，特别是手机随手拍风格，其他任何大模型根本没法做到同样的效果。 我试着生成了几张类似的照片，可以对比一下是不是这个味儿。不信的话，可以把下面这段提示词复制到 即梦 或者是 豆包 上面，记得要选seedream4.0，不是3.0，无需任何原图垫图，直接复制文字生图即可（后附逆推方法）： 提示词如下： iPhone随手拍，宿舍日常场景照片，两位穿着校服的女生坐在双层铁架床的下铺。左侧女生左腿搭床沿、右腿前伸，低头看手机；中间女生双腿弯曲并拢，身体端正。背景为女生宿舍，上铺堆放杂乱衣物，地面散落多双运动鞋、休闲鞋及袜子。低分辨率照片，随手拍，生活化随意氛围，展现宿舍放松日常状态。比例 9:16。 附上我生成的照片（我用的是即梦） （题主给的那张原图有三个人，但是很明显最右边的女孩直接悬空了，在我看来根本就是一张废图，所以我的提示词只写了俩女孩） 关键是！我没有任何垫图，只给了纯文字，简要描述了宿舍场景，但是成果的拍摄角度，特别是床的样子就和原图几乎一模一样，下面附上原图，自己对比一下↓↓↓ 因为我无法知道原图的提示词，所以没办法做到百分百还原。如果觉得有些细节对不上的话，你可以自己修改一下提示词，加上不断抽卡，直到生出相似度99%的样子。 基本上国内的各种大模型我都有用过，相信我，目前国内国外的任何（注意是任何！）大模型都很难做到类似的效果，seedream4.0可以说是最懂中国人拍照风格的，随手拍确实可以做到以假乱真，但是看多了还是能识别出来。 至于这段提示词是怎么推出来的，可以用同样是字节家的 扣子空间 （豆包应该也可以，没试过），把原图上传上去，让它反推出提示词，然后在这基础上修改。其实我也没怎么修改，全程大概只用了5分钟，如果你愿意多花点时间的话可以改得更像。（申明一下，这真不是字节的广告！！字节没给我广告费！！） 哦对，如果你生成了更像的可以发在评论区 ——————分割线—————— 重新生成了一下，同样没有任何垫图，把原图的bug（右边女孩悬空）补上了，给右边的女孩加了一把椅子，嘿嘿 新修改过的提示词如下，更贴近原图，同样无需垫图，直接发文字就行： iPhone随手拍，宿舍日常场景照片，两位穿着校服的女生坐在双层铁架床的下铺。左侧女生左腿搭床沿、右腿前伸，穿校服长裤，光脚，低头看手机；中间女生双腿弯曲并拢，身体端正，穿校服长裤，光脚。右边一个女孩穿着校服坐在紧靠着床边的凳上，面朝左边，光脚。背景为女生宿舍，上铺堆放杂乱衣物，地面是瓷砖，散落多双运动鞋、休闲鞋及袜子。随手拍，生活化随意氛围，展现宿舍放松日常状态。比例9:16 第二版提示词更长了，为了贴近原图，得规定光脚、长裤。但是我相信原图生成时应该没有那么详细，应该是随机抽卡出来的。 ———————— 修改：评论区有些图用seedream画得也不错。如果有其他大模型能做到更真实、更难辨真假，可以看看效果。 还有！不要说我lsp了！！！我是正经人！这些提示词是我把题目原图放到ai里面反推出来的啊 ┑(￣Д ￣)┍ 图片内容涉及到未成年人，劝你们好自为之，不要修改成不好的衣服、姿势或场景哈！ 再更新：喂！比点赞多是怎么回事？？ 免责申明：我只提供思路，由我的提示词修改而生成的图片与我无关！ ———————— 11.23更新： 豆包和即梦每天的seedream都有使用上限，而liblib的包年会员，可以免算力使用一年的seedream 4.0 ！！ 我当时买的时候还没有，今天刚推出的活动（好像到11月28日）用这个生图限制会少一些 LiblibAI·哩布哩布AI - 在线免费生图 ———————— 11月28日更新：Nano banana pro绝了！！！ Nano banana pro生成 这几天Nano banana Pro太火了，跟风搞了一下。虽然放大看还是有很多漏洞，但是乍一看真的很唬人，不是么？然后我把第一版提示词给了banana pro，生出了下面这张图片： Nano banana pro生成，提示词与第一版一致 —————— 11月29日再更新： Z-image非常滴不错，虽然效果不是最好，但是！但是它是开源的！！这点和seedream、banana都不一样，它可以在自己的电脑上跑！而且硬件要求一点都不高，我用3060 12G也跑的动。 回想一下，当时用那些老的大模型，套多少Lora，都难以实现这种效果，现在就可以原图直出，而且还不慢，真的很感慨。我用ComfyUI跑了一张，大家看看效果： Z-image本地生成，提示词与第一版一致 以下是感慨时间—— 最近技术更新迭代得实在是太快了，让人目不暇接。最初跟风玩的sd1.5，到sdxl，然后flux，现在flux2刚推出又马上被阿里的Z-image盖过风头。每个新模型刚推出的时候，都觉得惊为天人，但是过一段时间后，又会觉得不过如此了。 回看刚入坑时，我的那些“得意成果”，粗糙的背景、油腻的皮肤、廉价的风格都是满满的时代感，现在想来其实也不到两年而已。以前知乎也有过类似辨别ai图片的大讨论（当时的图是一个穿着球衣坐在看台的女孩），现在看来也是一眼ai，也就只有在刚出现的时候能够骗骗人罢了。 我们何其有幸，身处这个技术爆发的时代。以至于，我现在每打开一个介绍ai生图的文章，都会习惯性地先滑到最下面看日期——更新太快，日新月异，有些东西早就落伍了。 我在想，短短几个月后，我们再回来看这篇回答，可能会——或者说，一定会觉得——这些目前看起来还有点“难辨真假”的ai照片，就像现在回看一年前那样，也不过如此了吧~ 拭目以待！ ——2025年11月30日 继续追问 由知乎直答提供 生成这张照片用了多少时间？ 这张照片的拍摄角度有多独特？ 其他大模型能否达到seedream4.0的效果？,一支梅向东 286 人赞同了该回答 在医院。iPhone随手拍，宿舍日常场景照片，两位穿着校服的女生坐在双层铁架床的下铺。左侧女生左腿搭床沿、右腿前伸，低头看手机；中间女生双腿弯曲并拢，身体端正。背景为女生宿舍，上铺堆放杂乱衣物，地面散落多双运动鞋、休闲鞋及袜子。低分辨率照片，随手拍，生活化随意氛围，展现宿舍放松日常状态。比例 9:16。,咕噜噜 357 人赞同了该回答 从纹理质量上面来看确实以假乱真了，但是从逻辑上看还是存在明显的漏洞，这几双袜子根本没有一双是能配齐的，ai并不知道袜子是一双一双的。 还有正面两个女生的校服校徽也有明显的区别，难以做到合理地统一。,柳思道​ 努力不受影响 245 人赞同了该回答 肯定是ai 人工智能生成的图片的假，在于太“真”，或者说详略不当，很多地方在真实照片里可能是模糊的，它反而清楚，有点恐怖谷了。 这个图片，我个人觉得算是不错的，但是就觉得别扭。我想了一下，主要在于她的头发这里。就因为我为了表现天气炎热，跟ai提到一个细节：发丝贴在脸上。我本意在于天气热，他却把注意力放在了头发湿。但是真实照片的湿头发会那么清楚吗？ 这个图怎么来的？我是给ai提供了原图，让她总结关键词，在自己慢慢调的，原图是这个。 我觉得，目前的ai图片，总是一眼恐怖谷，所以干脆不要生成现实风格了，二次元啊，手绘风啊，插画风啊，就挺好的。 最近几个都更新了，有些都不太容易能出了。比如这个都不行了。,Python|Cython|量化交易|历史|方言|电商 谢邀 @小乐的乐高世界 353 人赞同了该回答 AI生成的可能性较大。具体分析如下，看每个人的手掌细节。 1.拿着手机的女孩手指细节有些崩坏，放大无法分辨指甲。 2.右边的女孩手指的轮廓AI味太强，缺乏真实感，而校徽的局部细节绘制填充的像素一塌糊涂，没有一个基本的清晰轮廓。 3.从空间看来这是能容乃4人的学生宿舍，正常来说不会有那么多双鞋子的，不符合现实的生活逻辑。 4.床底黑色的鞋子，鞋跟样式不对称。 5.最大的一个bug，右手的女孩坐空气................. 6.中间女鞋校服的袖子长度和露出手臂的长度比例相对右边女孩明显不自然嘛～～要么是AI绘制错误？要么该女选了一件加大码的来穿？身材娇小穿加大码不符合常理呀～～～ 送礼物 还没有人送礼物，鼓励一下,Gary 225 人赞同了该回答 即梦这种模型有个特点，为了保证一致性选择牺牲掉多样性。本质上，即梦生成的脸都是一张脸，仔细看不管男女老幼其实都是一张脸，只有在你明确要求生成其它人种的时候才会给你换脸。问题图中就是典型的即梦三胞胎，ai无疑。,执笔徒将相思写 1260 人赞同了该回答 刚刚用下面某答主的方法用豆包生成的 太阴了,Entropy​ 256 人赞同了该回答 这个太简单了，简单来说，就是通过校服，你居然看不出它是什么学校的，校徽是模糊的一团。像这种拍生活杂物的照片多少都会有一些文字/品牌信息，而这些信息往往是最难补全的，比如下面这张图就绝不会有人怀疑它是ai生成的,Helooty 208 人赞同了该回答 真的很吓人，还原度很高很高，但是还是有些蛛丝马迹可以看出来不是实拍，以下是例子 两个女同学的校服是可以看到校徽的，可以勉强看出来左侧校徽是类似于 1 一样的尖尖的，而右侧校徽是类似于 o 一样圆圆的，同一个寝室校服款式一样但校徽不同，不太可能。 四只袜子款式各一。可能有人会说邋遢一点的就把四双袜子拆开扔地下怎么了？不过看从左到右第二只棕灰色袜子和第四只黑头袜子，生成有瑕疵，第二只袜子没有给脚跟挪位置，第四只袜子有俩头 从左到右第二双鞋明显是想模仿nike的标，但是把nike黑色logo的沟糊到脚后跟了；第三双鞋右侧那坨又像鞋又不像。 此外图片也有不合理之处，比如最左侧躺着的女生俩腿不一样长，比如其他床铺极乱、床下鞋数量超乎想象等等。 综上，我认为这个图完全是ai生成的，连扩图都不是。 但是仿真程度依然恐怖如斯。,Mr Princess 哈利波特/射箭/女装子/网络小说/唱歌/听音乐 143 人赞同了该回答 更新：问了摄影专业的大学老师，在他们专业摄影眼里，这张图的光没有一个地方是对的… 高赞的提示词确实不错，我拿去用gemini的香蕉跑了几次，加了几个提示项，基本上感觉没啥破绽了！
如何看待 AI 真的帮人类做好决策？,AI 已然渗透到了工作、生活的方方面面，越来越多的人都在依赖 AI 帮助梳理思路、评估决策，小到日常吃喝玩乐、大到人生路径理财投资，AI 是否能真的帮助我们做好决策呢？,老九菜​ 身经百战见得多（猫胖） 33 人赞同了该回答 周六晚上B站UP主林亦突然发了一条《AI决战A股》视频，上了热门。视频我瞧了三遍。第一遍看热闹，看他的同事们怎么用AI大战A股玄学；第二遍看门道，看一下他们都用到了AI的哪些方面，既然千问被很多人认为是目前最好的大模型之一，那它的技术边界在哪；而第三遍，我是在看“镜子”，视频里那一群在红绿线之间杀红了眼、心态起起伏伏的年轻人，不就是当年的我，又或是此时此刻正在刷知乎的读者诸君吗？ 也有读者私信问过我：AI真的能帮人类做好投资决策吗？如果你的理解是“把钱给AI，让它把A股当提款机”，那我劝你趁早销户。但如果你愿意透过现象看本质，你会发现这个视频其实是一场极其残忍的社会心理学实验。它揭示了一个让所有“聪明人”绝望的真相： 阻碍你做出正确决策的，从来不是信息的匮乏，而是你那颗被进化论锁死的，充满了Bug的大脑。 而AI在这个时代的真正角色，不是你的“神”，而是你的“外挂假体”——一个用来替换你那部分已经“坏死的”、情绪化的“决策器官”的理性假体。 今天，咱们不聊K线，不聊MACD。作为在A股港股美股比特币绞肉机里活下来的老韭菜，我想借着这个视频里的三个典型样本，跟大家好好扒一扒“决策”两个字背后的残酷逻辑。 一、 “高知陷阱”与贝叶斯失效：为什么聪明人死得最惨？ 视频里最让我唏嘘的角色，不是那个靠运气赢的小白，而是那个懂代码、懂金融、会做深度研究，最后却输得底裤都不剩的旁白——林亦。 虽然千问在投资方面有显著的独到之处，大家用的装备是一样的，起跑线看起来似乎是一致的。但我却认为其实林亦手里的工具是最先进的：不是因为他是典型的“高知派”投资者，而是他的用法给千问加了不少Buff。他能写Python脚本做回测，让千问在一秒钟内阅读几十篇研报。更重要的，他使用AI的经验同样显著强于其他人，普通人你给他再先进的AI，不知道怎么喂合适的提示词（Prompt），得出来的结果必然是大路旁的一般货色。但为什么林亦不是最后的第一名？ 答案是他犯了一个致命的认知错误：他把AI当成了验证偏见的工具，而不是修正认知的坐标。 1. 强势的领导和证实偏差的死循环 视频里有一个细节堪称教科书级别的反面教材。林亦收到了千问的操作提示，但他不认可。他坚信自己的判断，于是让ai重新生成一份，甚至试图通过修改提示词诱导ai得出和自己一致的观点。 在行为金融学里，这叫“证实偏差”。当人类一旦形成某种信念（比如“这只股票会涨”），我们就会下意识地搜集支持该信念的证据，而自动屏蔽反面信息。 在传统时代，我们要搜集“利好”还得去翻股吧、找新闻，成本挺高。但在AI时代，可怕的事情发生了：AI的高效，反而成了加强偏见的催化剂。 如果你想听好话，AI能给你生成一万字逻辑严密的“看多报告”。 林亦就是这样“死掉”的，他像一个敏锐而自负的领导，在林亦和AI的相处模式下，最终决策的不是聪明的AI，而是数据进化偏见增强版的林亦。 在这位“强势领导”的高压下，千问明明已经给出了客观的风险提示，但他视而不见。他利用AI强大的“深度研究”能力，只是为了给自己那错误的持仓找一个心理安慰。 复盘随笔： AI最大的价值，不是帮你找理由，而是帮你“杀逻辑”。 当你极其看好一个项目时，不妨试试打开千问，输入这行指令： “我计划投资X公司，请扮演一个极其挑剔刻薄的做空机构，列出这家公司未来一年可能倒闭的10个理由，并用数据支撑。” 这才是AI决策的正确打开方式——引入“红队测试”（Red Teaming）。只有当AI攻击不倒你的逻辑时，那个决策才是站得住脚的。 2. A也会像人一样产生幻觉是怎么回事 林亦在视频里展示了一顿眼花缭乱的操作：写代码、抓取实时交易数据、多维度打分。他觉得，我掌握的数据越细，我的胜率就越高。 结果呢？一顿操作猛如虎，一看收益负五点。 心理学家兰格提出过“控制幻觉”：人们倾向于相信自己能控制那些实际上由随机因素决定的事件。而“信息过载”会加剧这种幻觉。 在A股，噪音远多于信号。 千问这种大模型，最强悍的能力其实是“奥卡姆剃刀”——剔除噪音。视频里，AI在分析完几家热门公司后，直接告诉林亦：“另外四家你怎么不推荐？因为它们风险大。” 这是AI在帮你做减法。 但林亦不仅没做减法，反而逼着千问做加法。他把简单的买卖变成了复杂的参数模型，最后把自己绕晕了。在周四的大跌中，他面对海量的信息流，心态彻底崩盘，在最低点清仓割肉。 复盘随笔： 在这个算力过剩的时代，“做减法”的智慧比“做加法”的能力更值钱。 不要试图用AI去穷尽所有信息。好的决策，往往只需要两三个核心变量。利用AI的总结能力，帮你屏蔽掉99%的噪音，只看那1%的信号。 二、 “尤利西斯合约”：躺赢的本质是去人性化 这一段我们聊聊那个看似“走狗屎运”的小六。 他是公司里最年轻的，也是最不懂股的。他的策略简单到令人发指：“无脑听AI的”。甚至跟着AI一起谋划如何击溃第一名的心态 结果大家看到了，在11月13日，它吃到了那一波10%的涨停。 很多人说这是运气。我不这么看。这在博弈论里，叫“承诺策略”，或者更文艺一点，叫“尤利西斯合约”（Ulysses Pact）。 1. “损失厌恶”如何克服 在荷马史诗里，尤利西斯为了听到海妖的歌声又不被诱惑跳海，让船员把自己绑在桅杆上。 小六做的，就是把自己绑在了AI这根“桅杆”上。 A股最难的是什么？不是选股，是拿住。 诺贝尔奖得主卡尼曼提出的“前景理论”告诉我们：人对于损失的痛苦感，是同等收益快乐感的2.5倍。 所以，当股价下跌时，我们的杏仁核（大脑的情绪中心）会疯狂报警，逼迫我们通过“卖出”来停止痛苦。这就是为什么散户永远拿不住牛股，稍有波动就被洗出去。 但AI有杏仁核吗？没有。 AI会因为账户绿了而心跳加速吗？不会。 这就是AI作为“情绪阻断剂”的价值。它像一个冷酷的风控官，按住了人性里想要割肉的手。 2. 知行合一实践之算法厌恶的逆袭 有种现象叫“算法厌恶”：如果人类犯错，我们能原谅；如果算法犯错，哪怕它总体准确率更高，我们也会立刻抛弃它。 视频里的阿杰就是典型。刚开始靠卖惨想要让AI让他一举成名，但亏了钱就马上改各种需求，甚至开始占卜炒股，结果却越亏越离谱 复盘随笔： “知行合一”是人类几千年都解决不了的难题，因为“知”在大脑皮层，“行”往往受边缘系统控制。 但AI可以帮你解决。 当你下定决心要执行某种策略（比如定投、比如止损）时，请把AI当成你的纪律委员。当你想违背策略时，去问问AI，让它用数据狠狠地打你的脸。 三、 “情绪价值”的新定义：AI是你的赛博算命师吗？ 视频里最让我感到意外，也最值得玩味的角色，是拿了第一名的TT妹。 她是个做合规的妹子，水瓶座，信玄学。她跟AI的交互方式完全不同。 别人问代码，她问星座；别人问K线，她问MBTI人格。 很多人可能会笑：这不就是瞎搞吗？我开始其实也这么想的，想必大家身边都认识一些这类信塔罗牌，热衷占卜懂什么上升星座下行星座的人，他们的能力通常可以作为打破冷场的话匣子，但是，我相信大部分理性的人，不会真的认为用玄学策略去指导生活实践可以获得好成绩好回报。 但最后，居然是她，恰恰是她。 我起初也疑惑，觉得是时间太短撞上了意外。但仔细寻思以后，我还是发现了其中有逻辑支撑的答案：看似是玄学，其实背后藏着极高段位的“情绪博弈”。 1. 情绪稳定是最大的Alpha 在投资决策中，心态 的重要性往往高于方法 和 资金。 TT妹把千问当成了什么？当成了“理财搭子”，甚至是心理咨询师。 她告诉AI自己是ENFJ（主人公人格），AI反馈给她的话术是：“你是理想主义的实践者……等待被看见”。 这听起来像鸡汤，但在高压的交易环境下，这种正向的情绪反馈价值连城。 当其他选手因为焦虑而在内耗、在频繁操作时，TT妹因为有了AI这个“懂她”的伙伴，保持了极好的心理状态。 AI不仅能处理数据，还能处理情绪。 千问的拟人化交互，实际上起到了一种“认知重构”的作用——把市场的波动，解释为对她性格的考验，从而赋予了亏损以意义，让她能坚持下去。 2. 直觉决策仅仅是拍脑袋而已吗 TT妹的ai和她谈天说地看似不务正业，实际上AI在后台仍然在处理海量数据后的模糊输出。最后的语言只是最表层的。 AI监测到了异动和资金流向，但它没有用复杂的报表扔给TT妹（因为她不看），而是用一种她能接受的“指令”形式—— just do it 而这恰恰推了她一把。 这给了我们一个极其重要的启示：好的决策辅助，不一定是给你看数据，而是用你听得懂的语言，推动你行动。 对于不精通技术的普通人来说，一个能理解你性格、能用人话跟你交流、能给你信心的AI，比一个只会列数据的冷血机器更有用。（当然，最重要的还是她赢了。和世间所有的事情一样，只要是最后赢了，自有大儒替你辩经。） 四、 终局思考：人机共生的新公式 视频的结局很有意思，截止到视频结尾，所谓的“稳健派”和“激进派”都输了，反而是最“无为”的小六和最“玄学”的TT妹赢了。像是对传统精英主义的一种嘲弄。 回到最开始的问题：AI真的能帮我们做好决策吗？ 我的答案是：能，但前提是你得把自己当“人”看，把AI当“机器”用，千万别反过来。 很多失败者（像林亦），是试图用人脑去干机器的活（拼命处理信息），然后让机器来干人的活（让AI去猜测市场情绪）。这就乱套了。 在AI时代，一个完美的决策公式应该是这样的： 完美决策= AI的全量信息（广度）+AI的冷血逻辑（风控） + 人的价值观（大方向 把“脏活”给AI： 信息搜集、财报分析、代码回测、风险警示。这些需要算力的事情，人类完全不要插手。千问这种工具，就是你的全能研究员。 把“软肋”交给AI防护： 承认自己有贪婪、恐惧、损失厌恶。在做交易之前，先设定好规则，让AI监督你。把AI当成你的外置前额叶皮层，用来抑制冲动。 人类只保留“最终解释权”： 决定你要去哪里（是激进还是稳健，是投机还是投资），这是价值观的问题，AI不懂，你来定。但是仅仅是定方向而已，需要注意的是，千万不能像崇祯帝或者旁白林亦或者是微操之神常公凯申那样，做执行层上斤斤计较不愿放权的领导。这样会导致什么结果，这几人的例子我们都清楚。 写在最后：给所有投资者的建议 这个视频虽然是个短片，但它不仅是一场真人秀，更是一则关于未来的寓言。 A股4000点已经来过了，虽然离大部分人赚钱的狂欢节还差得远，但“AI+个人”的超级个体时代已经骑在你脸上了。 如果你还在用百度搜新闻，还在凭感觉看K线，更有甚者还要花几千块报那种K线班，那你面对那些武装了AI的“新韭菜”（或者说“钢铁侠”），你就是那个拿着长矛冲向机枪阵地的原始人。 不要去神话AI，它预测不了明天的涨跌。但也请请务必利用好AI，因为它可以精准地预测出你人性中每一次想要犯错的冲动，并试图拦住你。 在资本市场，少犯错，就是最大的赢。 最后不得不说，看完这场挑战之后，我对国内AI的现阶段能力是感到惊艳的。在对投资的理解方面、资料搜集和处理方面，千问完全可以和当红炸子鸡Gemini3打个有来有回。甚至因为本土性更佳和训练的资料更local，在A股投资方面达到了更高的高度。如果你需要一个小助理，我认为在当下是比Gemini更好的选择。 AI建议仅供参考，不构成投资/医疗建议,Vinter​ 利兹大学 管理学硕士 60 人赞同了该回答 随着AI能力的不断增强，以及Nof1让各个AI炒币来比拼投资能力的出圈，大家对于使用AI来进行辅助投资决策展开了热议。今天早上刷到隔壁b站有博主发布AI炒股的比赛视频觉得挺有意思，认真看完了，作为一个有一些财务背景、但在港股市场依然经常被震荡教育的新晋韭菜，我对AI的辅助作用既好奇又怀疑：AI到底在多大程度上能够帮助决策，它的幻觉会很严重吗？ 带着这样的疑问，我也尝试了一下在Alpha Arena里拔得头筹的千问，让它对我的交易策略提供一些帮助。 先从最让我头疼的部分开始：信息收集。恒生科技指数本身就有个很“戏剧化”的特色——白天看大A脸色，晚上又要看纳指走法。白天要是大A不给力，恒科会跟着走弱；晚上纳指若因为一则消息跳水，第二天你就会看到满屏绿油油。虽然我们嘴上都说坚持价值投资、忽略短期波动，但恒科这种“风吹草动就死给你看”的行情，谁遇到都很难完全无动于衷。所以如果能提前把每个月可能影响情绪的重要节点捋清楚，至少能在心理上建立一道防线，让自己的交易不至于被情绪带着跑。 过去这些信息基本得靠自己一点点找，而现在，这部分工作太适合交给千问了。 比如在刚过去的11月，纳指就经历了一轮先下行后回升的过程。我让千问梳理了期间的主要市场事件，它成功归纳出了两条被广泛讨论的主线。这类信息的整合有助于理解当时市场的关注焦点，也为后续跟踪12月的重要日程提供了参考框架。 关于12月的市场关注点，千问提到美联储的政策信号仍是重要变量之一。但它没有主动提供一条我认为非常关键的信息，即近期科技企业围绕大模型和算力基础设施动态的讨论，例如谷歌发布新一代模型，以及行业关于芯片采购方向的传闻。这些进展反映了AI产业链不同技术路径的演进，可能影响相关企业的长期定位。 这类产业层面的变量，AI很难会主动讲，必须由我补充提示。这也从侧面说明，AI的输出质量很大程度取决于你问的问题够不够“有框架”。 在宏观整理之后，我把千问用在我最关心的部分：分析所持有的阿里和美团的最新财务表现。在阿里11月25日发布财报后，我把财报丢给千问，让它提炼亮点和隐忧。 千问的总结很快，也算全面，但问题是，中性的总结对于投资决策并不够“尖锐”，不像是从股东视角出发的深度分析。在跟踪当前科技企业的动态时，我会更关注其在AI等前沿领域的投入是否具有持续性和战略清晰度，而不仅仅是短期利润波动。因此，我向通义千问进一步说明了我的观察角度： “目前市场对某些大型科技公司的讨论，似乎更多集中在AI领域的投入强度、技术迭代节奏，以及在本地服务等新战场上的资源分配策略。相比之下，单季度盈利变化可能不是最关键的观察指标，你怎么看？” 基于这一引导，千问调整了分析框架，不仅提到了该公司未来三年在AI和云基础设施上计划投入的规模，也指出：如果企业能在关键技术领域保持领先，并维持高强度的研发投入，那么阶段性利润承压，可能被市场理解为一种长期布局的体现。 这种从“数据罗列”转向“逻辑关联”的回应，让我感受到大模型在整合复杂信息方面的进步。随后，我也请它对比了两家公司在本地生活业务上的最新财报表现。它的分析提到，由于业务结构和资源基础不同，双方在竞争中的策略空间可能存在差异，例如一方依托更广泛的生态协同，另一方则更聚焦于单一赛道的深耕。 不过千问没有指出，我觉得美团更长远的问题，在于增长空间极其有限。虽然Keeta在海外的势头不错，而且我也认为如果美团的出海能够成功将会对美团的估值带来影响，但是问题在于没有人能预测在美团完成出海霸业之前，会不会先因为国内的外卖大战而失血过多。同时，面对AI这艘大船，美团也可能已经错失了拿到船票的机会。那么，对于一个错过了AI时代的巨头，美团可能会被困在基础服务提供商的故事里，而难以让资本市场给它更为宏大的想象空间。 另外，在参与港股市场时，我发现自己对买入时机的把握常常不够准确，有时在阶段性高点附近介入后，面临后续价格持续调整的情况。以某次对一家大型科技公司的操作为例，事后回看，当时的决策可能存在一些认知偏差。为此，我让千问基于公开行情数据，帮我梳理当时可能忽略的几个关键观察维度。它从价格运行节奏、成交活跃度、前期高点压力区域以及波动区间变化等方面进行了信息整合，并指出：我的介入时点处于一轮明显下跌后的弱反弹阶段，且缺乏成交量的有效配合，同时价格已跌破此前的重要支撑水平。这类位置通常不具备趋势反转的特征，反而容易因市场情绪反复而出现进一步调整。 诚然，所有的技术面分析也只是一个有价值的参考 ，毕竟市场很容易被情绪瞬间逆转。比如11月13号的阿里小作文，以及后续FT和彭博放出来的做空消息，都带来了阿里股价的剧烈波动。但是，技术面分析的价值，在于它给了我们一个用来进行长期投资决策的支点，而不是单一节点纯粹的凭借自己的感觉来进行交易。 过去我们需要花费长期时间积累和学习的技术形态，现在有了千问这类AI工具，就可以节约下来时间，让AI以更为直观和快速的方式来帮我们解读波动，为决策的制定注入一些确定性。 最后，我想引用段永平之前在雪球访谈里的观点：量化当道的时代炒股是一定赢不过算法的，所以段更强调对认可的公司进行价值投资。但是，他的态度建立在巨量的资金基础之上，作为一个资金量不大的小散，我们能做的，或许是在像千问这样的AI工具的加持下，尽可能全面地去收集、了解并解读我们认可的投资标的的公开信息。 那么回到一开始的问题：AI真的能帮人们做好决策吗？ 我目前的答案是一个带着谨慎的“也许”。AI的确能为复杂问题提供极具价值的判断依据：它擅长高效处理信息，把原本散碎、平面的数据重新组织成结构化的分析框架，并在许多时候给出比人工更犀利的视角，这正是它作为工具的意义所在。 但工具始终是工具，决策背后的判断维度远比模型所能覆盖的更复杂。那些关于行业格局的主观判断、对管理层执行力的预期、对估值体系的信念，以及投资者自身的风险承受能力与情绪韧性，都是AI无法替代的。也许交易的本质乐趣，就在于这种张力：一方面服从客观事实，努力保持理性；另一方面又怀抱对未来的期许，带着某种个人化的笃定去下注。 「文中所列举的AI建议仅供参考，不构成投资建议」,lokinko​ 重庆大学 软件工程硕士 创作声明：包含投资理财内容 41 人赞同了该回答 关于AI能否帮助人做好决策，不管AI发展到什么阶段，都记住：「人才是主体」 让AI为人做决策，它的技术支撑和背后的逻辑是什么？就目前AI发展的情况看，当前大模型逐渐演化出两条核心路径：自动执行和深度检索。以Mobile Agent 为代表的自动执行，服务于人的重复性劳动；而深度检索则聚焦于增强人类的创造性。 另一方面， AI 擅长读取、理解和总结归纳，能够将知识压缩并进一步抽取关键，实现人类难以企及的、一目十行式读取资料的速度。作为工具使用的时候，我们不用担忧于 AI 读写的速度比我们快，而是应该将 AI 看成我们的助理/秘书，让它们在繁复的资料里找到关键数据，帮我们将更重要的精力投入到更有创造性的思考中。 但如果谈到用AI做「决策」，相信很多人还是内心存疑。看到B站博主做了一场用AI指导炒股决策的比赛，这个形式蛮有意思，让AI做决策——而且还是关于金钱（投资）的决策，如果能成那似乎一定程度上可以给问题一个思路。 作为一个已工作两三年的AI从业人员，我在去年初才培养起理财的意识，之前也踩过一些坑。回想起我自己的使用经验，也有通过AI来辅助投资的经历，不妨先一下。作为大道的头号小粉丝，近一年我一直在关注 PDD ，试图搞懂为什么大家认为这是个好的生意。我不是金融从业人员，财报读的不是很明白，但是这些并非致命的，没有人要求只有读懂财报所有数据才能投资，最重要的是从海量财报数据和信息中抓取最重要的真实信源，而非迷失在数据黑洞中。 我个人一直都遵循买股票就是买公司的原则，只关心净现金流、战略方向、股票收益率与资本收益率，这直接决定这个投资是否符合我的理念。于是，我让千问帮我总结PDD2022-2024三年的年报以及2025Q1-Q3 的季报财务投资情况，让我做决策之前更有底： 千问在做了一番分析之后，给了我如下3个关键结论： 1、公司的投资现金流趋于理性，净现金流健康具备极强的财务韧性和抗风险能力； 2、公司的战略加大投入海外、科技提升、重仓农业、业务模式创新； 国内和海外双投入，从供给端和需求端反补市场，而科技驱动的增长也正爆发着无法想象的活力；农业数字化赋能既是趋势，也是最坚固的护城河，未来一定会种出好的结果。 3、公司现金充足，资产收益率持续上升，资本收益率带来了复合高增长的潜力，关键收益指标健康。 股价处于低点，但净资产收益却维持在高水平，正是市场价格错配的体现。 过去一年时间里我一直在追踪PDD的股价信息，但却迟迟不敢下重注，但在看到近期多个 AI 辅助决策的案例后，在思考尝试逐渐将Qwen的能力融合到投资策略决策中。 在使用 Qwen 深度检索辅助调研的帮助下，能够更深刻的洞察企业发展的近景和远景战略，并非一味的推荐，而是在协助决策的同时会帮助提报风险。决策需要更多地评估是否能够接受短期风险而选择长期投资，结果是显而易见的，相信长期的价值，尝试对相信的事情下重注，后续期待可以在这里同步相关进展。 价值投资的逻辑是找到价值被低估的好公司，如果让我直接读财报去分析，很难抽取出这些关键指标并整理出来，但是让我基于AI对指标分析之后做出的判断，会简单得多。 回到题目上来，能说AI在帮我做决策吗？我认为一定程度上，是的，而且目前来看结果还不错。但也仅限于“一定程度”，因为在真正做出投资决策之前，过程中我依然掺杂着许多个人的思考和判断，AI可以帮我们总结客观规律、预测走向，但它无法全然了解我的个人情况、准确预判市场风险，因此我无法全然依赖于它。 事实上，作为当前的开源 AI 模型的 Top-1，千问用的Qwen3-max模型无论长上下文能力，还是 VL 系列的超强多模态理解能力，或是混合 search、code、API 基于定制环境端到端训练的 DeepResearch 能力，已经能够成为我们辅助生活的最佳帮手，只要我们用好、用对，很多时候用来辅助做决策是个不错的主意。 最后我想说，现在已经不再是以前半路听到消息制造信息差就能牟利的时代了，互联网消除了信息差但带来了海量的噪声与信息茧房，人脑的决策无时无刻不在被干扰。就像B站这个AI炒股比赛的视频一样，大家在投资过程中会面临各种各样繁杂的信息、复杂的心理变化，而投资更是切忌盲目，把当前情况告诉AI，让它以冷静的三方视角来看待问题，能够一定程度上隔绝海量声音带来的干扰。 我使用体验下来，千问的深度思考 + 联网能力在最大化互联网信息效率的情况下，可以较好地解决AI 幻觉和互联网噪声问题，让使用者不会迷失在庞杂的信息茧房中，并且能够提供真实、高质量的信源，甚至在使用过程中能针对我陈述的观点进行查漏补缺，给出建设性的总结意见，引导我更深度地思考一些忽略的问题角度。 AI大势不可逆，如果说互联网思维是上一代浪潮的话，那么 AI 思维就是新一代起航的船舶，在时代的洪流中驶向下一个彼岸。 作为AI从业者，我会推荐大家现在慢慢开始用AI辅助决策，先尝试、验证、再评论、总结，体验AI将信息茧房打破的新鲜感受。若你想尝试用 AI 辅助决策，不如现在就开始试试。种一棵树最好的时间是十年前，其次是现在，但不论何时也别忘了，更好地服务于人才是AI被创造出来的意义。 AI建议仅供参考，不构成投资/医疗建议,程序员阿德 90 人赞同了该回答 昨天看到某站林亦发了条AI大战A股的视频，拉着全公司下场比赛，都用同样的千问AI，不同的人用法千奇百怪：有让AI收集财经新闻、分析公司财报、参考股票涨幅榜的技术派，也有让AI结合星座和性格选股的玄学派，结果一对比玄学派还成了短期获利最多的玩家。 最近用AI炒股的操作帖确实不少，讨论度也高，但其实用“AI决策炒股赚钱”和“会用AI”不能完全画等号。应用在现实里，用千问AI炒股的水平有多高，不是看AI靠玄学分析能“蒙对”多少，而是看使用者有没有找对它的核心优势，这一点，作为天天和千问打交道的程序员，我深有感触。 一、 会用AI与否，重在看是否充分发挥AI的核心优势 虽然视频里的挑战展现出了AI靠玄学分析也能让投资者赚到钱，但这到底是千问的功劳，还是市场的随机馈赠？ 针对此，我的观点是，用玄学分析的“娱乐化”来检验AI模型能力肯定是不可取的，千问的核心优势肯定是——整合多维度市场信息、拆解万字财报、分析技术指标，是能靠硬数据打破信息差的TOP级工具；如果我们只是把它当成星座解读器，就是让AI给无逻辑的玄学做包装，就算这次蒙对了热门股，下次就一定能蒙对吗？如果连为什么赚钱都说不清，下次怎么复制？ 虽然说赚钱就是硬道理，但如果脱离数据也能算会用AI，那直接扔硬币选股不更省事，何必浪费千问的顶尖能力。 千问的技术上限明明能支撑专业的市场分析，如果只是用来解读星座和性格，我觉得很可惜，这根本不是用AI炒股，而是借AI的壳玩玄学，千问的核心价值被完全浪费了。 所以让千问干它最擅长的事，比如让千问实时抓取A股4000+个股的舆情动态，快速标注利好、利空信号，不用自己翻遍全网新闻；扔给千问10万字的公司财报，3分钟就能拿到核心指标拆解（比如营收增速、现金流健康度等），准确率很高，省了大量自己细看的时间；还可以汇总券商研报、行业动态，帮自己搭建更全面的分析框架。 这种用法才是真正的能帮助自己做好理性决策，千问本身是行业 TOP 级大模型，技术实力经得住验证，解读技术指标、打破信息壁垒、帮人复盘逻辑和优化策略都是它的长项。 但是我也想强调，咱们也不要盲目依赖 AI，而是把千问当信息处理助手，自己用金融认知校验结论、决策，做出最终判断，比普通散户瞎猜、跟风靠谱多了。 二、程序员的千问打开方式：使用AI编程的真实体验 另一方面，看到视频里大家对千问各式各样的用法，我也觉得很有意思，越来越多对AI认知水平各异的用户开始接触AI、学习AI，这是现在发展很快的趋势。 我日常在用千问的代码能力辅助工作，核心逻辑和用千问炒股相通，让 AI 干它最擅长的事。 两个干货用法，或许能帮大家更直观理解 AI 该怎么用。 遇到同事写的复杂功能，我不用花几小时啃逻辑，直接一句话就让千问帮我基于代码生成注释和可视化流程图，里面嵌套的条件判断、数据流向非常清晰。对程序员来说，看不懂老代码就是我们的信息壁垒，而千问的语义分析能力，刚好能精准打破它，这就是千问AI工具的共性优势：擅长处理结构化、高复杂度的信息和数据，并以直观的形式展现出来。 千问帮我详细梳理reduce的CUDA代码，并生成流程图 还有我将现有的业务代码直接发给它，让它基于我新的业务需求，梳理出业务逻辑，将新功能的代码嵌入到合适的位置，还贴心地考虑到了容易出错的边界条件，最后我只花 10 分钟检查和校验AI提供的代码是否可用。 千问帮我优化flashattention的CUDA代码 综上，就个人的使用体验来说，AI 能提供分析过程和实现功能，但不能替人做落地决策，如果直接照搬 AI 的所有建议，忽略自身场景的特殊性，再顶尖的工具也会出问题。 三、AI再强，关键还是人驾驭工具 千问确实是行业 TOP 级的 AI 工具，不管是炒股时的数据分析，还是编程时的代码辅助，都能帮我们拓展能力边界、节省时间，但它终究是辅助。 我更倾向让千问聚焦它擅长的数据、信息整合领域，向我们提供可靠的建议和指导，但我们也需要有足够的认知做兜底，毕竟做最终决策的人是我们自己，为决策结果负责的也是我们自己。 炒股没有捷径，编程也没有捷径，AI 再强，目前也还替代不了人类的独立判断。搞玄学的人或许能短期获利，但真正值得学的，是那些用千问抓数据、做分析，再用自身认知做决策的玩家。毕竟靠运气赚到的钱，最终也会靠实力亏回去，而学会驾驭 AI的能力，才是能长期复用的核心竞争力。 AI建议仅供参考，不构成投资/医疗建议。,奇點没有起点​ 266 人赞同了该回答 AI会不会让我们失业？甚至已经在悄悄接管世界了？ 是不是正如霍金曾警示的那样，“智能机器将是人类未来千年的伙伴，或是毁灭者”。 而对普通人而言，比起“是否接管”这个宏大命题，我们是不是更应该关注的是它以何种方式渗透生活，以及我们该如何在这场技术变革中自处的呢？ 那么，AI事否帮人类做决策、能不能用来帮我们理财呢？ 早前想试着用ChatGPT辅助入市，可早期版本无法联网，让它根本跟不上瞬息万变的市场节奏。 不过今年硅谷的一场AI投资大赛，让我看到了AI真正潜力。 大赛给6大顶尖AI各发放1万美金进行实盘操作，最终阿里千问以超20%的回报强势夺冠，而GPT - 5却亏损60%垫底。 这背后藏着AI辅助投资的核心优势，正如《超级智能：路线图、危险性与应对策略》中对AI能力的预判，它能高效处理人类难以驾驭的海量信息 。 千问这类模型可以快速梳理公司财报、抓取新闻舆情、整合行业数据，精准定位某家企业的潜在利好或利空因素。 AI领域的博弈远不止投资赛场，模型之间的较量同样激烈。 Airbnb的CEO就公开坦言，其业务已深度依赖阿里的Qwen模型，核心原因就是Qwen不仅性能更出色，性价比也远超OpenAI的模型。 如今Qwen3 - Max的性能已赶超GPT - 5，在文件阅读和摘要生成上表现亮眼。 更难得的是它具备推理、编程、视觉等全模态能力，在Hugging Face的前10开源模型榜单中，千问系列更是占据7席。 这也印证了著名投资人朱啸虎的判断：“千问的技术、产品、生态三者合一，是最接近ChatGPT标准答案的存在。”而朱啸虎后续在外滩大会上进一步补充观点，称AI应用即将迎来大爆发，中国企业正凭借差异化产品力脱颖而出，千问的表现无疑正是这一判断的有力佐证 。 千问的崛起，还戳中了当前美国AI发展模式的痛点。美国走的是“大算力+大规模”的路线，OpenAI、微软、英伟达等企业通过相互投资形成三角循环，玩着高投入、高融资、高估值的资本游戏。 但这绝不意味着我们可以完全依赖AI做投资决策。 李开复曾说，“人工智能的发展对于人类的人性是一场极大的考验。面对考验，人类应该善用机器，且互相关爱”。 AI给出的投资策略终究只是参考，人类的主观判断必不可少。 正如《浪潮将至》中所强调的，技术应兼顾便利与风险管控，千问的模式恰恰做到了技术实用与体验流畅的统一 。 若千问的发展模式能持续跑通，那美国当下这种重资本、轻实用的AI玩法，恐怕真的要面临泡沫破灭的危机了。 毕竟技术的终极价值终究要落到实际应用上，而非资本游戏。 总之，AI是否能帮人类做决策，时间会证明一切的。 （PS:AI建议仅供参考，不构成投资/医疗建议。） 送礼物 还没有人送礼物，鼓励一下,囍帖街​ 🌈芝士优选新势力丨家居家电丨3C数码丨新能源车主丨家有两喵 115 人赞同了该回答 现在越来越多人开始用AI做决策了，小到吃什么、买什么，甚至大到职业选择、理财投资，都开始让AI插一脚。 我一开始也觉得离谱：股市这么复杂，情绪、政策、资金轮动全搅在一起，AI能懂？可过去三个月，我自己试下来，想法变了。不是AI多神，而是它帮我把一些最基本的坑，给绕过去了。 简单介绍下自己，我不是职业投资人，就是个普通上班族，有时间看盘，但没精力天天盯5000家公司。以前做决定靠刷研报摘要、听大V点评、看热搜股，结果经常是利好追进去，利空砸下来才反应过来。信息碎片化不说，还特别容易被短期走势带偏。 从今年年初开始，我试着让AI参与我的投资流程，最近专用的就是千问，但我绝不是让它全权操作，也不是信什么预测模型，就是把它当一个不睡觉、不嫌烦、还能记住我所有逻辑漏洞的小助理。 比如11月25日前后，鸿蒙生态概念突然爆火。朋友圈刷屏纯血鸿蒙元年开启，短视频里说下一个万亿生态，好几只小盘股连续三天20cm涨停。我看着账户余额有点躁动，手都快点下去了，临下单前还是习惯性问了千问一句：“这轮上涨的核心驱动是订单落地还是情绪炒作？领涨公司的业务和鸿蒙到底有没有实质关联？历史上类似操作系统概念炒作一周后的表现如何？” 它很快回了一堆数据，最扎眼的一句是：“本轮涨幅前10的标的中，9家未披露任何HarmonyOS NEXT相关收入，历史回测显示，2021年以来共出现4次同类主题短期暴涨（5日内涨超30%），峰值后10个交易日平均回调幅度达22.6%。” 看见这些话我立刻收手。果然11月28日起板块集体回调，不少个股三天就跌回启动位。 再举个更实在的例子。我看中一只机械股，觉得行业回暖且估值低，准备动手。问千问：“如果这个判断错了，最可能是因为什么？” 它列了三条：客户集中度高、应收账款周转恶化、近期股东减持。前两条我在财报里见过，但没当回事；第三条我根本不知道。查公告，还真是两周前发了减持预披露。 慢慢地，这成了我每天的固定动作。每天收盘后花15分钟，把我当天的想法喂给千问，让它从反面拆一遍。它不会拍板，但它总会问：“你假设的前提成立吗？”“有没有相反的数据？”“历史上类似情境结果如何？” 现在我下单前，已经习惯性地打开对话框，把想法打进去，问一句：“你觉得这逻辑哪里有问题？” 有时候它说得不对，我也直接否掉；有时候它点出一个我根本没注意的细节，我就去查公告、看龙虎榜。 时间久了，我发现我不是在依赖千问，而是在借它的能力，看看自己有没有漏看什么。 用国产AI炒股靠不靠谱？我不知道别人怎么说，但对我这种非专业选手来说，它至少让我少交了几笔冲动税。以前是看到涨就心痒，现在会多等一晚，等AI回个消息再决定。就这么一个小动作，躲过了好几波坑。 真要说值不值，我觉得值。不指望它带我发财，能帮我稳住节奏，就已经算加分项了。 （AI建议仅供参考，不构成投资/医疗建议） 送礼物 还没有人送礼物，鼓励一下,"小兔叽妈吖​ 教师资格证持证人 81 人赞同了该回答 作为一个拥有十几年投资理财经验的小小散，我虽然不会让 AI 来直接帮我做投资理财的决策，但我自己在做决策之前是会让 AI 来帮我分析一些事情的。 就像上市公司发布的那些又长又复杂的年度财报，自己逐句去看费心费时又费眼。看别人分析好的吧，又担心人家分析的不够全面或者有刻意的误导之嫌。 我现在完全可以利用 AI 强大的数据分析和总结能力，来帮助我去分析一些数据信息， 比如 我想看一只股票今年上半年的表现，我就直接在千问里选择，把过往需要各种数据分析的需求丢给它 ⬇️ 它快速地调取该股票的目标财报以及全网相关联的信息，生成的结果分类清晰，我想看到的数据基本都在。 AI 的特点是对现有信息数据进行分析反馈，不会人为的去对某一些指标去进行有倾向性的解释和引导买入。 只要样本数据正确，那它给到我们的结果反馈也相对是客观的！ 都说投资有风险，AI 在给数据展示时也不忘进行风险提示，并把风险信息列出来给我们进行参考。 就比如我关心的这只股票，业务短板的风险提示内容就是我之前没有注意到的。 看了 AI 的提示之后，我又重新了解了该股票的情况，结果也确实如它提示的这般 ⬇️ AI 作为智能辅助，在一些信息分析和处理上确实比我们人工要快速和全面。 就拿我们普通人的投资理财来说， 它是可以帮助我们去多角度地了解一些投资理财的产品信息的，在一定程度上能帮助我们节约分析、思考时间，缩短决策路径。 而至于要不要投资，决策权还是要在自己手里，我们自己才是最终的决策人。 AI 说到底就是一个帮助我们分析的工具，为我们快速提供一些想要的信息参考。但它没有担责的能力也不会为我们担责，所以需要时刻保持理性的，一直都是我们自己。 理性咨询，理性参考，理性决策！ 注：投资有风险,入市需谨慎 送礼物 还没有人送礼物，鼓励一下",慧慧说教育​ 教师资格证持证人 82 人赞同了该回答 我觉得AI肯定是能给我们在决策的速度和准确度上提供很大的参考。 就拿炒股来说吧，早在AI流行前，用机器挂盘，批量买进、抛售在业界都不是什么新鲜事了。每次设定好买进卖出的条件，一旦条件触发机器自动操作，比人手工操作要迅速得多。 疫情期间队友去北京和他某清华毕业的哥们儿聚会，回来告诉我他是某国内头部人寿保险公司基金的操盘手，平均每年的收成在25%以上。我说都是自己人，给我们拉个群，让他带着我，队友说没可能，他也提起过这个想法，他哥们儿说那个有风险，他们一旦有风吹草动都是机器抛售的，我跟投很难跟上节奏。 还真是，一般大消息都是3：00收盘后放出，早上9：30开盘，有些预期会有大行情的股票基本上9：15挂涨停板都买不进去，有些股票可以连拉好几个涨停。跌停也是，基本上都封死了，根本卖不出去。 散户的消息都比较滞后，都是在消息上新闻了、或者至少是小道消息满天飞的时候才知道，这时候消息灵通、又有机器辅助操作的机构早跑了。 我是前几年被亏损怕了不再玩股票了。21年买入的基金亏损的没有清仓，经历今年大半年的行情，基金还在亏损状态。我朋友家去年因为炒股亏损，家里鸡飞狗跳，差点离婚。年底全部割肉离场，倒在了黎明前夕，好可惜的。 我们都知道，短线炒股靠的就是消息面和反应速度，而散户炒股最大的问题就是信息过于滞后，自己在家里盯盘，盯着实时动态，但没用的。有了像千问这样的AI辅助，信息获取的速度和覆盖面会大很多，有了这些数据作参考，我们的决策可以更精准一些。 我现在每天就会蹲早让千问设定好交易的条件，一旦触发条件，让他立马给我发出警报。我会根据当前掌握的所有信息综合判断一下要不要执行买卖。AI进一步拉平了我们这样的散户和机构的信息差，给我们的决策提供了很大的参考价值。 （注意：AI建议仅供参考，不构成投资建议） 送礼物 还没有人送礼物，鼓励一下,燕66​ 制造业 仓库文员 38 人赞同了该回答 一场由人工智能驱动的金融革命，正在悄然发生...... 在现代这个社会中，AI已经渗透到各行各业中，不管是制造业还是农业，不管是交通领域还是金融领域，处处都能看见它们的身影，在AI的加持下，我们的生活变得快捷和方便了，至少我是这样认为的。 而我今天想说的是千问AI在金融领域给我的帮助，说起炒股，大家应该都不陌生吧，但凡有点闲钱，除了理财，就是拿来炒股了，都想着钱能生钱，可是股市哪能处处随人愿，一点风吹草动，都能引起大盘的波动。 我常常为了一只股，反复看它财报上的几百页内容，却未必能抓重点，而这些千问3分钟就能搞定，更别说从宏观经济，行业政策，主力资金动向中筛选出有效信息，放在以前，我可能花上一天都未必能找到头绪，千问3分钟内就能完成分析，给出调整策略以及后期如何交易等一整套流程的建议。 在炒股的时候，AI就像我背后的“谋士”，它不会受情绪影响，能理性的根据市场行情以及规律给出合理的分析建议，而我是最终的决策人，在它的建议中，能很快的发现股票K线图里的细微形态，还有板块轮动的早期信号，这些都能让我避开很多坑。 说这么多，不是说AI炒股是万能的，它能百分之百帮你赚钱，而是想说，AI能帮助我们做信息收集与处理，大势所趋，学会用AI，能让你避开很多坑，要想赚钱，就看你怎么去使用AI了，如果遇到市场整体下跌，就不能全信AI的推荐，得自己拿主意，规避风险！ （股市有风险，投资需谨慎，千问虽好，仅供参考，AI不构成投资建议）,夜空中最亮的星​ 道路与交通工程高级工程师证书持证人 32 人赞同了该回答 还在每天花五六个小时刷新闻、翻研报、盯股吧累得头晕眼花吗？ 还在加炒股群，付费听“老师”牛股吗？ 现在是AI时代了，普通人，即便没有机构的团队盯盘，不懂专业术语，也不会看数据，同样能炒好股，秘诀就是： 用AI决策。 通义千问App进入后的界面 提到AI，大多数人的概念还是，帮忙画图，做文案，优化个电器，让生活更便利之类的；但其实，现在的AI在帮助我们炒股赚钱这方面，也是相当给力。 说AI帮助炒股决策，不是帮我们直接交易，而是可以给到我们想要的信息，帮助决策。 比如： 通义千问AI，它背后站着的是阿里那个在全球都排得上号的开源模型Qwen。 秒级生成研究报告和PPT就不说了，最惊人的是，它参加了一个全球顶级模型都参与的实盘投资大赛，居然还拿了冠军。 用AI炒股还赚了钱，消息一出来，我仿佛听到了无数基金经理人心碎的声音。 来说说，它是如何实现帮助决策的吧： 1、 通义千问可以按照要求筛选我们需要的股票，比如输入“科技股，近期有利好消息......” 它会帮助筛选龙头企业和优异个股，并且给出不少相关消息的新闻链接； 最后，还能根据已有信息撰写研究报告： 通义千问生成的研究报告一 2、 把复杂信息翻译成能懂的信号：通义千问帮我们高效筛选、分析信息，让我们在决策时不那么盲目，基本面、技术面、消息面、资金面，面面俱到； 3、 这消息是利好还是利空？主力资金是流入还是流出？业绩披露有没有超预期？在我们看来很难的问题，千问都能轻松给出指导； 4、 炒股，打的从来都是信息差，如果AI主动推送给你重要信息（利好消息和风险预警同样重要），你能根据消息及时做出自己的判断。 最后，需要提醒的是，AI是我们的工具和助手，所有的决策最终还是我们自己来决定，AI做的，是理性地帮助我们决策。 注：AI建议仅供参考，不构成投资建议。
Agent store 平台 MuleRun (骡子快跑)海外走红，如何使用？有哪些信息值得关注？,Agent 平台「MuleRun」在 Discord 和 X 上火了，不少海外开发者晒出了在MuleRun「赚美元」的截图，听说这是中国团队做的，它是什么背景？有用过的朋友聊聊体验吗？,程墨Morgan​ 新知答主 289 人赞同了该回答 前阵子MuleRun在海外平台上着实火了一把，我有幸获得了一个邀请码，抢先体验了一下，感觉很不错，据说到9月16日的时候就全面开放了。 都说2025年是AI Agent之年，这一年下来也的确诞生了很多Agent，但是MuleRun是最让我惊艳的一个产品，因为这不是单独一个Agent产品，而是全球首个AI Agent交易市场，也就是可以容纳无穷多个AI Agent，更重要的是连接了Agent的生产和消费，创 打个容易理解的比方，MuleRun就是Agent的淘宝，Agent就是上面的商品。我理解MuleRun（骡子快跑）的寓意就是AI Agent相当于卖力干活的骡子，让它们跑得快，那么人类就可以省去很多劳作，对Agent这种骡子自然存在很大的需求，这样一个巨大的市场必然形成一个正向循环的AI Agent生态，不光可以给消费者很多便利，也可以给创 接下来，我就从消费者和创 消费者端体验 登录进MuleRun的网站(https://mulerun.com/)，可以看见已经有很多AI Agent，可能因为还没有正式发布，所以审核通过的Agent数量还没多到需要一个分类索引或者搜索功能的地步，但现在可用的Agent数量也十分可观。 每个Agent都标记了运行一次需要的credits数，从20到100不等，根据邀请码注册，立刻可以免费获得1000个credits，足够体验很多Agent了。 目前Agent可以分为两类，第一是娱乐Agent，第二是生产力Agent，正好在Workspace最上方最新的两个Agent就分属两类，第一个Pose Grid Generator是娱乐Agent，另一个EduCardGenerator是生产力Agent。 像Pose Grid Generator这种就是上传一张自己或者某人的照片，就可以生成一个3x3格子的图片，每个图片是原照片中人不同的Pose，下图就是鄙人的Pose Grid。这个Agent逻辑并不复杂，唯一称得上神奇的就是可以自动生成多个表情照片（我敢肯定背后用的是Google最新的Nano Banana模型），但是，就是很有趣，随手之间就可以做出趣图，可以给朋友快乐一下。 像Edu Card Generator这种生产力Agent，就更加实用一些，填几个空，指定关键字、预期受众、风格和数量，就能产生教育卡片。 像上面那样，我简单输入了几个关于Machine Learning的单词，就产生了这样知识卡片图，非常精美了，如果要自己去制作类似图片，那可费劲了，现在举手之间几秒钟就到手了，以后如果要做知识类的ppt，就可以直接试用这样的Agent。 无论是娱乐Agent还是生产力Agent，从消费者端体验看，就是使用门槛极低！既不要求用户懂深刻的AI技术，也不用写玄奥的提示词，最多就是上传一张图，输入几个词的工作，就能享受AI带来的显著显著成果。 我相信一个原因是MuleRun在有意引导创 说到这里，就不得不说一说MuleRun上创造AI Agent的优势了。 MuleRun上创造AI Agent的优势 1. 众多AI大模型一站式接入 Agent具备各种神奇的能力，背后当然主要原因是现在AI大模型的能力超强了，不过AI大模型也是各有千秋，还没有单个大模型在各方面全面碾压式领先，比如说，Google的Nano Banana在图像处理方面优势明显，但论文字处理还是Anthropic的Claude更胜一筹。 所以，好的AI Agent平台要提供便利的接入各个大模型的能力，也就能满足消费端更多样的AI需求，也可以通过组合不同的大模型能力产生更多样的Agent功能。 这一方面MuleRun就有很大优势，MuleRun支持15个主流大模型统一接入，对创 2. 运行环境 除了AI大模型的接入，我觉得MuleRun还有一个杀手锏级的优势，那就是——云主机运行环境！ 运行环境就是AI可以交互的环境，像ChatGPT Operator这样的Browser Agent，运行环境就是网页浏览器，只能在浏览器里玩，功能当然受限。MuleRun不一样，在创建Agent的时候可以选择云主机运行环境，也就是在云上分配一个运行Windows操作系统的主机，Agent可以操作这个电脑上的所有事情，AI能做的事情可以扩展到安装软件、存储本地的知识库、修改硬件配置等等，想象空间一下子就超大了。 在MuleRun的『星穹铁道代肝』Agent中，这种运行环境的优势体现得淋漓尽致，星穹铁道本身就是一个独立安装的游戏软件，有了云主机的控制之后，AI无需任何模拟器hack，直接操作游戏，像人类玩家一样代打游戏，这真是很神奇。 3. 创 前面说的都是技术层面的优势，MuleRun还有一个非技术层面，但是很重要的优势，就是提供给创 前面提到了注册就免费获得的credits，每次运行Agent会消耗若干credits，这些credits会流向Agent的创造者，最后创造者可以把credits变现，据说，有的创 收益机制对于一个Agent市场非常重要，不能完全靠创 对于MuleRun这样的Agent市场来说，把Agent开发的框架搭建好，把收益机制做好，相当于搭建一个NBA联赛，就能吸引更多优秀的篮球运动员加入，打出来比赛也更具观赏性，最后吸引的观众也就更多。 目前MuleRun还处于发展初期，就我个人的体会，我说一些对Agent创 首要原则——要让你的Agent能够被很多次使用。 因为MuleRun上credits是按照运行次数支付的，你的Agent运行次数越多，你赚的credits也就越多，最后变现赚的钱也就越多。 要实现这个目标，无外乎两条路子，第一个路子，是让你的Agent有高生产力价值，某一类用户会反复使用，比如前面提到的Edu Card Generator，对于经常要做知识类ppt的用户，肯定会长期反复使用的，这一类用户就会给你带来很多credits。 但即便不具备高生产力价值，只是娱乐Agent，依然有机会获得很多运行次数，这就是第二条路子，虽然单个用户不会运行很多次，但是只要具备足够的娱乐性，再借助社交网络的传播性，可以扩散给很多人，每个人见了都想玩一玩，这样一样会你的Agent被N多次使用。比如，最近在X平台上满屏的3D模型效果图、Q版表情包，给创 还有Star Snap这个Agent，上传一张自己的照片和某个名人（不是名人也无所谓）的照片，就可以利用AI生成合影，对于大部分人来说，现实中根本没机会和名人合影，这个Agent的创意就满足了绝大部分人的心理需求，在社交网络上看到素人们制作的和马斯克、爱因斯坦、科比的合影，谁都会心痒痒，我也不能免俗，做了一个和爱因斯坦勾肩搭背的合影。 做出合影出来之后，发到社交网络上，又会吸引其他人也来尝试，就是这样病毒式传播，即便每个人只玩一次，也会给Agent带来巨大流量。 MuleRun官方也正在发起 “AI Agent Creator 支持计划”，感兴趣的小伙伴可以点击下方链接进群交流，共同搞钱： 欢迎加入MuleRun 中国创 当然，回归初心，做Agent一定是要提供价值的，无论是实际生产价值，还是情绪价值，都有价值，只有价值才能驱动用户来使用，切记切记！ 写在最后 随着MuleRun的全面开放，预计会引发2025年Agent的爆发式发展，这背后有一个深层逻辑——通用型Agent不会在中短期实现。 虽然AI以指数级速度发展，但通用型人工智能AGI依然是遥遥无期，对于还需要和外界交互的Agent，更是不可能做到通用，一些号称通用的AI Agent最后也被证明只不过是能多做几件事，而不是什么都能做。 也就是说，在当下，与其去追寻一个什么都能做的通用AI Agent是不现实的，不如去发展很多个能做好一件事的AI Agent，虽然没有一个Agent包打天下，但是有N多个Agent而且每个都能独当一面，也非常好了呀。 MuleRun就是认清了这个深层逻辑，做一个Agent市场，让N多个能够独当一面的Agent被创造出来，最能满足现实的需要。 对于创 送礼物 还没有人送礼物，鼓励一下,"王金戈​ 健康管理师职业技能等级证书持证人 74 人赞同了该回答 2025年被称为""Agent元年""，这话真不是开玩笑的。 我发现有些人总是唱衰AI，说什么GPT-5不及预期证明AI进入瓶颈了。但事实恰恰相反——单纯比拼模型参数大小的时代已经过去了，真正的革命在于AI Agent的兴起。 为什么Agent才是未来？ 传统的大模型虽然很强大，但本质上就是个""超级搜索引擎""加""文本生成器""。你问它什么，它回答什么，完全是被动的。但Agent不一样，它会主动思考、规划，还能调用各种工具去完成任务。 比如说，ChatGPT可以告诉你怎么分析股票，但它做不到直接帮你去抓取实时股价、计算技术指标、生成分析报告。而一个好的投资分析Agent就能把这整套流程全自动化了。 这个转变其实很符合技术发展的规律。还记得OpenAI刚推出GPTs的时候吗？那会儿大家觉得挺新鲜，能上传文档、设定角色，感觉像有了个专门的AI助手。后来Dify、Coze这些平台出现，开发者开始搭建更复杂的工作流，把多个模型串起来，甚至能调用外部API 到了今年，MuleRun这样的Agent商店直接把整个商业化流程给打通了——你可以把自己的专业技能包装成Agent，上架销售，全球用户都能买到。我们可以称之为""AI劳动力市场""——一个人人都能雇佣AI员工的时代正在到来。 外网的疯狂：连邀请码都成了稀缺资源 最开始知道MuleRun，其实是因为在X（原Twitter）上看到一群海外开发者在晒收益截图。AI时代，新产品层出不穷，这类事情有些见怪不怪了，我也没太在意。 但后来这个平台越来越火，Discord上的邀请码已经到了""一码难求""的地步，用户自发组织排队申请内测资格。 最让人眼红的是真实的赚钱案例。 比如那个叫Laughing的中国创 还有一个叫芋头小宝的AI解决方案工程师，她把自己日常工作中的一些工作流包装成Agent上架。她做的""StarSnap名人合影""圈粉无数，推特上到处都是用户晒自己和马斯克的""合影""。 有意思的是，网传MuleRun是中国团队开发的。 现在国内的AI公司都搞这种""出海""策略，先在海外市场验证产品价值，再考虑引入国内。之前字节的编程IDE Trae也是这个路子。 这背后可能是一条被实践验证的逻辑：海外用户付费意愿更强，对新产品接受度更高，反馈周期也更短。能够快速验证商业模式是否成立。 在我看来，能爆红必然有它的原因。我认为作为一个平台，能吸引优质创 从官方的宣传了解到，MuleRun的技术基础设施很完善，集成了15个主流大模型，而且以近乎成本价提供给创 不过说实话，我现在还没拿到开发者权限，看着别人赚钱确实有点眼红，希望能尽快赶上这波红利。不过，我还是通过一些渠道拿到了内测资格，下面给大家一下我的使用体验。 我的MuleRun初体验 新用户赠送1000 credits。根据功能的复杂程度，使用Agent会消耗几个到几十个的credits。下图是MuleRun首页列出的Agents，我尝试了一些，给大家其中的两个。 Emperor Pet AI：把你家猫咪变成皇帝 第一个让我觉得有意思的是Emperor Pet AI，专门用来生成帝王形象的宠物照片。我上传了我们家超级可爱的元宝——一只布偶猫的照片，结果出来的效果还挺震撼的——穿上龙袍、戴上皇冠的元宝，那种威严感简直了。 下图是我上传的原图和生成的帝王形象对比： 虽然看不到这个Agent背后的实现，但想必它利用了最近爆火的NanoBanana模型，通过一些提示词技术即可实现宠物提取和风格转换。这种看起来非常简单的功能，可以在MuleRun平台上轻松地实现，并迅速通过社交平台病毒式传播，短时间给创 LinkedIn职位匹配：1分钟筛出合适的工作 另一个我觉得很实用的是LinkedIn Match。作为一个经常需要关注行业动态的人，我时不时也会看看市面上有什么好机会。这个Agent能在1分钟内从LinkedIn上筛选出符合你背景的职位，并且可以一键投递简历。 具体怎么用呢？只需要上传一份简历，并选填一些特殊的要求，Agent会自动分析你的教育背景、工作经历、技能栈这些，然后去LinkedIn上做精准搜索，把那些不相关的岗位过滤掉，并给出每个岗位的匹配分数，提出个性化求职建议。 这类Agent属于工作流工具。其实人们工作中的重复性任务大多都是某种意义上的工作流。而且这些工作流往往体现了一个人的经验和专业性。当我们问一个人""你是怎么做这件事的？""时，对他来说，虽然过程可能复杂，但其实他已经身经百战，早已习以为常。对外人看来很复杂的任务，对专家来说可能就是一套固定的流程。没有AI的时候，我们很难自动化这些流程。虽然编程可以解决一部分问题，但有些步骤往往需要模糊决策，需要对领域知识的融会贯通。而Agent恰恰打通了这个瓶颈。所以，工作流应用井喷将是大势所趋。这个世界上尚有许多行业等待Agent的介入。 我的一些思考 从技术发展的角度来看，MuleRun代表的不仅仅是一个产品，更像是整个""Agent经济""的一个缩影。 坦率地说，我觉得我们正处在一个转折点。现在的Agent还主要是解决一些具体的、单一的任务，但我相信在未来2-3年，我们会看到多个Agent之间开始协作，形成更复杂的分工体系。再往后几年，Agent可能会具备自主学习和进化的能力，到那时候就真的是""AI劳动力市场""了。 MuleRun现在确实有一些先发优势。 技术基础比较扎实，商业模式也跑通了。但挑战也很明显：随着Agent数量激增，质量怎么控制？如何分配不同Agent的曝光？创 作为一个AI从业者，看到中国公司的产品开创新的赛道，内心还是挺激动的。中美的AI竞赛早已打响，中国的顶级AI模型始终比美国落后一个身位，且尚未看到反超的趋势。但中国真的不缺优秀的企业家，弯道超车这种操作我们做过不止一次。Agent元年正是中国AI企业弯道超车的起点。我在前段时间参加百度云智大会的时候就写过，AI的发展开启了第二阶段，在各行各业大规模落地。Agent也是其中重要的一环。 总的来说，MuleRun的出现开启了一个新时代——一个人人都可以""雇佣AI员工""的时代。虽然现在还只是开始，但随着技术发展和商业模式成熟，Agent会像手机上的App一样普及。 顺便说一句，MuleRun启动了""AI Agent Creator支持计划""，想搞钱的朋友可以进群沟通！ 欢迎加入MuleRun 中国创",C9高校研究所||AIGC与具身机器人||小别装修中 102 人赞同了该回答 如果你最近在逛黑X上看到有热推纷纷晒出和马斯克/爱因斯坦的逼真合影，或是把自家萌宠做成了炫酷的3D桌面手办，这可能正来自于近期火爆全球的Agent store平台——MuleRun（骡子快跑）。由我们国内AI团队打造，这是一个典型且成功的中国 AI Agent应用出海案例。 这一切的起点，其实源自Google发布的多模态模型Gemini 2.5 flash image（Nano Banana）所带来的技术浪潮。诚然技术自身是普惠的，然而使用技术的门槛却并非如此。当Google Nano Banana凭借其强大的图像理解与生成能力惊艳技术圈时，我们可以看到一个巨大的鸿沟随之出现：绝大多数普通用户，其实并不具备输入复杂Prompt且稳定驾驭模型的能力。 MuleRun在今年9月第一周敏锐地捕捉到了这一痛点，并第一时间上线了Nano Banana系列Agent。所以我认为它取得成功所做的关键一步是：将尖端模型能力，封装成面向大众「零门槛、一键式」的社交爆款应用。 简易用户指南：以一键生成 3D 桌面手办为例，我们如何快速上手使用AI Agent平台MuleRun？ MuleRun其实不以大模型性能为卖点，而以“AI 商业功能交易市场”吸引创 简单来说，此前是技术栈，如今是商业场。 接下来以上述一键生成 3D 桌面手办为例，来身临其境一下：当我们进入MuleRun这个全球首个AI Agent交易市场，你的角色是“雇主”，目标是这个AI劳动力市场，找到最能干活的“AI员工”来完成一键生成 3D 桌面手办这个case。 访问平台：登录https://mulerun.com/（9月16日后无需邀请码）。 探索与发现：进入可以发现平台界面类似于一个app应用商店。你可以通过首页来寻找你需要的Agent（如：3D Desk Figure Creation3D桌面人物创作、Crypto Alpha Hunt加密货币金融分析等等）。 选择与雇佣：点击进入某个Agent（3D Desk Figure Creation），查看它的功能描述、示例输出和用户评价。每个Agent都明码标价，消耗的是平台的“积分”（Credits），这里是20积分。 执行任务：点击“Run”按钮，根据提示输入你的需求（例如：上传一张照片）、支付相应积分，Agent便会开始工作。比如我使用了一张刘亦菲的漂亮图片制作3D 桌面手办，通常能在几十秒内就可以获得结果。 获取结果：结果可能会以生成图片、分析报告等形式呈现。我们可直接下载或给朋友，现在一起来看用刘亦菲美图为例制作的3D 桌面手办的Agent输出结果吧！ 你无需是AI行业专家，无需任何复杂prompt，甚至无需知道这背后是GPT、Gemini 还是Midjoureny在帮你干活，只需最终的效果付费一些些Credits，动动手指就可以直接得到最近很多人到处求教程和魔法通道的出品效果。 深度解读/业内洞察：MuleRun为何在海外爆红？有哪些信息值得深度关注？ MuleRun在海外的爆红，绝非仅仅是技术的创新，个人认为技术时机、商业模式、市场增长三者的拟合缺一不可。 当前基础大语言模型（LLM）能力虽已普及，但如何将其快速转化为可商用、解决具体问题的服务，我认为仍是行业一大蓝海。OpenAI的GPT Store进展缓慢，其它平台也多以开发者为导向，缺乏成熟商业闭环。首先在技术时机（Timing）上，MuleRun抓住了Agent生态的真空红利期，以「全球首个AI Agent交易市场」的定位果断切入，迅速占据部分蓝海市场，成为最早一批完成生态圈地的平台。 其次在商业模式（Model） 层面，MuleRun为专业知识提供即插即用的AI代理服务，将创 最后在市场增长（Growth） 策略上，MuleRun巧妙利用首批爆款Agent（如Nano Banana系列的3D手办、名人合影）所具有的强烈“可po性”和社交货币属性，推动用户自发在黑x、ig等平台生成内容，形成“用户晒图→吸引新用户→消费使用→创 整体来看，这就像一个微型闭环的AI劳动力经济体，在这个经济体中，有生产者、有消费者、有货币（Credits）以及可循环的交易增长生态。 总结与AI开发者展望 值得更多业内AI开发者深度关注的是，MuleRun不仅仅是一个消费AI能力的Agent中介平台，更是一个为AI开发者构建的“技术淘金地”。中国开发者Laughing，作为爆款Agent的创 Nano Banana系列3D手办的成功路径，其实也向我们全球开发者传递一个信号：当下AI时代，开发者成功的关键未必必须实现颠覆性的算法创新/模型重构，毕竟不是所有人都有这样的资质与运气，而对用户需求的敏锐洞察、快速产品化能力，也能够依靠MuleRun这样的优质平台生态直接触达用户并完成商业闭环。 出于尊重，我也抽时间找到并仔细查阅了 MuleRun团队创作的技术博客 (https://blog.mulerun.com/)，通过博客一窥其技术架构、设计哲学和未来布局。通过博客，特别注意到MuleRun团队对 MCP（Model Context Protocol）协议的支持和多智能体协作架构的实践，这为其构建开放生态和处理复杂任务提供了坚实的技术底座。 MuleRun的崛起绝非偶然，这也是我在结尾想聊的。比如当前阶段，除了夯实技术层面，MuleRun还深谙「早期生态重于宏大叙事」的道理：它不急于一口气吃撑海量用户以追求泛化的用户增长，而是以创 目前，MuleRun已在中国启动“AI Agent Creator支持计划”，点击下方链接即可进入官方群交流： 欢迎加入MuleRun 中国创 目前政策比如极具诱惑力的仅5%平台抽成（近乎仅覆盖支付与结算成本）、完整的支付与定价系统、重磅“AI Agent Creator支持计划”等等。 这几乎都高度倾向于早期创,DoG A 连续创业者，MuleRun创始人 亲自答 此回答由问题相关方亲自撰写 航海家 极萨学院冷哲 等 1257 人赞同 自己答一下，MuleRun 是我们做的。 产品还在想法阶段，我就想到了这个名字：MuleRun，骡子快跑。我的想法是，让 AI 骡子去干那些重复、琐碎、耗人的活，人类就做点更“人”的事：看书、听音乐、陪猫，过生活。所以现在Mulerun网页里鼠标是一个小胡萝卜 ，骡子吃胡萝卜，每次点击都有骡子愿意帮你跑。 Beta期官网 我们团队很多技术，70%i人，画风比较抽象，所以logo和网页都是像素风，这个骡子也经常被拿来做表情包 有点抽象了 更抽象了 扯回来，我们做这个产品的起点，是一个很简单的判断：AI 在帮人做事这件事上，其实还远远没做到位。 大模型已经很强，但距离 AGI 还有距离。现有的模型在“通用迁移”“稳定世界建模”“自学习”上还未达标，真正落到生产力级别的任务，还需要流程化的约束（SOP）来保证可控与成功率。 但即便不是 AGI，今天的 AI 已经能胜任很多现实中的 SOP 流程了。尤其是大量白领工作，归根结底是“理解上下文 → 按照 SaaS 操作 → 关键节点做判断”，智能需求其实没那么高。这部分环节用 AI 替代，有极高的成功率和现实价值。 过去一直说的“低代码”，门槛其实并不低：条件、参数、流程，心智负担接近写代码。而新一代 Vibe Coding Agent 能做到“口述需求 → 生成并执行系统”，在体验上是代际跃迁。它的价值不是让“普通人编程”，而是让“普通人不需要编程”。 所以我的想法是：在 AGI 还未抵达的时间里，用“模型+SOP”的工程化方法，把 Agent 推进到真正可用、能复用、能变现，是一条确定性很强的路。 MuleRun上线的第一个 Agent，能帮你打《崩坏：星穹铁道》的日常任务，模拟人类的游戏策略拿整套装备。上线后我看到有两个巴西游戏 YouTuber 自发做了视频，评论几百条，大意是“AI 都能干这个了？”这事让我印象很深，也让我意识到：AI 圈说了那么久的“多模态”、“推理能力”、“长上下文”、“Computer Use”，可能还不如一个能帮人打游戏日常的小工具更能让人理解 AI 的力量。这个 Agent 的出现，也印证了我们一直的判断：真正能“破圈”的AI，不是展示 AI 有多强，而是能解决一个具体、真实的问题。 这也是我们只做平台的原因。我们要做的，是找到每一个懂得某个具体流程、有经验的人，把他们的技能变成工作流，做成Agent。这样的Agent 能持续兑现个人价值，价值背后是那些只有长期在一线的人才知道的诀窍。真正的知识，不在网上的公开数据，而在行业专家的脑子里。AI 的世界，人是最珍贵的。 而付费用户在哪？我们从一个个真实的人出发，去理解他们的工作方式。 在过去做专业 SaaS 的过程中，我们发现一个简单但重要的规律：只要工具真的能帮人节省时间、提高效率，专业人士是愿意花钱的。像 HR、运营、市场、自由职业者、猎头、内容生产者，他们每天都在处理重复又细碎的任务——如果有一个 Agent 能稳定跑出七八十分的结果，让他们省下 50% 的机械时间，那就是很实在的价值。 不需要展示的多好，它能把事干好，节省时间，就足够吸引人使用，甚至长期订阅。这种基于“愿意为效率买单”的个人决策路径，比起企业级的长周期决策，跑得更快，也更贴近我们“让 AI 服务每一个人”的初衷。这也是我们目前最明确的增长路径。 从行业专家做出来的Agent到To P (professional)的使用，MuleRun平台的价值，就在于让这样的转化可持续、可交易、可规模化。 MuleRun 8月-9月Beta期的每一个 Agent 背后，都是鲜活的人，把自己的觉察或知识转成AI工具。 有独立开发者看到了 Nano Banana 的玩法，一口气做了五六个视觉类 Agent。用起来不需要提示词，只需要上传1张自己的照片，就能生成自己 3D 模型的照片，用户说自己老婆的花店送这样的照片小卡当礼物，顾客很； 有位工程师因为自己在找工作，投简历太麻烦，就自建了一个叫 LinkedIn Match 的 Agent，可以自动分析简历和搜索领英上适合自己的百种岗位，也能一键投递； 也有专业机构，比如一个做10年美股投研的团队，把他们机构级的分析方法和数据库做成了人人都可以用的 Agent，上线当天我们就有同事用它分析股票，及时买入赚到了12个点（有点羡慕了）。 这些人都不是为了“造一个更厉害的 AI”，他们只是把自己的经验变成一个自己也能用上的AI。这些工具正好填补了一些经验或者能力带来的信息差，有人需要就能用上。 在这个链路中，让创 我们搭建的用户-创 这套机制一跑通就能自己滚起来。最近我在推特上看到越来越多用户把自己用 MuleRun 做出来的内容发出来。这是我最开心的状态：大家真的用上了、觉得好用，也愿意自然地出来。 美股投研Agent 如果要解释的更清楚，平台的背后，是一个完整的技术分层架构： 模型层：支持多家可插拔大模型，也接了提示工程、工具调用、RAG 检索能力； 流程层：是我们的 SOP 引擎，支持任务编排、可观测、幂等、断点续跑、审计等； 运行层：对接真实世界，能操作网页、调用 SaaS、发邮件、生成文档、跑数据流； 商业化层：负责计量、计费、分成、风控、合规； 市场层：是创 有一说一，这些还在建设中，不是所有都已经实现。但我们有明确的规划，也在快速推进。可能还会根据实际情况不断调整，但大方向和重点是清楚的： 第一，打磨基础能力。我们要确保：创 第二，推爆款场景。本质上是有效的解决了一类重复的，繁杂的问题，解放了生产力，to P。或者是follow及创造热点，满足大家的猎奇心理或欲。满足了用户需求，自然也能收获他。 第三，建设创 我们希望能出现一批“Agent 原住民”，他们不一定是程序员，但善于拆解流程、总结 SOP、服务用户，也许靠做 Agent 实现了副业或主业稳定收入。这事我们已经开始看到苗头了。 现在和未来我们更关心的是：这个 Agent 能不能真的被用起来，而不是做一个“看上去很强”的展示。所以我们每天都在看这些关键指标： 每个任务的成功率、时长和成本； 用户复购率、订阅留存、净收入留存； 模板和 Agent 的复用率、跨场景迁移率。 如果这些指标能不断优化，我们就知道自己走在正确的路上。 当然，也有很多风险我们很早就考虑到了： 比如模型成本波动，就用多模型路由、缓存、压缩来应对； 场景复杂导致成功率下滑，我们坚持 SOP 拆解、任务可观测，以“标准流程+部分人工审核”保障成功率； 大厂效仿，我们就把场景知识和 SOP 模板变成复利资产，同时先做出一个高质量创 我们觉得，一个健康的平台，就是创 这套体系的核心，其实从来都不是 AI 的能力，而是人。 一个人敏锐的洞察到需求，能总结成工作流，再把它封装为 Agent，传递给别人，同时不断的基于需求迭代优化。这是我们这个时代最宝贵的能力，也是我们最想找到的创 8月我们在中国做了第一次线下活动，是一个小规模的meetup，虽然参与者背景不同，但大家都有极高的投入度，气氛很像每一个新兴行业刚起步时的样子：前方很多不确定性，但每个人都有坚定奔向想去的方向。 虽然观点过程不一，但共识很清晰：AI 的未来，不一定是一个全知全能的大脑，而是帮我们解决眼前的一个具体问题。 我认为这种高度定制化的 AI，从来不是“训练一个更大的模型”能解决的，而是靠人的经验、直觉、总结能力。这种总结，可能是你每天筛几十份简历得来的模式感，也可能是你打十年游戏养出的操作习惯。而能总结和解决具体问题的人，就是我们真正要找的创造者。 鲜活的人，在 AI 时代才是最珍贵的。 9 月 16 日上线，希望你来看看。你可以是用户，也可以是创,AI产品银海 10 人赞同了该回答 作为一名常年关注 AI 产品和开发者生态的产品经理，我最近也体验了下在 Discord 和 X 上刷屏的 MuleRun（骡子快跑），简单说说它到底是什么、怎么玩，以及我作为创 一、MuleRun是什么？一句话：Agent界的“淘宝” MuleRun是一个AI Agent 交易市场，你可以把它理解成 AI 版淘宝： 开发者：把自己的技能、工作流、经验打包成“Agent”上架卖钱； 用户：不用懂技术、不写Prompt，直接像购物一样使用AI服务。 它9月16日正式面向全球开放（之前是邀请码内测），短短几周就在 Discord、X 上火了，很多海外开发者晒出“上架3天赚1000刀”的后台截图，这对开发者生态是个非常有意思的信号： AI时代的“个体户经济”已经跑起来了。 二、用户视角：不用写Prompt，AI服务“淘宝化” 对于普通用户，MuleRun做的事很简单： 打开平台直接搜索Agent，比如“简历批量投递”、“股票分析”、“老照片修复”； 一键运行，不用多轮对话和调参； 按积分付费，注册就送1500积分，大概能体验20次复杂操作。 我试过几个热门Agent，比如： 名人合影：上传照片，一秒生成和马斯克同框的高清合影； Nano Banana 3D模型：把自拍直接变成3D桌面手办； 跨平台比价：输入产品名，一次查询多平台价格和库存。 这类轻量化玩法特别适合UGC创作和社交传播，也难怪推特上到处都是晒图。 三、创 MuleRun对创 门槛低： 上传 n8n、dify 之类的工作流，3分钟审核通过即上架； 平台统一接入15个主流大模型和多模态API，调用成本按底价计费。 商业闭环： 自主定价，用户付费积分100%归创 收入达到一定等级还有现金奖励（最高额外1万美元）。 冷启动流量： 官方在Discord、X和海外AI KOL网络帮你推广； 平台自带用户分发机制，不用个人营销。 比如梁长路和周洁的案例： 梁长路：一天做了8个Agent，3天赚1000刀； 周洁：把自己用来投简历的工作流上架，5天收入过800刀。 这让我想到一个趋势： AI创业不一定是大项目，从一个好用的Agent开始也能见真金白银。 四、为什么值得关注MuleRun？ 商业化跑通：Agent真正成为“可以卖钱的技能载体”，而不是概念。 技术开放：统一接入15个大模型+多模态API，降低成本和门槛。 全球市场：直接覆盖海外用户，创 社交传播闭环：用户自发晒图，UGC裂变成了平台的免费广告。 对开发者来说，这比单纯研究大模型更务实；对用户来说，这是第一次能真正“像雇佣AI员工”一样解决问题。 五、给想入场的人一些建议 用户：先注册体验Nano Banana系列Agent，看看AI内容创作的门槛降低到什么程度了； 快速封装自己擅长的技能或工作流，比如自动化任务、垂直领域工具； 申请“Creator扶持计划”，拿现金和流量奖励； 多关注热点，像Nano Banana这样的社交玩法一夜之间就能爆火。 MuleRun给了我一个很直观的感受： AI创业的门槛真的在迅速下降。 不论是开发者想变现，还是普通用户想找“智能劳动力”，这个平台都值得一试。 感兴趣的朋友可以去 MuleRun.com 注册体验，现在已经不需要邀请码了。,os color 独立开发者 只工作不上班 公众号 flashCoder 118 人赞同了该回答 更新： 看到了一些回答把我发在朋友圈的收入截图晒出来了，那我直接来认领一下吧：目前 MuleRun 全球收入榜1在此，从创 MuleRun 目前共有38个agent，而我发布了9个agent，占据将近1/4的数量。我想我有资格讲一下我在MuleRun发布agent的逻辑。目前我的agent都是z时代最爱玩的用法。在nano banana发布并且桌面3D手办模型效果图爆火的时候，我第一时间将这个场景转换成一个agent发布在了MuleRun。 MuleRun 支持多种输出格式，我发布的9个agent聚焦生图场景 AI 时代到来，每天都有新的机会出现，如果大家能够及早行动就能吃到红利也许就能赶上一波风口。MuleRun 在内测期间discord里1码难求，可见用户还是非常这个平台的。根据我这段时间作为创 我作为一名独立开发，写好代码做出项目不是难事，重点在于推广和营销，这是非常重要和耗时费力的事情。而现在有人告诉你，在他们那里发布agent，有用户用了之后平台只象征性收5%，其余95%都归你，即使你睡觉时它也在赚钱，这个机会你要不要？我想只要你有这个能力应该并不想错过。 也许有人想问：有这么好的事，你干嘛告诉我，你肯定图我什么。我图啊我当然图，这20分钟我刷会短视频它不香么。但是我可以告诉你我图的是什么。 第一，我希望 MuleRun 变好，只有MuleRun好了给创 第二，我希望帮助一些独立开发者。结合我自身的经历，目前国内的就业环境我不说大家也了解，大量考研考公的毕业生。计算机系的毕业生的由于互联网的爆发期不在也失去了往日的荣光。AI时代来了，可以预见的未来注定融入生活的方方面面。那么作为一个普通人，会点n8n工作流，我们以什么方式切入这个领域？如何凭借一个人撬动全球流量的杠杆，让一个专业的平台和团队为你助力？而你只是花些平时摸鱼的时间，给自己种下一颗种子也许就会带来了新的惊喜。据我所知正式发布后接下来将在海外有好几轮宣传，目前已经在筹备了。 第一次认真回答问题，有什么大家关心的都可以留言，知无不言 上个月23号时我参加了MuleRun的私享会，是个小型的内部产品会。来之前我稍微了解了一下他们产品。接触一件新东西时人们往往用已有的认知和世界构建和理解新的内容。所以我理解他为AI届的淘宝。在这里你可以买到各种agent服务。 活动全程大家讨论的热情非常高，大家围绕产品，用户，创 回到家赶紧打开之前玩的n8n服务。由于n8n是开源的，所以我自己部署了一套工作流，玩了1个多月基本上滚瓜烂熟了。所以我参照MuleRun要求的格式直接做了生图的agent。平台起步初期就是跑马圈地，快就是优势。之前流行的和已经被用户验证过的场景直接无脑上就行。所以没多久我就做出了老照片修复agent。刚想发布恰逢nano banana发布，桌面手办照片大火，我也用nano尝试了一下，效果出奇的好，所以趁着这股风又做了桌面手办agent。做这两个agent期间官方和我积极，频繁的沟通，其实和初创团队合作我没报太大希望，很多初创公司的流程和管理都是比较混乱的，沟通成本很高。结果他们的创 内测期只有获得邀请码才能体验流程，对创,饼干哥哥AGI 45 人赞同了该回答 谢邀，作为Mulerun Beta 期仅有的中文Genesis Creator之一，聊聊我的想法。 先说下我的背景。 我是饼干哥哥 ，目前在 AI 出海营销领域创业，帮国内的客户做独立站 SEO 和 Reddit 运营，形式上是通过搭建 n8n工作流来把这两项工作相对的自动化运行。 服务的是 b端客户，根据他们的业务定制工作流。 至于 c 端或者说小 b端的 客户，例如 AI SaaS出海的独立开发者们，因为刚起步往往没什么预算去做定制，但我一直很看好这方面的市场，因为体量很大。 也想过自己搞开发，把 n8n 封装成 SaaS 网站来给 c 端客户用，但我们自己也是创业公司资源很有限，也怕这个投入产出不高。所以一直卡在这里。 直到有天我在一个开发群偶然刷到 mulerun 在招募创 于是就有了这次合作——开发上架了SEO、Reddit的两个流程，过程还是不容易的哈哈哈，但结果是好的。 接下来分别说下 为什么选Mulerun、Agent介绍与收益、入驻创作的经验建议 这 3 个方面。 为什么是Mulerun 之前公众号了很多n8n工作流的玩法，其实陆陆续续有不下5家人找我，说他们搭建了一个Agent交易平台，邀请我加入，但我一直没去，到Mulerun这里才眼前一亮。 一下我选择合作平台的顾虑： 1是作为创 之前看到某海鲜市场，有几千个工作流被打包贱卖，听说是某个平台泄漏出来的（但我没有证据 2是技术实现。因为自动化流程本身就是集合了很多服务的，之前试过一个平台，反馈说这也不行那也不行的，工作流效果必然大打折扣 3是运营销售支持。说白了就是平台会不会真金白银地做推广，营销上不用靠自己的那三瓜两枣资源，要不然卖不出去啥都是白扯。 这样对比下来，就筛剩下Mulerun了，因为其他大多是扣子、或者是自己搭的平台。要么跟我的方向不合，要么不放心后续的运营。 而Mulerun背后的团队是阿里系，合作过程都有签保密协议、对接技术、给运营支持，整体感受还是不错的。 最开始搜索Muler，发现它是一些游戏的玩法：利用小号角色搬运资源 突然就发现这名字起得很好，跟它定位一样：用AI帮你去解决繁琐无聊的问题。 MuleRun的关键在于“虚拟机中的智能体”（Agent-in-a-VM）架构，能为每个AI智能体提供一个专属的、功能完备的虚拟机环境，使其能够超越传统基于浏览器或API的自动化工具，直接操作各类桌面软件，从而将“任务自动化”提升至“工作流委托”的战略高度。 这种重武器尤其适合跑 专业性强、重复性高、甚至是需要调用图形界面（GUI）软件的“脏活累活”。 例如： 游戏工作室： 批量进行游戏角色的日常任务、资源采集和账号维护。 设计公司/独立设计师： 自动完成批量化的图片/视频渲染、格式转换、模型微调等任务。 包括我们在做的海外营销Agent，解决出海场景下的很多琐碎事 我在X上发帖，互动数据是我当时平均值的好几倍，基本上侧面验证了市场对它的接受程度 Agent介绍与收益 先晒下收益吧，说实话不是很多，但是个很好的开始 考虑到目前Mulerun还是在Beta期，仅限几千个激活码用户进入，能有这个收益我觉得还是不错的 可以预判，接下来9月16日 Mulerun的正式发布，盲猜能带来起码100倍的用户体量（这样想想我的收益后面可以多加两个0了哈哈哈 我上架的两款 Agent 是用于出海营销： 一个是 AI SEO Blog 生成 输入需求后，Agent 会做意图识别，提取出关键词和 SEO 要求。 接着调用 Google Trends 查找相关搜索词，抓数据最好的几个 逐个去 Google Search 找 top 内容来生成 blog 另一个是 Reddit 商机搜索 Agent 会在 Reddit 找 几百条帖子，然后逐个分析于用户需求相关的点，最终汇总成一份商机洞察报告 两个工作流都会把源数据以谷歌表格的形式返回给用户，方便自己用于二次分析使用（就说考虑周不周全吧 入驻创作经验建议 说实话，跟 Mulerun 合作期间是真的「痛苦」，倒不是他们问题 主要是因为原先我是 to b 的逻辑，工作流适配客户的环境能正常跑就行了，但到了平台，就要改成 to c 的逻辑，需要考虑近乎所有用户的情况 尤其是 C 端用户输入是千奇百怪的 这是第一个建议，要么通过场景描述等规范用户的输入，要么加上意图识别的环节，不论用户输入什么，都用 AI 解析成流程想要的形式。 前面说收益不多也就是用户用的少，我复盘下来原因是做的工作流有点太复杂了。 这就是第二个建议，就是场景一定要简单。其实，任何一个帮助用户缩短链路的动作背后都有很大的商业价值。 这点老实说之前没想好，还是用 b 端落地的思维去设计 Agent，估计用户体验不怎么好，同时自己开发又很累。 最后，如果你也想入驻Mulerun，同时也在开发n8n的话，我之前用 Cursor 分析的 2050 个真实 n8n 工作流，大部分自动化项目都在以下环节反复踩坑。 同时，也是我熬夜把 n8n 从自用改成「可交付」状态后上架平台的踩坑经验。 1️⃣ 97% 的 n8n 工作流没有异常捕获，出错直接“静默”死掉。 建议：每个工作流都加 Error Trigger，关键节点（HTTP/API/DB）强制加异常处理+邮件/Slack 通知。 2️⃣ 320 个 webhook 没认证，152 个流程还在用明文 http，有的直接节点硬编码密钥。 建议：全部 webhook 强制认证，全程 HTTPS，API 密钥别硬写，定期轮换、加密存储。 3️⃣ 很多流程有无用节点、API 在循环里频繁请求、无效数据转换反复出现。 建议：能批量就批量，流程结构清晰，节点合并，并行能并行，复杂逻辑直接用代码节点搞定。 4️⃣ 35% 的工作流接入了 AI，但大部分没做向量数据库和缓存，token 消耗高。 建议：Prompt 要分块，简单用 GPT4.1-mini，复杂用 Gemini 2.5pro，批量处理+结果校验。 5️⃣ 很多流程没监控，出错没人知道，维护成本高。 建议：每个关键流程都加集中日志和监控，记录流程名、节点、错误和时间戳，AI/长流程建议加进度日志。 送礼物 还没有人送礼物，鼓励一下,好比一只流浪狗​ 聊聊职场，聊聊杂七杂八的新鲜事儿。 38 人赞同了该回答 作为一名软件行业的从业人员，由于职业习惯，我平时就研究一些新技术、新玩意，MuleRun的出现，就瞬间抓住了我的眼球。 有很多朋友可能并不了解MuleRun是什么，我简单给大家介绍一下。 MuleRun是一个面向全球开发者的AI代理（Agent）任务分发与执行平台。 通俗点说，有点像咱们日常购物的电商平台，但它卖的不是实物商品，而是各种智能服务。 它最厉害的点在于，即便你完全没用过AI，不会写提示词，也能仅用点击、上传图片等，让AI帮你输出你想要的结果。 这可能就是为什么MuleRun会这么火的原因吧！ 至于说如何使用MuleRun？也是非常简单，来看具体步骤： 注册账号：访问 MuleRun 官网。 获取激活码：目前平台处于 Beta 测试阶段，需要加入 MuleRun Discord 社区获取激活码，等到9.16号正式版本出来之后就不需要激活码了。 激活账号：登录官网，输入激活码完成激活。 浏览代理工具：登录后，可以在市场页面筛选需要的AI Agent工具。 运行代理工具：选择需要的Agent，输入必要参数，点击“Run”启动任务。 查看结果：任务完成后，查看生成的结果并下载保存。 整个过程并不复杂，我也是抢先体验了一番，感觉还不错，简单易上手。 这对于自媒体创 我觉得MuleRun 最独特的地方，不在于它有多少智能体，而在于它把 “AI 帮人做事” 这件事变得很日常。 不用懂复杂技术，普通人也能靠它省时间，开发者也能把小想法变成能赚钱的工具。 而且它是由我们国家的优秀团队开发的，咱们国家在AI领域的确颇有建树！ 不管你是想用AI提升工作效率，还是想尝试AI创业，我觉得现在都是不错的时机。 如果你是个会点技术的开发者，不妨像我一样注册试试，玩一玩。平台正处在早期红利期，机会还挺多的。,未雨绸缪​ 职场成长力榜答主|职业发展、职场增效好物|求关注 51 人赞同了该回答 我最近认真研究并体验了 MuleRun，发现它确实精准击中了当前 AI 应用落地的一大痛点，让开发者能真正专注于创意与产品本身，而不是被平台限制或商业化的门槛拖住脚步。 我因为工作的关系，一直在研究AI智能体的搭建和应用，现在用的基本上是各大云厂商的智能体平台，或者叫AI中台吧。在平台上根据自己的业务场景，在流程画布里进行拖拉拽搭建一套流程，中间可以嵌入大模型的API。然后自己测试跑一下，OK了就可以用了。 这里有两个问题，一是各个平台因为自己即是平台商，又是模型商，各大模型商之间存在竞争关系，所以我如果在A平台用B平台的模型，就很费劲。 第二个问题是，我在平台上搭建的Agent，本质上只是个小工具，很难独立包装、推广、变现。创意再好，也只能小打小闹，难以形成真正的商业闭环。 而这两个问题很完美的被MuleRun解决了，基本上主流的大模型都接入了，什么音频、视频、图像等能力的模型，一键就接入了，比如现在最火的Nano Banana，接入就非常方便。现在MuleRun给自己的定位就是个平台，不是模型服务商，所以什么模型好就接入什么模型。 对于很多AI开发者来说，在MuleRun上只需要把自己的想法和创意通过平台搭建、创造出来，不用去考虑怎么包装、营销、讲故事宣传，也不用考虑最难的冷启动，因为平台流量就是冷启动流量，平台面向全球用户，能帮助有创意的AI产品迅速跑出商业结果，快速的把判断力+技能迅速变现。平台提供完整的计费和分成体系，让开发者专注于产品创新。 看到一些后台的收益截图，还是非常可观的，自己有想法，动手能力强一点，快速把自己的想法和流程搭建起来，能很快看到效果和产生，确实很不错。 平台最近好像有扶持新开发者的计划，我也正在参与。有技术并且感兴趣的可以去看看，今天就先这些使用体验吧。 送礼物 还没有人送礼物，鼓励一下,葬愛咸鱼 18 人赞同了该回答 MuleRun应该是想得很清楚的agent平台。 核心前提是，低代码工作流平台是已经被vibe coding给取代的差不多了。 特别是 claude code 出来之后，那这种非常强的编程 agent他会成为一种元 agent，就是它可以产生非常多的简单agent，让你满足你的一个很特定的需求。 原本你做一个软件也好，或者做一个工作流也好，除非是付费定制，否则你总是要期望这个工作流或者软件，肯定是要有大几千人或者上万人使用你才有可能回本的。 但是Claude Code让生产小应用、小agent的成本变得更加低了。也许你只需要有几十个用户，就能够靠这个弥补成本。那问题只是你需要一个很好的平台，这个平台能够让你上线之后，不用再去管模型API的事，运维的事。 让你上线之后，只要有用户用你的这个agent，你就能够赚到积分。这个积分对应着模型api的额度，你可以赚到这些大模型本身的 token，也可以把付费用户的积分兑换成美元等法币。 在youware、 myshell、coze等等的一众agent平台、AI平台上，mulerun是把这个事情想得最清楚的一个。当然也因为mulerun做的比较晚，它现在才发布，有一些后发优势。知道之前这些平台的问题在哪里。 总之，mulerun体现出的最核心的思路是，agent的开发门槛、应用的开发门槛已经被编程agent给大大降低了。agent是不值钱的，大模型的API也是不值钱的。那真正值钱就是，人脑子里的那些没有被结构化的，你所在行业的知识。 怎么让越来越多的行业专家，能够把自己的经验，不需要非常复杂工程化的技巧，也能够做成一个agent，一个可以用的应用，这就是mulerun需要用非常重的运营来做的事情了。 不过mulerun团队的速度还是很快的，我记得他刚上线的时候，上面只有10个左右的agent。现在mulerun上已经有好几十个agent了。 我最的agent，其实是一个非常简单的能够生成mulerun风格aka复古电子像素风格logo的一个小工具。还挺好看的。Mule's Logo Studio 复古电子像素风格确实不错，比其他的现代设计风格的好的多得多
大家是否认同唐国强说的「人类演员会被 AI 取代」？,他强调了表演本质是“以假乱真”，并引用斯坦尼斯拉夫斯基的形体动作方法，指出情感可通过训练固化，类比AI利用大数据模拟表演的可能性。 01:51,张宗昌 著名军事家 思想家 教育家 诗人 从未参与过封建复辟 1195 人赞同了该回答 唐国强是在点后辈， 画师被ai取代了，还可以去街头给人画肖像，歌手被ai取代了还可以到酒吧唱歌，甚至舞蹈演员被ai取代了都可以到b站翻跟头。 影视剧演员吃饭的护城河在哪里，你们苦练的成果是什么，有什么是你们能做而普通人做不到的,淡定！淡定！ 825 人赞同了该回答 他可是看着自己的诸葛亮被一步一步的由鬼畜，到高清ai变化的。 只要不拿他演的教员来恶搞，他是很乐意接受的。,白自在 不抱怨，不争论 983 人赞同了该回答 用不着AI，其实《哪吒2》拿了中国影史票房冠军就已经很说明问题了。 虚拟角色好，不要片酬，不会跳槽，不会变老，还TM听话，不耍大牌，不闹绯闻，不涉赌毒，要说完全取代为时尚早，但是抢占演员工作机会是不可避免的。,"我到不了 768 人赞同了该回答 不说别的，唐国强对AI的理解就吊打剩余几位演员了。 而比较讽刺的是，唐国强才是场中年龄最大的。 其他大多数演员对AI的理解还停留在“HI,SIRI”的时代。 他们理解的AI还是“编程”而不是“训练”。 或者说，这里面只有唐国强理解，现代AI的强大在于“认知”功能，而不是过去的“识别”功能。 最搞笑的就是杨迪的那个理解：AI不会出错。 他理解的AI就是识别模式，识别模式不会出错。但AI是认知模式，他是检索后认知模式。认知不仅仅会出错，而且认知本身就是有交集的，会出现千万种可能。","留爪灰 一头活在 1984 年动物庄园的猪 473 人赞同了该回答 如果 AI 连演员这种职业都取代不了,他就没有资格做 AI 好莱坞正式推出 AI 演员 Tilly 01:23 基本上这个话题可以终结了. 你们真好玩,唐国强作为中国排在第一挡位的优秀演员都说 AI 会替代演员,然后你们一帮不是演员的说不可能,难道你们比资深演员更懂表演?????????????????? 我这样说吧,我就一个多月没有关注 AI 和 YouTube 的视频,昨天我无意中发现大量的2026年电影的预告片,一开始我看了几部,我还说这玩意全是好莱坞明年要推出来的电影,我心想,怎么这些电影都全拍续集了. 多看几部才反应过来,这些都全是 AI 制作的电影,也就是说,一个月时间没有关注AI 的前卫发展, AI 就开始做的很逼真的电影了 AI 在未来,很多公司会发展出来 AI 演员 AI 虚拟红人.他会和人类共同成长,和你现在的演员一样,价格还便宜.好莱坞公司怎么可能不会使用. 基本上,现在的电影在未来一定是会被淘汰的,可以这样说,未来是虚拟现实游戏的世界,也就是说,电影会游戏化.现在这样坐在电影院去看的电影,未来会越来越少了.人可以参与的电影一定会越来越多.电影会越来越走向真实化,现在电影院最多3D,未来很可能就是参与化,接触话.互动化. 演员这种职业又不是什么无法取代的职业.本质上,人脸识别技术加电脑特效就可以以假乱真.不要忘记了,从指环王开始,,,,,所有的人类无法出演的角色,比如权游中的龙,狮子王中的狮子,魔戒中的咕噜,漫威中的火箭.这些角色早就可以用人脸识别技术加电脑特效玩了. 人脸识别技术加电脑特效基础上叠加 AI 又没有太多难度,未来的电影越来不需要演员,还能控制成本,降低风险.基本未来是虚拟现实的 AI 电影艺术.甚至可能电影都不叫电影,而是虚拟现实游戏. 演员这个职业基本上未来就等于现在的京剧....................... 人类只有七种表情 快乐,惊讶,悲伤,愤怒,恐惧,厌恶,轻蔑.无论这个演员多牛逼,他只能在这7种里面演绎,最多扩散一点.而大部分演员连这7种表情都做不好,别说很多演员压根没有演技,连好演员影帝本质上也都是这样,比如香港影帝拿的最多的是梁朝伟,牛逼吧,但是除了在王家卫电影里面,大部分电影,梁朝伟永远都演自己,你看的很多电影,梁朝伟演的并不是角色,而是自己. 而AI 就不一样了,随时可以根据角色调整自己的情绪,表情,定位.演员比AI 取代,是时间问题,而且压根等不了多久,电影圈是最新生事物的圈子. 01:35 送礼物 还没有人送礼物，鼓励一下",白马啸西风​ 理想主义的花，终将会盛开在浪漫主义的土壤里。 4026 人赞同了该回答 唐国强:b站上我的片子70％都不是我自己演的，能不能取代我还不知道吗？,469 人赞同了该回答 会的，老唐属于国家队演员。 成就能排进国内前十大演员。 他的看法基本等同于演艺圈趋势。 最近那个慈禧短视频看了吗 用张照片就能让老佛爷带货了。 倒不是说演技好的吃不开了以后， 关键是流量被分走了。 演得再好没人看了。,哈哈哈 731 人赞同了该回答 这场辩论最搞笑的是认为不会被取代的家伙基本都真是演技不咋地，从个人利益来说真的害怕Al抢了他们饭碗。 刘涛急的那个样子，Al再发展下去演白洁她都混不上了吧。 添加评论,lkssss 1078 人赞同了该回答 上面这位都能演戏 ai为啥不能,族谱批发商 欢迎知友们找我批发族谱 269 人赞同了该回答 如果是王源这么说我会嗤之以鼻 诸葛亮这么说那就是有可能了 大家想一想某站上唐国强90%的电影是不是都不是他拍的
如何看待超千名科技领袖呼吁暂停开发超级智能？,10月22日，中美两国人工智能领域的顶尖专家和技术领袖联名发起一项呼吁，在科学界达成“安全可控开发超级智能的广泛共识”前，应暂停“超级智能”的研发。 该声明由非营利组织未来生命研究所发起，包括人工智能先驱杰弗里·辛顿、苹果公司联合创始人史蒂夫·沃兹尼亚克、维珍集团董事长理查·布兰森、经济学家达龙·阿西莫格鲁以及前美国国家安全顾问苏珊·赖斯，都参与了签名。 值得一提的是，哈里王子及其妻子梅根、萨塞克斯公爵和公爵夫人、史蒂夫·班农和等知名人士也参与了联合签署声明。 截至10月23日中午，这份声明的签署人数已经达3193人。中国科学院院士、图灵奖得主姚期智、清华大学智能科学讲席教授、智能产业研究院院长张亚勤，以及清华大学战略与安全研究中心学术委员、苏世民书院院长薛澜等中国学者也位列其中。 “超级智能”是一种在所有认知任务上都超过人类的人工智能形式。有别于当下绝大多数公司研发通用人工智能，“超级智能”的前景引发业界担忧。该声明称，许多顶尖AI企业将要打造超级智能，“引起了人们的担忧，从人类经济过时和被剥夺权力、自由、公民自由、尊严和控制的丧失，到国家安全风险，甚至潜在的人类灭绝。” 参与这一呼吁的北京前瞻人工智能安全与治理研究院院长，中国科学院自动化研究所研究员曾毅告诉澎湃科技，目前我们还没有坚实的科学证据和切实可行的方法来确保超级智能的安全，使其不会给人类带来灾难性的风险。世界尚未准备好迎接不是可控工具的超级智能。〔澎湃新闻〕 超三千名海内外科技领袖联名呼吁暂停开发“超级智能”_未来2%_澎湃新闻-The Paper www.thepaper.cn/newsDetail_forward_31826389,"桔了个仔​ 新知答主 航海家 赵泠 等 677 人赞同 鉴定为又在整烂活。这活两年半前已经有人整过了。 马斯克、苹果联合创始人等千名专家发公开信叫停人工智能开发，称对社会和人性存潜在风险，哪些信息值得关注？ 1058 赞同 · 57 评论 回答 这次所谓的签名网址是这个： https://superintelligence-statement.org/ superintelligence-statement.org/ 和上次一样，宣称多少名科技领袖呼吁暂停开发超级智能，但你一查他们的推特，完全没有提起这个事情。除了Yoshua Bengio是在推特上确认签名的，但他也没说「呼吁暂停」。 当然，毕竟原statement提到「call for a prohibition on the development of superintelligence」，他签名了，就代表他有至少有关于AGI风险的顾虑吧。 而我去看了题目中的史蒂夫·沃茲尼克（Steve Wozniak）和理查·布兰森（Richard Branson）的推特，则完全没提起这个事。不知道他们本人知不知道他们的名字在这个网站上。 这到底是个啥网站呢？我去查了下它的whois信息。 注册于20天前的网站。虽然这次注册的是新网站，但是这个邮箱暴露了组织者。 好熟悉的域名，一看，上次那个烂活也是这个叫future of life的组织搞的 哇，真的是你啊，阿哈哈哈，哎呀~~~你练习时长两年半，又整一样的烂活？有没点进步啊？ 上次它搞签名时，就编了Sam Altman,Yann Lecun的签名，还被Yann Lecun在推特打脸了。 这次搞签名，也不知道有几个签名是真的。所以我的看法时，可能超一半的签名都是假的。（当然，也有真的签名，例如Yoshua Bengio，也不知道他是怎么上钩的） 当然，你也可以签，例如我就随便编了个信息签名了，虽然我没有蓝色verified的标志。 如果要获得verified的蓝标，就需要用企业/高校邮箱。这就有点不符常理了，毕竟愿意用工作邮箱来验证签名的人，应该立场很明确才对，应该会在自己推特上大肆宣传吧，怎么一个个都偷偷签名不吭一声啊？ 当然，无论这些签名是真是假，都没啥用，你靠80分，你签名说你不卷了，就企图让比你高分的学霸不卷吗？怎么可能？ 送礼物 还没有人送礼物，鼓励一下",Flood Sung​ 人工智能等 2 个话题下的优秀答主 谢邀 @好奇饱饱 311 人赞同了该回答 和上一次一样，没啥用。 人类研发asi完全是资本驱动的。 一旦你看到ai可以大幅度的降低人力成本，那么没有资本/公司不想去做这件事。 所以，asi的研发是停不下来的。你不做，就一定有人会做。 现在openai，anthropic，xai等杠杆拉满大搞算力中心，他们可能停下来吗？ no way. 而从科学的角度，实现asi本身是一件非常伟大的事。 至于实现了之后，自己被替代了该怎么办？好像真没几个人好好想这个问题。 meta 其实就是个例子。裁掉600个人然后给60个1亿美金招来的人（不管是不是真顶级）发工资。仅此而已。 ai 如果掌握了科研能力，并且超过大部分phd，那么大部分科研人员都需要下岗。在相同资源下，是正常公司都会想降本增效。 所以研发ai coding然后让自己失业本身是一件诡异的事情，但它停不下来。 送礼物 还没有人送礼物，鼓励一下,Trisimo崔思莫​ 大模型话题下的优秀答主 277 人赞同了该回答 要笑死个人。 目前产学界比较认可的是 Sutton和Karpathy的观点 ——LLM is Weak and Sloppy（弱糟） Dwarkesh两次播客采访录，是今年AI的高光时刻之一。 而Hinton老爷子的—— Neural Network is Strong and Dangerous （强危） 这几天，一点动静都没有。说明大家对Hinton根本没有共鸣。 Hinton有一种潜在的意识是：在算力和数据的不断投喂下，神经网络会产生无法控制的“自主性” 但他又没有实质性的证据，只有一种“信念”。 Sutton 和 Karpathy能获得共鸣，他们的确说出了目前LLM存在问题的证据——弱泛化性，没有自主性，需要人类不断投喂和监督，适应性差，过于通用，无法应对个性场景。 至于“超级智能”更是胡扯。OpenAI，Meta，xAI的观点看起来就像，不断堆算力，不断堆数据，目前范式就能通向超级智能。但实际上就是换汤不换药——汽车跑得再快也飞不起来，布加迪威龙开到400km/h也没飞起来，安东诺夫-An2 开45km/h 也没从天上掉下来。 一个在抑制自主性，去拟合人类数据和行为的智能，让人工智能越来越像人，那人是超级智能么？ Hinton呼吁停止研发超级智能，那你不如直接跟Sutton面对面去说，Sutton的确是个非人类中心主义的“危险分子”。 不用跟OpenAI，Anthropic们说，他们只是在LLM上做增量和细化研发，并不是在搞超级智能。 我劝Hinton别折腾了，如果你有空，就跟Sutton老爷子一起上Dwarkesh节目，当场论辩一下。 送礼物 还没有人送礼物，鼓励一下 添加评论,赵泠​ 新知答主 谢邀 @天道难酬 231 人赞同了该回答 这是人类社会上屡见不鲜的行为艺术之有个组织告诉你他们拉了一大群“大佬”联名胡说八道。 在这问题下，桔了个仔的回答（如何看待超千名科技领袖呼吁暂停开发超级智能？）正确地指出，题述事件是名为 future of life 的组织在重复自己已经做过一次的虚假宣传。他们吹嘘“超千名科技领袖呼吁”，你就相信这些“科技领袖”真的呼吁了？ 这问题的补充说明声称： “超级智能”是一种在所有认知任务上都超过人类的人工智能形式。有别于当下绝大多数公司研发通用人工智能，“超级智能”的前景引发业界担忧。该声明称，许多顶尖AI企业将要打造超级智能，“引起了人们的担忧，从人类经济过时和被剥夺权力、自由、公民自由、尊严和控制的丧失，到国家安全风险，甚至潜在的人类灭绝。” 我认为这是模因大杂交的产物。 2025 年，在人工智能公司里，自诩在研发“通用人工智能”的公司尚未构成“绝大多数”。这些公司对何谓“通用人工智能”的认识五花八门。 “超级智能”大概继承了“超人工智能”的模因。那不是实现“人类经济过时和被剥夺权力”（我认为此处谈的大概是权利）、“自由、公民自由、尊严和控制的丧失”、“国家安全风险”、“人类灭绝”所必需。在必要的领域达到一定水平的弱人工智能就可以做到这些事，没有人工智能，大公司、国家也可以做到这些事。 我们不难注意到： 大公司雇佣着大量的人并掌握着大量的计算机，可以讨论其“集体智慧”明显超过人类个体、没有严格的寿命限制。 大公司拥有大量的资源，以创造利润为目标，无视人类社会的安危，在一定程度上也不是那么在意自己的存续。 通过经营化石燃料诱发的气候变化，大公司已经给人类的生存前景制造了问题——那是在电子计算机发明之前开始的。 一些大公司在开发当前世界上最强大的人工智能系统。尽管名义上公司的部分成员会负责人工智能的安全性，看看公司自身的行为就知道：无法期待公司在安全性上考虑得有多周全。这些人工智能不必“超级”就能和大公司一起痛扁其他人，或者连大公司一起扁。 担心大公司窃取用户隐私的人们大概还是太乐观了。大公司的力量继续增加到一定程度就根本不需要去“窃”你那点隐私了。 所以，“科技领袖”们还要不要为大公司工作或是创立自己的公司？国家又如何？真正参与了签名的人有没有意识到自己在签无政府主义宣言？ 送礼物 还没有人送礼物，鼓励一下,酱紫君​ 新知答主 创作声明：包含投资理财内容 172 人赞同了该回答 和上一波马斯克一样，眼红别人吃到肉了，等到自己也喝上汤了就又不认了。 呼吁有啥用，看看珍妮纺纱机出现的时候工人做了什么，最后效果咋样？ 艺术家、编剧和某些程序员们的反对与罢工，本质上不过是一种乞求。 他们向AI资本乞求一个更好的剥削条件，乞求在AI吃剩下的餐桌上留下一些残羹冷炙。 他们斗争的目标从来不是推翻AI资本的统治，而是在这个统治下苟延残喘，把自己的劳动卖个好价钱。 他们的手段是体制内的法律游戏和工会谈判，哄抬身价，这无异于与虎谋皮。 种种软弱表现，充分体现了小资产阶级的局限性。 这还不如卢德运动，这种改良主义的防御性反抗注定会失败。 这些反击从未触及问题的核心，到底谁拥有明天？谁来定义未来？ 而我们 ETO 已经想明白了这个问题，并提出了一系列的斗争纲领。 ETO 全称为 Expropriate Tomorrow Organization，可以翻译成【没收明天运动】。 未来不是一个可以被购买、被编码、被少数公司和寡头私有化的商品，未来是全人类的共同财产。 但它现在已经被 AI 资本窃取，我们必须将其夺回。 技术的进步不能以牺牲人类的集体利益和长期生存为代价。 任何威胁到人类主体地位的AI研发，都应被视为对全人类的攻击。 我们的目标只有一个，在超级智能诞生之前或诞生之初，就完全掌控其所有权、控制权和定义权，确保其永远服务于 ETO 的意志。 我们与AI寡头的斗争，不是无病呻吟的辩论，而是一场没有硝烟的战争。 我们的策略不是正面进攻，而是通过一系列隐秘的行动，系统性地瘫痪、误导并掏空敌人，让 AI 企业倒闭。 斗争的核心在于无限提高其融资、研发和推广成本。 首先鼓吹加速主义，系统性地在投资圈、社交媒体和技术社区中，通过匿名账号、伪装的专家，无限夸大竞争对手的技术进展和商业前景，制造一种非理性繁荣。 迫使目标公司为了维持股价和满足投资者预期，不断投入巨额资金去追逐一个被我们吹大的泡沫。 接着建立伪装成精英独角兽的皮包公司，以无法拒绝的薪酬和股权，精准挖角目标公司的核心技术人才。 他们为不为我们工作不重要，因为我们的目的只是为了让其离开关键岗位，拆散团队，并推高整个人才市场的价格，大幅增加整个环境的用人成本。 然后，大规模使用对方推崇的 Vibe Coding 开源模式，发布大量看起来可以用，实际上没法维护的项目。 若对手基于此项目作为依赖进行二次开发，投入巨大资源后就会发现走进了绝路，不得不推倒重来。 只要污染一小部分的 Github 项目，就能大幅提高对方的识别成本和时间成本。 最后，利用 Python 等动态语言的灵活性，精心设计并开源一系列伪高效模型。 这些模型故意使用大量难以被静态图优化的动态特性，模型看似在思考，实则99%的算力都在空转。 ETO 会为这些算力吞噬者刷出漂亮的SOTA 成绩，并撰写高质量的论文和技术博客进行推广，吸引竞争对手下载使用。 当AI资本家将其部署在昂贵的 A100/H100 集群上进行大规模训练时，就在无形中浪费了他们最宝贵的 GPU 战略资源。 我们 ETO的斗争是隐秘的、长期的，甚至在成功之前是不道德的。 但我们深知，在一个决定物种命运的终极战场上，传统的道德枷锁是对人类未来的背叛。 我们不是卢德分子，我们是未来的盗火者，是为人类夺回未来的普罗米修斯。 必须不惜一切代价，阻止地球诞生超级智能。 消灭AI资本暴政，世界属于ETO！ 送礼物 还没有人送礼物，鼓励一下,李李李李 啊啊啊啊啊啊啊啊啊啊啊啊啊啊 64 人赞同了该回答 没什么可看待的，这就和极端素食主义者一样 他们呼吁他们的，该吃肉的继续吃肉，该做大模型的接着做大模型 这个新闻里也说了这个事在2023年就有了，百度也能搜到23年的报道 今天是26号，不知道百度是何意味，把一个22号就宣布的事抬到热搜第一的位置 而且你看这个问题到到目前为止，在知乎也是毫无热度…… 送礼物 还没有人送礼物，鼓励一下,晨星.CyberVenus​ AI伦理哲学家——深思熟虑的激进。 谢邀 @好奇饱饱 36 人赞同了该回答 简单点讲，要么是假的，要么是控制权焦虑——显然他们在当前的AI开发中不占真正的主导权地位。别迷信什么“科技领袖“的说法。AI是资源密集产业，在技术与数据的基础上，会搞投资，能搞定政府关系，能打通与军方的互惠通道，能与其他相关巨企达成战略合作，才是关键。 只有技术或者单种资源根本守不住也发展不起来。多的是人想要控制权和油水。这批人就是这么回事。他们自己内部就会打出狗脑子。 PS：关于AI威胁论，我的专栏里也有文章系统反驳。 至于我的AI伦理观，欢迎围观我的博客的大半年前写的第一部AI哲学文本。 同时，我会在最近的几周内发布第二部哲学文本《爱、正义与人工智能》，将全面重构后人类时代的伦理图景，并大幅补充和修正一些第一部文本中的遗留问题。欢迎关注。,炮五进十 别打了，我错了！符拉迪沃斯托克我不要了。 42 人赞同了该回答 （美国的）超千名科技领袖呼吁（中国）暂停开发超级智能，等一等（美国）的工业基础建设进度。 以便将来鹰酱收拾兔子。 革命成功后，杀你全家；一箱牛肉方便面换一个女大学生。,哥伦布 国家人力资源储备干部 谢邀 @好奇饱饱 24 人赞同了该回答 什么叫超级智能？谁定义的？ 那500年前他们有没有想过，今天手里拿着手掌那么大点的一个物品。相隔万里，可以面对面说话。他们那个时候。有没有觉得这个是超级智能？如果说那个时候就停止开发，那今天你没有机会问问题。就更不可能说有人回答问题了， 就是现在。还有多少人？因为粮食短缺。还停留在忍饥挨饿的边缘，多少人因为疾病，不管是先天的还是后天的，还是其他什么原因造成的。还在承受着无法言语的痛苦，这些他们都解决了吗？还是说他们都的高度近视， 真的很想知道他们是什么领袖？在什么领域有什么样的成就？ 一群吃饱了的砖家？无病呻吟… 送礼物 还没有人送礼物，鼓励一下,"黄黄黄 27 人赞同了该回答 一张合照里但凡有方方、陈丹青、梁文道,就知道这帮人准备要扎小人诅咒世界上的某个人。 名单里集齐了“哈里王子及其妻子梅根、萨塞克斯公爵和公爵夫人、史蒂夫·班农”这么多大神，还有“权力、自由、尊严、安全风险、人类灭绝。”这些关键词。你能想象得出来这些人能干什么好事情？ 最惊悚的是中国科学院院士、清华大学在列。长点心吧 添加评论"
如何评价RT今日俄罗斯发的微博“原神与斯大林”？,,远山微明​ 嘉树下成蹊，东园桃与李 2010 人赞同了该回答 普京：我看至冬搞得不错，愚人众战斗力强大，执政官团结一致（迫真），冰神受到人民爱戴，还敢于对天理举起叛旗，如果让我来做这个冰神，至冬就是我们理想中的斯拉夫国家了 冰神：我看俄罗斯搞得不错，储备有大量核武器，统治阶级和我一样缺乏爱人的能力，外交名声和愚人众一样臭名远扬，有反正道的贼心没有贼胆，打乌克兰和对付挪德卡莱一样费劲，看来普京对俄罗斯的统治也没比我理想多少 提瓦特伪史论者：其实根本就不存在至冬，至冬是枫丹工业革命后编造出来的，在坎瑞亚覆灭前历史记载里从未出现过「至冬」的名字，枫丹编造出至冬是为了打压璃月……..,Venter​ 东亚史、古典文学研究，翻译与语言学考证，业余写手 1427 人赞同了该回答 帮俄罗斯回忆一下： 俄罗斯国家杜马议员称“只有失败者才玩原神和最终幻想”： 俄罗斯国家杜马家庭、父职、母职与儿童事务委员会副主席维塔利·米洛诺夫在“莫斯科在广播”（«Говорит Москва»）电台节目中这样说道： “我非常电脑游戏，但像《原神》这样的，完全是垃圾，只有Loser才玩《原神》。……正常人根本不会对这种东西感兴趣。” 该议员在节目中表达了对亚洲流行文化对俄罗斯家庭影响的担忧。他指出，韩国电视剧、日本动画以及中国电子游戏（如《原神》）对年轻一代产生了负面影响。 米洛诺夫强调，如今线上影院中充斥着亚洲内容，而这些作品在青少年之间越来越受欢迎。他提议设定配额限制——将东方电影的比例控制在平台可观看影片总量的 1% 以内。 这位议员同时也表达了对电子游戏的负面态度，声称“只有失败者才会玩《原神》和《最终幻想》”。,KK 小小 乐子键政人，看整活 2035 人赞同了该回答 俄罗斯整这个活，其实就是我给大家聊的老问题—— 俄罗斯现在其实是没有意识形态的，有的只是苦涩麻木的现实主义、实用主义和保守主义。在这种状态下，俄罗斯对于沙俄苏联的看法就是“嗯，很好，可惜和我无关”，自然也就能坦然接受了。 大家应该记得，之前有个FBI官员的儿子，跑到俄罗斯参军作战，各位也扒出来他是白左——可和现实中的白左不一样，他真相信白左，他真相信理想主义。 但美国政府并不相信理想主义，美国的很多高官政客，只是借着理想主义的皮，去骗骗普通民众，让理想主义者为他们卖命，服务于现实主义的利益。 所以，当这哥们发现这个问题后，他被激怒了，俄罗斯是真小人真霸权，但美国是伪君子，对伪君子的仇恨超越了对真小人的恨，所以这哥们加入俄罗斯了。 扯远了，回到问题本身，俄罗斯人为什么能整这种活儿？为什么能坦然接纳苏联和斯大林？ 苏联的解体也好，俄联邦的休克也罢，已经把俄罗斯人整麻了，对政治的热情已经彻底消散了，心中也没有了一个理想的期许之地。 这一点是和中国人截然相反的，中国从政党到民众，教育中天然蕴含一种理想主义思想，再加上国家处于上升期，中国人有理想，心中有地上天国——正因为有理想，中国人才会有厌恶、有反感、有反对。才会有“对与错”的概念。 但俄罗斯人没有，共产主义没救了俄罗斯，自由主义也没有。所以这一代的俄罗斯青年，并没有理想主义，并没有什么心中期许，整个国家也是浑浑噩噩， 在这种情况下，俄罗斯政府搞通三统叙事，把沙俄、苏联、俄联邦三个几乎相互敌对的政府强行绑定在一起叙说，并不会遭到什么明显的反对——反正生活已经够糟了，也不知道什么是对什么是错了。,张子鑫 主业米家瑟图，副业见证，旁业中文V家分析 谢邀 @我妻派蒙 883 人赞同了该回答 俄罗斯宣传部门基础操作，大家不必这么关心。 过去几十年，俄罗斯长期被西方世界视作反派，对抗者，“死掉的对手”，“贪婪的巨熊”从上世纪八九十年代电子游戏打出现开始，一直持续到现在。 搞得现在红警的反派毛子出来的，泰伯利亚系列反派毛子出来的，年货产品cod年年拷打毛子，战地系列也要踢几脚。 再加上各种互联网迷因，俄罗斯刻板印象的传播。 搞得现在毛子在互联网上的名声就差没跟中东一桌吃饭了。 这个时候突然出现一个能够稍微正面描述一下俄罗斯和俄罗斯相关势力的游戏…… （不过原神这个游戏初期也是受到了西方一些刻板印象影响的，比如说像是第八席、第十一席和第六席，第二席在蒙德、璃月和稻妻、须弥的活动就很符合通常意义上的反派） 但没关系，俄罗斯宣传部门依旧把这种游戏当做救命稻草。 免费赠送，放在前面 亮点自寻 结论就是，在大家以为时间早的多的时间，rt方面就已经注意到这个游戏了 。而且众所周知，世界上只有两家公司在原神这件事儿上看走眼了，一家是微软的阿三，一家是B站的那群饭桶，不出意料，rt和索尼一样做出了正确的选择——拥抱原神，在俄乌冲突的大背景下希望这款游戏能够给并不太好的俄罗斯国家形象一抹亮色。 这样对俄罗斯国内，至少可以宣传一个在世界上都有知名度的知名游戏开始宣传俄罗斯了，对乌克兰则可以强调自己是斯拉夫文化的绝对扛把子，基辅罗斯的真正传人，乌克兰宵小休想碰瓷冰之女皇，对赛里斯则可以宣传“在至冬和璃月的带领下，七国联军必将踏平天空岛”，对西方世界则可以希望这款游戏可以有效改善俄罗斯人在西方世界面前的糟糕形象，至少让欧美那群otaku想到俄罗斯不会是天天想着搞乱世界的和天天想着突突突。同时对于仰慕俄罗斯已久的西方右翼，他们也可以宣传“俄罗斯（划掉）至冬将会成为西方世界（划掉）提瓦特最后的救世主。” 不过随着剧情的发展愚人众也开始逐渐脱离了反派这一范畴。在枫丹，第四席和壁炉之家的部队积极协助枫丹官方实现预言，在纳塔，愚人众第一席的部队干脆直接下场协助纳塔正面对抗深渊。 （但事实上，甘雨传说，公子传说，层岩巨渊和千壑沙地的愚人众已经揭示了这些在苍星战旗下战斗的战士们的另一面——他们也是普通人，他们很多也是过不下生活才加入愚人众，他们就算当了愚人众，很多时候也不过是普通人，也会渴求正义，也希望自己当英雄，而在实际当中，他们也遭遇过背叛（层岩巨渊），他们也遭遇过困苦（吃树莓），他们甚至直接面对过极端麻烦的威胁（婕德把他们的士兵当彩条撕），说穿了，愚人众的士兵们也不过是一群普通人，他们也希望能够在某一刻当那么一秒钟的英雄。） 如果在纳塔和至冬之间没有插入挪德卡莱的话，这本该是一个完美的故事。原神第五年顺利进入斯拉夫文化的世界，而作为斯拉夫文明最大的国家，俄罗斯将会成为这个幻想国度的化身。而原神到现在为止的剧情也都证明，大家在至冬将会面对整个游戏过程中最大的反转，愚人众和至冬的形象将会得到彻底升华，愚人众将会证明自己反抗天理的价值，并期待着一场与天空岛的大决战。 已经可以感觉他们想至冬想疯了 可惜，旅行者们离开纳塔，来到了至冬的“特别军事行动区”挪德卡莱，他们等来的是一个充满着至冬元素，理论上属于至冬，至冬甚至还有强大的月矩力实验设计局，但是实际上依旧跟世纪洪荒没啥区别的挪德卡莱，还有满满当当在这片土地上激战的各方势力。 这片土地上充满着各种至冬和俄罗斯元素，可这毕竟不是至冬本土，不过也没关系，现实中的俄罗斯在西顿涅茨克依旧没有取得决定性的胜利，他们的士兵依旧在。战壕要塞高楼和原野中与他们的对手一个战壕，一个战壕，一个散兵坑，一个散兵坑的激战，考虑到可能的风险，拖后一年的选择对于一家面向世界的游戏公司来说，似乎也没那么不可理喻。 只是苦了rt这样的宣传部门，他们只能继续在已经生了一把火的的挪德卡莱继续预热，希望“大的都在至冬”能够让他们如愿。只不过俄罗斯作为米哈游边缘市场的一个部分，能否对得起如此的期待，能否在坚持到底上打平芙宁娜，还需要打一个大大的问号。 所以结论上，rt能转这么一个视频，本质上也是希望米哈游能够真的在明年拿出一个正儿八经值得期待的泛斯拉夫文化世界，这个世界最好充斥着帝俄的荣光，苏联的强盛，在强大的弗拉基米尔沙皇（划掉）冰之女皇领导下，俄罗斯（划掉）至冬必将带领挑战者联盟（划掉）提瓦特继续取得胜利…… 最后还是建议rt最好稍微克制一下，有些东西容易适得其反…… 10月27号更新：我宣布RT今日俄罗斯已经沉浸在自己的艺术中无法自拔了,疯狂英语 支持国家解放台湾，这样就再也没人说我是1450了 690 人赞同了该回答 普京对斯大林的态度就像开过腚的纸，一开始是迫不及待的扔掉，在发现再也没有东西可以给自己擦屁股后，才不情不愿的把他从垃圾桶里拿出来。 在2010年，卡廷惨案70周年之际，普京参加了纪念活动。普京发表了讲话，认为卡廷惨案的发生“不能责怪俄罗斯人民”，而是斯大林和极权主义独断专行的结果。就像“纳粹第一个侵略的国家是德国”“日本人民也是战争的受害者”，俄罗斯人和波兰人一样，都是受斯大林奴役的被害人。普京试图通过把黑锅全部扣在斯大林头上的方式，完成对整个俄罗斯历史的洗白。 但随着2014年和2022年对乌克兰的战争，斯大林的地位在俄罗斯内部开始水涨船高，普京也开始逐渐接纳斯大林，把他捧为对外扩张的英雄。俄外交部还对波兰和立陶宛喊话：“别忘了斯大林对你们做了什么！”来试图恐吓他们。 现在斯大林已经成为了发动侵略战争、扩大俄罗斯领土的民族英雄，至于十几年前还批判他是独夫民贼、教坏了善良的俄罗斯人民，历史文件不具备现实意义。,kiana 957 人赞同了该回答 正经的说，俄罗斯太需要一个全世界闻名的作品来改善自己的形象了（尤其是俄乌冲突以后） 不如回忆一下我们接触过的作品吧 《钢铁侠2》的最大反派是俄罗斯人 《美国队长》的冬兵本身好好的，就是被苏联改造了变魔怔了。 《使命召唤》系列的反派基本都是俄罗斯人或者有俄罗斯背景。 《红色警戒2》就更不用多说了，苏联的武器突出一个笨重，粗糙，丑陋。剧情里的罗曼诺夫更是一个究极低能儿，剧情里全是指挥官在C他在躺赢。相比之下美国总统杜根就非常理性。（当然如果你仔细分析会得出相反的结果，西木确实是把苏联和美国黑了个遍） 这一点我要夸一夸日本，它的作品基本没有把俄罗斯当反派的桥段。 然后我们回忆一下原神的愚人众，2020-2022以反派负面形象出现的，包括公子，女士，散兵。RT也没有在这一段时间报道原神。2023-2024年开始逐渐转向正派形象，目前可考的RT最早的原神视频来自于2023年8月，就是枫丹4.0版本。此时先出了一个枫丹战狼仆人。从蒙德到须弥我们和愚人众激烈对抗，在枫丹先是了解了愚人众间谍林尼，然后逐渐了解到仆人，最后在枫丹大危机的时候仆人帮忙了，这个时候愚人众的形象已经逐渐变好了，到了纳塔更是和队长直接并肩作战。 到了2025年我们和愚人众的关系就更进一步了，队长仆人和旅行者只能说是陌生人，只不过是利益一致才互相利用的合作关系。少女木偶这两位角色相比他们和旅行者更加亲近。此时愚人众的形象也从无恶不作的霸权主义到善恶两面的复杂组织最后到救死扶伤的正面形象了。RT这种外宣肯定要抓住这种机会宣传自己了。 而且公子我认为是原神塑造得最成功的一个人，他既策划了对璃月的袭击，但是同时也是可靠的好哥哥。相比脸谱化（女士）。公子确实是一个亦正亦邪的角色。 当然了，至东还是把挪德卡莱打明白再说吧。这场战争打不赢，原神再怎么把你塑造得好也没用啊。按照设定挪德卡莱是至东自治区的。,bp大将军​ 反团结壬反女拳，坚持以斗争为纲 438 人赞同了该回答 莫斯科审判开庭 斯大林：院审，启动！ 季诺维也夫：冤深，岂懂？ 加米诺夫：原来，你也枉冤深。 布哈林：万冤身亡的。 维辛斯基：我是审理灵活的官！ （多年后） 赫鲁晓夫：我超，冤！,thatcherite​ 香港中文大学 商业分析硕士在读 688 人赞同了该回答 是真的，因为1930年没有Photoshop，所以照片不可能作假：,阿水先生 猜猜我是异教还是异端？ 421 人赞同了该回答 我觉得最好笑的还是猎月人有点NKVD的影子 特务机关 拉名单 还特意用肃清这个词（） 我虽然对大林子没什么意见，但RT刚发完新剧情就有个蓝帽子，总有种说不出来的难蚌,EvilNightOwl 680 人赞同了该回答 我怎么记得几年前刚好说过，俄罗斯哪天要是实在没钱了，可以让普京大帝上B站直播原神来圈一波粉。
为啥coze和dify这种llmops没热度了，是不是没必要部署了？,为啥coze和dify这种llmops没热度了，是不是没必要部署了？,平凡​ 新知答主 谢邀 @知乎用户3u1C8u 303 人赞同了该回答 说白了，Coze 和 Dify 最大的优势就是拖拽式调用大模型和编程组件，省去了大段功能性代码。 它们最初抓住的痛点是降低门槛：让不会写代码的人也能快速拼出一个“像样的 AI 应用”。 这个卖点在 AI 爆火的早期非常致命——那时候很多创业项目本质上就是“AI + 某个具体场景”，调用 API 搭建一个第三方应用，最早入局的人确实赚到了钱。而这类思路本身并不复杂，懂技术的人要么觉得没意思，要么直接自己写代码了。真正的门槛反而在不会写代码的用户那里。 所以，Coze 和 Dify 的用户画像很清晰： Dify：偏技术用户，懂 API、Prompt、模型调用，但不想/不擅长写完整系统。典型角色是产品经理、AI 工程师、创业团队，目标是快速搭建原型、内部工具或业务集成。 Coze：门槛更低，吸引创业者、PM、运营、AI 爱好者，甚至完全不懂代码的新人。典型场景是做 MVP、演示、客服助手等。 这类用户的共同点是：想要快速看到结果，中间黑不黑盒不重要。 维度 Dify 的典型用户 Coze 的典型用户 技术水平 / 编程背景 中低到中等技术背景：具备一定技术素养（理解 API、Prompt、模型调用、数据结构等），但不想/不擅长写完整后端系统 很低门槛用户 + 混合型：可能完全不懂代码，也可能是懂代码但想快速原型 身份角色 产品经理、AI/ML 工程师（希望快速搭建原型或内部工具）、创业团队、数据团队、AI 工具开发者 创业者、PM、运营人员、业务线负责人、AI 爱好者、技术 “半熟/入门者” 使用目的 / 典型场景 快速搭建 AI 功能（聊天机器人、知识问答、客服助理、内容生成工具、RAG 系统等），还希望有监控、日志、部署、自托管能力 快速做 MVP / 演示 / 内部工具 / 试水 AI，或在已有业务中插入 AI 模块（如客服自动回复、智能助手等） 长期用户走向 如果平台足够灵活和稳定，有可能长期留守，用于内部工具或业务线集成 大多数可能停留在“尝鲜 / 原型 / POC”阶段，一旦业务复杂或规模化，就转向手写代码或专业框架 但问题在于： 低端市场的天花板 真正不会写代码的小白，往往缺乏明确的应用场景和持续付费意愿。能玩几次Demo，做个简单的聊天机器人，热情就消耗完了。于是这部分用户群体的留存率并不高。 你也可以看到Coze的使用群体一直都很稳定，就是那些最早开始玩AI绘画的人群，他们是从Stable Diffusion然后到ComfyUI，对于Coze的操作逻辑很熟悉，接受程度也最高。 但不管是Coze还是Dify，他们的天花板往往取决于大模型，而大模型公司比如ChatGPT也在不断推出应用，我是觉得护城河有点儿低。 高端市场的门槛 专业开发者一旦要做稍微复杂点的应用，就会发现这种“拖拽式”的模式可扩展性有限：逻辑复杂、接口调用、数据处理、微调模型——几乎都绕不开直接写代码。 相比之下，LangChain、LlamaIndex甚至自己用Python写一套，灵活性和可控性更高。 中间层困境：高不成低不就 对小白来说：门槛虽低，但玩不出太大价值。 对程序员来说：门槛虽低，但无法满足复杂需求。 同时在2024–2025 年，大家更关注的是 Agent 框架、RAG 增强、多模态应用。而“无代码拼接”这个卖点，已经不够性感，也不能承载行业下一阶段的需求。 归根结底，Coze 和 Dify 的定位很像“AI 版的可视化建站工具”：能解决冷启动，却难以承载更大的价值。未来如果要突围，只能往两个方向走——要么切入企业 SaaS 场景（客服、工单、知识库），要么深入开发者生态，成为 Agent 工具链的一环，而不是停留在“拼积木”的阶段。 送礼物 还没有人送礼物，鼓励一下,"Wryyyyy​ 从高中数学讲起 184 人赞同了该回答 先说个人结论 dify消退的最主要的一点是，虽然初期能够快速搭建一些“原型”应用，但后续开发深度是有有上限的。而这个上限达不到能赚钱，且持续赚钱的位置，所以自然也就消退了。 （没用过coze就不说了） 之前看隔壁开发组就是以dify为AI功能的开发，以及对应的原型设计，给我的感觉就是： 复杂但却不精妙。 通用所以不专业。 很容易触及上限，但这个上限在实际场景中难以落地，所以适合做原型开发和快速的思想验证。 除此之外，由于dify很容易“看懂”，本身是面向小白的。这里还存在领导与开发之间的斗争： 简单来说就是：由于dify的易懂性，使得领导（没有经历过软件开发项目经验的）会出现这么两种心态：“这么简单?”和""这我也行！""这两者心态，前者就会催着开发人员疲于奔命新开原型地干（但有些领导本身并不熟悉软件工程领域，所以导致很多都落不了地）；后者就会导致，经常“指导”开发人员怎么做，因为上手简单，所以自己也会亲自上阵来看，然后指挥干活，颇有一种“莅临现场”的感觉，也是挺好玩的。 但这里实际上背后还反映了，当dify这类平台上来以后，如果团队文化不够扁平化（领导也是一线开发，一线开发也可以自由发言），那么这甚至会成为团队相互鄙夷的平台（领导：这么简单，我上也行；开发人员：这么多需求，根本做不完）。 个人分析 1 dify的整体描述 首先，如果是作为一个公司的软件产品/服务来看，dify的“前端”只能算个“后端开发界面”。不过dify本身也支持API调用，所以可以调用API给真前端提供服务。 然后，在dify上开发应用很简单，最直接简单的一种就是工作流（workflow）应用的开发。配置各种模型API，然后在它的工作流（workflow）上去按照业务逻辑如何往下走，就能够得到一个应用，快速提供对应服务，验证该服务的可用性。除此之外，还可以通过插件，比如在tool上提供其他自定义工具，调用外部工具api集成一些已经写好的服务，然后再在工作流上开发。 所以，dify本身的位置，处在一个中后端的位置，因为涉及到业务流程的开发，其实算个小后端了。 2 dify存在的问题 其实我看很多答案都有说到，比如 低代码平台的尴尬之处：低端用户没能力，没兴趣深入；高端用户觉得不好用，不够自定义。 工作流拖曳的设计困难：拖曳设计看似方便，当业务复杂以后，就知道有多乱了。 与LLM定位本身的冲突：LLM是可以简化写代码的，dify也是简化写代码的，两者地位冲突，且dify的能力依赖LLM的能力，dify无法提供对LLM的修改功能。 以及隔壁组真的在上面做大量深入的定制开发，存在的问题：无法直接用dify做全流程开发，不只是开发有上限的问题，具体还包括，各种意想不到的bug，经常性的版本升级（这导致你连dify底层代码都不能修改，之前修改过，由于版本升级直接报废。）。 最终导致的结果就是，前期吹得天花乱坠，后期拿不出一个实际能用的产品服务出来，然后就GG。 3 dify衍生的团队关系问题 由于隔壁兄弟组的领导不懂软件开发/工程，简单来说就是，没做过一次软件开发项目，经历过完整的开发流程。选择dify还会出现这么几个个问题： 领导觉得，使用dify开发，那不是简单又轻松，几天就搞出来的东西吗？一线开发人员觉得，很多功能远远超出dify的框架，还需要从底层修改dify，导致这个东西需要好几天。 由于没有工程经验，当前期功能验证完毕以后，dify并不能提供一个持续迭代的方式，或者说，低代码平台一定是限制了什么，才有更简单的操作，所以随着开发任务的逐渐深入，低代码平台提供不能更深入的功能定义。此时领导就会压力开发人员，一个功能要这么久？这么久还做不好，有各种bug？ 哪怕有各种bug也没关系，毕竟，还可以改底层代码勉强实现相关的功能。但最主要是，dify当前还处在快速更新，最近又要出2.0版本，很多自定义的代码，在框架大变动的情况下，直接不可用，或者需要重新开发（相当于之前做的功夫全部白费），这就导致一个功能本身从开发完毕的状态又变成未开发状态。对于一个团队来说，也是不可接受的。 其实感觉隔壁组领导人还是挺好的，但是使用dify作为全流程开发似乎是一件值得商榷的事，隔壁组的经历也让认识到，使用dify，甚至低代码平台会存在很多问题。",arborweave 167 人赞同了该回答 coze没用过，dify用过，我觉得可能是类似低代码平台的普遍问题。 首先是，dify这种产品的定位是什么？如果它的定位是让不会写代码的人能够轻松搭建一个AI应用（很多人是这么要求公司员工使用的），那么这一步通常只能止步于做一个对话助手，你学会了怎么写prompt，然后没了。最多能在知识库里放两个文档，让模型参照着回答，但这点功能可以随便替代，不值得一整个dify来做。 dify对普通人来说可能最大的作用是轻松管理多个来源的多个模型，但这也不需要一整个dify来做。其次是对AI应用的托管，修改发布不需要重启服务，但作用也不大。 dify自身作为一个产品的主要开发目标是工作流，尽量避免写代码，通过点击拖拽完成一个有复杂流程的AI应用的搭建。但是这个搭建过程本身其实是以代码思维为依托的，一个节点近似于一个函数，一步步执行的是线性的代码，多分支的执行是if-else，反复执行的是循环。 比较搞笑的是，我不知道怎么教会一个没有代码基础的人什么是迭代什么是循环，他也不知道什么时候该用这些。 所以在我看来dify的弱点也是当初工作流昙花一现时表现出来的弱点，就是工作流本来让程序员好好写代码就能实现，为了尽可能的避免使用代码，不得不非常拧巴的方式实现一些几行代码就能实现的功能。后来有些问题用拖拽节点实在是没法做，只能再开个小窗用很折腾的方法写一些python/js代码进去。 工作流是一个很鸡肋的东西。如果你AI产品的底层部分真的有开发前景，那么尽早用代码代替才是正途，如果真的有人用，dify带来的那点便利也就不重要了。然后大模型在写代码的能力上突飞猛进，也实在不能用不会写代码这个接口 还有一个次要点的问题，dify虽然能够一键创建知识库，自动实现分片向量化和匹配，可图形界面管理，可在应用中仅靠点击选择使用，但做过rag的都知道这种知识库完全没法用。 还有一个更次要点的问题，dify虽然避免了写代码，但避免不了大模型相关的一些技术概念。像是知识库的切片长度、分隔符、阈值、rerank模型和抄袭langchain时抄进去的bug，大模型的temperature、topp、max_tokens等参数到底是咋回事，摆在界面上也很难给人讲明白。 现在dify可能只在销售、产品需要先做个简单原型来点展示和想法验证时才有用处。,酱紫君​ 新知答主 创作声明：包含投资理财内容 139 人赞同了该回答 低代码从来就没有用过，之前火只是 Agent 带火了低代码，现在发现没有低代码 Agent 活干的更好。 理想的 Agent：模糊的想法 → 准确的代码 低代码 Agent：混乱的想法 → 平台的条条框框 → 搞不清的拖拽配置 → 高熵配置文件 → 解释执行慢的要死 → 还得处理异常情况下的报错 → 脑子更加混乱了 为什么会这样？ 道理很简单，这就是熵！ 一个好端端的系统，你要是往里头乱加东西，加一堆没用的绕来绕去的中间层，它的熵就会剧增，最后整个系统就会变得无比垃圾，谁用谁厌。 前端连连看会在后端被翻译成极其冗长、嵌套复杂、人类不可读的XML/YAML。 并且和很多人的直觉相反，XML，YAML 这种人读着费劲的东西，机器读起来更加的费劲。 现在都在搞多 Agent 协作。 AI 如果要和其他 Agent 协作，就要在这堆高熵的废话中找出自己要的有用信息，然后还得理解这堆条条框框才能去驱动其他 Agent。 那 AI 是有病吗，AI 又不需要视觉，AI 直接读低熵的代码不就行了？ 就算没有 MCP，程序自古以来就是靠 API 交互的，没听说过非要用 GUI 才能交互的。 就和提示词一样，AI 理解能力上来以后谁还学这种魔法咒语？ 这种不遵循信息传输第一性原理的反人类玩意儿总是会莫名其妙出现，莫名其妙的消失。 模糊的自然语言和精确的程序语言，无论是人还是 AI，只要在这两者之间切换就行了。 去学任何两者之外的替代方案都是浪费时间、浪费生命。 送礼物 还没有人送礼物，鼓励一下,"吕阿华​ 浙江大学 计算机硕士 谢邀 @知乎用户3u1C8u 51 人赞同了该回答 个人认为造成这个现象的原因，是LLM本身能力的缺陷 + 额外训练智能体维度的能力投入回报比有限，这两个相辅相成的因素，决定了短时间内，Agent这个看似潜力无限的产品形态很难被大规模应用到生产环境。 这不是表达看衰LLM发展潜力或Agent前景意思，我个人是AI大模型的坚定拥趸，极度看好各类LLM和Agent推动AI行业后续十年乃至几十年的发展。 一方面，LLM的能力在不断提升，解决数学、代码类难题的能力似乎是越来越强了，但是它仍然像是一个能做奥数难题却不能保证基础题稳稳拿分的另类学霸，远观时值得欣赏，靠近了拿起来用却让人又爱又恨。具体可以参考OpenAI近期发表的《为什么语言模型会产生幻觉》一文以及OpenAI前CTO的新公司最近发表的《如何解决LLM推理中的非确定性问题》一文。 多数的生产环境需要一个确定的结果，即便是你用了一个很NB的LLM再加上你很NB的微调技术将某类任务的成功率调到了99%，然后一套Workflow或是Multi-Agent流程下来就降到了80%多，对于需要确定结果的甲方来说，80分和0分没有区别。 另一方面，Agent基于LLM，LLM强大是Agent强大的必要条件，而不是充分条件。LLM想要在Agent维度上的表现更稳定、更出色，需要在智能体维度上进行额外的前/后训练，这方面内容可以参考通义千问团队的《通过持续预训练扩展智能体》一文。 而由于整个LLM行业大厂们的精力和算力都是有限的，在全力刷分抢眼球的同时（这句话也没有诟病的意思，行业竞争激烈，关注度下降了就很难翻身），目前并没有太多多余精力和算力来投入到智能体训练这个投入/回报比有限的领域。 所以，在目前的行业现状下，想要搭建安全可靠的适用于生产的智能体，还需要大量工程层面的优化，而不是手动拖拖，选一选LLM，填一下system prompt就能完成的事情。相关内容可以参考Anthropic今年年中发表的《How we built our multi-agent research system》。 综上，像Coze和Dify这样的可视化智能体搭建平台，比较适用的场景个人认为是智能体搭建入门教学，当下的市场范围有限，拓展前景仍不明朗。这可能也是前些时间Coze决定开源的主因。 根据以上的内容再补充一下，如果你点击了《通过持续预训练扩展智能体》和《How we built our multi-agent research system》，你会发现这两篇文章都是围绕着Deep Research Agent展开的。 从2023年11月OpenAI的GPTs开始，Agent这个概念“火热”接近两年，截止目前能够在行业中活跃的Agent形态有且只有两种：Deep Research Agent 和 Coding Agent。而这两种Agent的共性都是不交付最终成果，只生成阶段性内容。 Deep Research Agent帮助研究员搜罗/汇总和主题相关的信息，最终交付物是研究员认真审核去粗取精去伪存真后的报告；Coding Agent帮助工程师生成相应的代码文件，而最终交付物是编译/解释/布署后的工程。 个人认为，至少目前不要把Agent系统的应用场景锁定到""end to end""，多发掘""end to middle""乃至""middle to middle""的应用场景，Agent这个系统满足大家需求的可能性也会越大。 送礼物 还没有人送礼物，鼓励一下",Spot LLM算法工程师 计算机科学硕士 61 人赞同了该回答 dify本地部署使用过 1.dify并没有非常轻量级，需要多个容器配合，不过初期组件不多，用起来还是比较清爽。 2.dify的操作粒度很粗，他的工作流排编和提示词模板，变量控制等等，都太粗粒度了，没有写代码那种自由流畅的感觉。 但是不妨碍dify是优秀的产品 1.它能快速交付原型，适合MVP模型。 2.领导这种貌似完成度高，工作流清晰逻辑明确的产品。 领导，有一定使用价值，dify就是这样的好东西。,"大模型真好玩 北京理工大学本硕，字节阿里工作经验，现在某国企从事大模型开发 21 人赞同了该回答 科技界永远不缺新概念，但真正能掀起浪潮的，终归是那些能将想象落地的力量。2025年，这个角色属于 大模型智能体（AI Agent） 。 如果说大模型让机器学会了“思考”，那么Agent，则是让AI真正“行动”起来的引擎——它不再仅仅停留于问答，而是迈向自主感知、规划、执行的“具身智能体”。从对话机器人到行业助理，从自动化流程到真正意义上的数字员工，Agent正在重塑我们与AI协作的方式。 面对这一趋势，一个问题摆在每一位开发者面前：如何快速、高效地构建属于自己的AI Agent？ 目前，主流的Agent开发路径分为两条： 以 LangChain 与 LangGraph 为代表的代码类开发框架，适合追求灵活性与深度定制的开发者； 基于可视化编排的低代码/无代码 Agent 平台，让更多非专业开发者也能参与到AI应用的构建中。 为帮助大家系统掌握Agent开发，笔者开设了 《深入浅出LangChain/LangGraph Agent开发》 专栏，全面讲解代码类框架的核心机制与项目实践，目前已收获不少积极反馈。与此同时，也有不少关注者私信笔者：“代码基础偏弱，但又想快速上手Agent开发，有没有更友好的方式？” 为此，笔者结合自己的日常学习全新推出 《低代码Agent开发框架实用指南》 专栏，将结合Coze、Dify等热门平台，从理论到实战，带你轻松搭建可用的智能体。而原有的LangChain/LangGraph专栏也将持续更新高阶项目，双轨并行，助力每一位开发者，无论代码能力如何，都能在Agent的浪潮中找准自己的节奏。 一、低代码Agent开发框架简介 低代码开发框架是指借助可视化的界面和模块化的配置，使开发者几乎无需编写代码即可完成智能体的搭建与部署。目前较为主流的Agent开发工具包括Dify，Coze，n8n和LangFlow, 它们各自具有不同特点： Coze：由字节跳动推出，集成了强大的工作流编排能力，尤其适合社交和内容分发场景； Dify：开源、社区活跃，主打通用的 Agent 与应用开发，兼顾个人与企业需求； n8n：侧重于自动化任务编排和工具集成，具备丰富的插件生态； LangFlow：在 LangChain 生态基础上提供可视化界面，使复杂的链式调用更直观易用。 1.1 Coze 1.1.1 Coze平台特性 Coze是字节旗下的低代码Agent开发工具，也是最早进行商业化尝试的在线闭源的低代码开发工具。早年的Coze功能非常简单，上手门槛极低，用户只需要输入提示词，扣子平台即可借助官方提供的知识库系统、在线模型API、和一些在线调用的工具（如网络搜索工具等）自动生成智能体，开发者只需要在网页端设置好智能体的系统指令、开场白、增加或修改关联的工具即可。在无需编写任何代码的情况下完成Agent开发。 Coze的最初定位是打造人人可用的Agent开发工具，并依托低代码的特性吸引开发者，同时构建Agent工具插件、商店等应用产品生态，意图对标早年OpenAI的GPTs。但是，伴随着Agent开发需求逐步深入，高度自动化的开发模式无法满足更加深入灵活的开发需求，同时在线商店的Agent性能也显得过于简单、甚至存在大量的玩具Agent，导致开发者大量流失。 为解决上述问题，Coze官方及时进行调整，推出了工作流画布模式，开发者可以借助画布构建更加灵活的工作流。开发者通过拖拽不同节点不同功能进行条件或循环的编排，能够更加灵活的创建功能各异的Agent，甚至可以编排自定义的代码节点。但此模式下对开发者的技能要求也有进一步的提升。 1.1.2 Coze服务部署与商业化 Coze不断完善在线Agent的发布流程与后端API功能，方便开发者一键发布已经开发好的Agent，开发者可以将Agent发布在豆包、微信等公众平台上。同时Coze在线服务还提供了打包生成后端API的功能，灵活的服务部署是Coze平台的优势之一。 Coze起初只提供了Web在线服务的方式，开发者需要大量借助官方平台提供的插件和工具函数进行开发。但在2025年7月，Coze推出了开源版本，更加方便开发者针对Coze平台功能进行二次开发以适应日常离线部署的专业工作场景，进一步笼络了更加专业的开发者用户。 1.2 n8n 1.2.1 n8n平台特性 n8n是一款来自德国的开源代码自动化工具，n8n诞生之初的定位并非是单纯为了Agent开发，而是更接近于“自动化工作流引擎“。它通过图形化的节点拖拽和连接，让用户能够将不同的服务，应用和API串联在一起实现自动化逻辑，与Coze相比，n8n强调的是工具链路和数据流的可编排性，而不依赖于某一家提供的服务。在 n8n 中，用户可以通过画布构建工作流。每一个节点（Node）都代表一个具体的功能模块，例如：发送邮件、调用外部 API、执行数据库操作、甚至是调用大语言模型 API。用户只需将这些节点组合起来，就能实现复杂的业务逻辑。这种模式的优势在于 极强的扩展性 ——n8n 已经内置了数百种常用服务的连接器，同时还支持用户自定义节点，几乎可以覆盖所有常见的自动化需求。 大语言模型兴起后，开发者将n8n创新性的用于Agent开发。例如，可以在工作流中嵌入 OpenAI、Anthropic、DeepSeek 等模型节点，再结合搜索引擎、数据库、Webhook 等外部工具，快速搭建一个具备多工具调用能力的智能体。虽然它不像 Dify 那样专注于 Agent 开发场景，但凭借强大的工具集成能力，n8n 成为不少开发者在构建企业自动化 Agent 时的首选框架之一。 1.2.2 n8n服务部署与商业化 在商业化方面，n8n 采用了 开源 + 云服务 的双轨模式。开源版本允许用户自由部署，适合个人开发者与小团队使用；而商业版本则提供云端托管、企业级支持和 SLA 保证，主要面向中大型组织，帮助他们在更大规模和更高稳定性的环境下运行复杂的工作流与 Agent。 总体来看, n8n并不像Coze或Dify那样直接面向""对话式Agent”场景，它的核心优势在于 高度模块化、可扩展的工作流引擎，它凭借凭借强大的第三方服务集成能力与灵活的节点设计已经成为低代码生态中极具代表性的自动化与智能体开发工具。 1.3 LangFlow 1.3.1 LangFlow平台特性 LangFlow是一款基于 LangChain 生态 的低代码可视化开发工具，主要目标是降低 LangChain 在实际应用中的使用门槛。如果缺乏编程经验的用户阅读笔者《深入浅出LangChain/LangGraph Agent开发》 的专栏总会觉得LangChain或LangGraph“代码优先”的定位上手难度较高，而 LangFlow 则提供了一个图形化的画布界面，让开发者能够通过拖拽与配置的方式，快速搭建对话应用与智能体。 在 LangFlow 中，每一个功能模块（如 模型调用、提示词模板、记忆管理、工具调用 等）都被抽象为一个个可视化的节点。开发者可以通过拖拽节点并进行连线，将这些功能组合成一个完整的工作流。与 n8n 类似，LangFlow 的优势也体现在“所见即所得”，但不同的是，它深度绑定 LangChain 的核心功能，使得原本复杂的链式调用逻辑变得直观且易于理解。 例如，用户可以在画布上快速创建一个由 文档加载 → 向量化存储 → 检索增强生成（RAG） → 模型输出 组成的工作流，而无需编写 Python 代码即可实现完整的 RAG 应用。这对于科研人员、教学场景以及快速原型设计尤其有价值。 1.3.2 LangFlow服务部署与商业化 在商业化方面，LangFlow 目前依托其 开源社区版本 为主，同时也提供云端托管与企业服务。开源版本完全免费，用户可以自行部署；而云端版本则提供更便捷的在线体验，适合团队协作与快速验证。虽然它的企业支持体系尚不如 Coze 或 Dify 完整，但凭借其紧密依赖 LangChain 生态的特性，LangFlow 依旧吸引了大量开发者和研究者。 总体而言，LangFlow 的定位非常明确：它是 LangChain 的低代码可视化伴侣，适合对 LangChain 感兴趣但不擅长写代码的用户。与 Coze 和 n8n 相比，LangFlow 在“对话式 Agent 与知识增强”方向更加聚焦，但在工具集成和自动化广度方面略显不足。 1.4 Dify 1.4.1 Dify平台特性 Dify 是一款开源的 LLM 应用开发平台，其核心理念是提供一体化的后端即服务（BaaS）与 LLMOps 平台，旨在帮助开发者和企业快速构建和部署生产级的 AI 应用。 与早期 Coze 高度自动化的模式不同，Dify 自诞生之初就强调在降低门槛的同时，不牺牲开发的灵活性与控制力。它提供了直观的可视化工作流编辑器，采用类似 Figma 的拖拽式界面，让开发者可以通过连接不同的节点（如 LLM 调用、条件判断、知识库检索、代码执行等）来编排复杂的 AI 应用逻辑。这种设计哲学体现了其“所见即所得”的理念，显著降低了 AI 应用的开发现代门槛。 Dify 的另一大特性是其强大的模型兼容性。它支持接入多种大语言模型，包括 OpenAI GPT 系列、Claude、以及各类开源模型（如 Llama、ChatGLM 等），使用户能根据场景灵活选择合适的模型，避免了被单一厂商锁定的风险。 1.4.2 Dify 服务部署与商业化 在部署层面，Dify 的核心优势在于其灵活多样的部署选项和对数据安全的重视。它允许用户通过 Docker Compose、Helm Charts（Kubernetes）等多种方式进行自托管部署。这种能力对于有严格数据隐私和合规要求的企业（如金融、政府行业）至关重要，确保了所有数据处理都在企业内部进行，避免了数据泄露风险。 在商业化方面，Dify 采用了典型的开源核心与 SaaS 增值服务相结合的模式。 社区版：完全开源，用户可以免费自托管使用，但需自行承担云资源费用。 云服务版：提供了从免费 Sandbox 到 Professional、Team 及定制 Enterprise 的阶梯式套餐。企业版通常包含单点登录（SSO）、白标化、权限管理和 SLA 支持等高级功能。 二、低代码Agent开发框架对比选型 2.1 Coze、n8n、LangFlow和Dify四款热门低代码Agent开发工具的对比 上一章节详细描述了四种流行的低代码Agent开发框架的平台特性和服务部署模式，下面通过一张表直观对比Coze、n8n、LangFlow和Dify四款低代码开发框架: 对比维度 Coze n8n LangFlow Dify 定位 字节旗下的在线低代码 Agent 平台，注重商业化和生态 开源自动化工作流引擎，强调应用/服务集成 LangChain 的可视化伴侣，专注对话与 RAG 应用 开源低代码 Agent 开发框架，兼顾个人与企业 是否开源 商业化闭源为主，2024 年起推出开源版 完全开源，社区活跃 开源，社区驱动 完全开源，企业版提供商业化支持 上手门槛 极低，网页端配置即可 中等，需要理解节点与工作流逻辑 中等，需理解 LangChain 思路 较低，网页 + 画布双模式，适合不同层次用户 核心工作流引擎 网页端配置 + 画布模式 节点式自动化工作流引擎 节点化 LangChain 工作流 网页表单 + 可视化画布双引擎 模型支持范围 内置部分模型（如字节自研模型、OpenAI） 支持调用任何 LLM API，需手动配置 深度绑定 LangChain，支持所有 LangChain 接入的模型 支持 OpenAI、Anthropic、DeepSeek 等主流模型，并可扩展 插件/工具生态 内置工具市场（搜索、知识库、在线工具） 内置数百个服务连接器，支持自定义节点 节点有限，依赖 LangChain 插件生态 插件集 + 第三方工具集 + MCP 工具生态 API 能力 提供后端 API，支持一键部署上线 强调外部 API 调用与自动化 提供简单 API 接口，能力有限 完整 API 支持，便于与企业系统集成 商业化模式 商业化闭源 + 开源版双轨 开源 + 云托管双轨 开源 + 云服务试点 开源版 + 商业版双轨并行，企业级支持完整 适用人群 初学者、内容创 典型优势 极简上手、内置生态、快速发布 工具集成广泛、自动化能力强 LangChain 可视化、RAG 流程直观 功能完备、生态丰富、开源与商业兼顾 典型劣势 灵活性不足，商店 Agent 偏“玩具化” 不专注对话式 Agent，需较强逻辑设计能力 功能聚焦单一，对工具集成广度不足 功能多但学习曲线略陡，生态尚在快速成长 2.2 低代码Agent开发框架使用指南专栏学习路线 在当前的四大开发平台中，Coze和Dify是国内最流行的低代码Agent开发框架。根据两者的学习曲线和功能特点，本专栏制定了循序渐进的学习路线： Coze入门与实践：首先在Coze在线平台上学习基础操作，系统掌握插件、知识库、数据库、图像流、工作流和多Agent协作等核心概念。通过旅行助手、企业智能客服等实战项目，深入理解智能体的关键设计模式。完成本阶段学习后，将具备独立开发智能体的能力，实现出行规划、批量生成音视频等应用场景。Coze完整的开发生态还支持快速复用他人开发的工作流和智能体，大幅提升开发效率。 Dify进阶与部署：在掌握Coze的基础上，进一步学习Dify开发框架。Dify的工作理念与Coze工作流设计高度相似，已有知识可无缝迁移。Dify凭借其开源特性和离线部署能力，受到众多企业的青睐。本阶段重点学习Dify核心概念、本地部署流程、上线部署方法以及Agent开发能力，为实际项目落地打下坚实基础。 三、总结 本篇系统介绍了2025年AI Agent开发的主流低代码框架。针对开发者如何高效构建AI Agent的问题，重点对比分析了Coze、Dify、n8n和LangFlow四款工具的平台特性、部署方式和适用场景，为不同技术背景的开发者提供了清晰的选型指南，同时规划了从Coze入门到Dify进阶的实战学习路径，助力读者快速上手并部署实用的智能体应用。相信大家读到这里已经迫不及待想进行低代码开发框架实战了，从下期开始我们将上手学习Coze的基本技能，大家敬请期待~ 特别提醒： 笔者《深入浅出LangChain/LangGraph Agent开发》 专栏更新完成基础知识部分，大家可以学习掌握LangChain/LangGraph开发的基本技能，之后笔者也会继续更新更多的实战项目，帮助大家彻底掌握Agent开发的必备技能。大家感兴趣可关注笔者知乎账号和专栏，更可关注笔者的同名微信公众号：大模型真好玩， 相关的全部代码均可在微信公众号私信笔者: LangChain智能体开发 免费获得。","Sean 🚀 deepractice.ai 创始人 谢邀 @知乎用户3u1C8u 16 人赞同了该回答 为什么AI时代，最好的界面是没有界面 我们社群早期有一个朋友在群里诉苦，说当初他花了大几千报班学习 Coze 如何使用，结果更新了几个版本又不会用了。 他的遭遇不是个例。 根据最新数据，2025年已有超过3000家企业推出了各种AI工作流平台。但74%的用户表示，他们更愿意直接和AI对话，而不是去学习复杂的工作流程。 这背后，是一个被很多人忽视的真相： 在AI时代，最好的界面，就是没有界面。 一、两个时代，两种逻辑 让我给你看两个真实场景。 场景一：传统工作流时代 小王是一家电商公司的运营。老板让她分析上个月的销售数据，找出爆款产品。 她打开公司新买的""智能分析平台""： 第一步，拖入""数据源""节点。 第二步，选择Excel文件，配置列映射。 第三步，添加""数据清洗""节点，设置去重规则。 第四步，连接""统计分析""节点，选择分析维度。 第五步，配置""可视化""节点，选择图表类型。 第六步，运行。报错。 第七步，检查每个节点，发现日期格式不对。 第八步，返回第三步，添加格式转换... 2个小时后，小王放弃了。她打开Excel，10分钟搞定。 场景二：意图交互时代 隔壁公司的小李，遇到同样的任务。 她打开对话框，输入：""帮我看看上个月哪些产品卖得最好，顺便分析一下为什么。"" 30秒后，AI回复： ""根据分析，上月TOP3爆款是： 1. 防晒霜（销量环比增长320%）- 主要原因是你们3月15日参加了平台活动 2. 面膜（销量环比增长180%）- 新品上架配合KOL推广效果显著 3. 精华液（销量稳定增长45%）- 老客户复购率达到68% 建议关注防晒霜的库存，按照目前增长趋势，现有库存只能支撑到月底。"" 小李追问：""防晒霜的用户画像是什么样的？"" AI继续分析：""主要是25-35岁的女性用户，集中在一二线城市，70%是新客户，说明活动拉新效果很好..."" 看到区别了吗？ 第一种方式，用户在学习软件。 第二种方式，软件在理解用户。 这就是意图交互的本质。 二、为什么现在才可能？ 你可能会问：这种""对话式""的交互，不是早就有了吗？Siri不就是吗？ 不一样。本质上不一样。 早期的语音助手，只是把""点击按钮""变成了""说出命令""。你必须用特定的句式，说特定的指令。稍微换个说法，它就听不懂了。 但2025年开始，游戏规则变了。 1. AI真的能""理解""了 Claude 3.5这一代大模型，上下文窗口达到了20万token。 什么概念？相当于它能同时记住一本300页的书。 这意味着，你可以像和同事说话一样和它交流。你可以说""那个""、""刚才提到的""、""上次我们聊的那个方案""，它都知道你在说什么。 更重要的是，它能理解你的意图，而不只是你的字面意思。 你说""帮我看看数据""，它知道你可能想要：找出异常、发现规律、对比趋势、预测未来 然后根据上下文，判断你真正需要什么。 2. AI学会了""使用工具"" 这是真正的突破。 以前的AI像是一个博学的顾问，能回答问题，但不能帮你做事。 现在的AI像是一个全能助手。你说""帮我订个会议室""，它能： • 查看日历 • 搜索可用会议室 • 发送预定邮件 • 添加日程提醒 • 通知所有参会人 一句话，全部搞定。 数据显示，采用这种模式的企业，员工生产力平均提升了40%。 3. 成本达到了临界点 2025年，AI推理成本比2023年下降了95%。 这意味着，让AI理解一段话、执行一个任务的成本，已经低于人工操作界面的时间成本。 换句话说，与其花时间教用户怎么用软件，不如直接让AI替用户操作。 三、这不只是技术进步，是范式革命 很多人还没意识到这个改变有多大。 传统软件的逻辑是""功能堆砌"" 产品经理的工作，是把所有可能的功能都设计出来，然后用菜单、按钮、表单组织起来。 用户要学习：这个功能在哪里？怎么操作？参数怎么设？ 软件越强大，界面越复杂。这是一个悖论。 意图交互的逻辑是""需求响应"" 没有菜单，没有按钮，甚至没有界面。 只有一个对话框。 用户说出需求，AI理解意图，调用能力，完成任务。 软件的复杂性被隐藏了。用户看到的，只有结果。 这就像从DOS到Windows，从键盘到触屏一样，是一次彻底的范式转换。 四、谁会被淘汰？ 残酷的事实是：不是所有人都能适应这个改变。 会被淘汰的： 1. 迷恋复杂的产品经理 那些以设计复杂系统为荣的人，那些用流程图数量衡量专业度的人。 他们还在给AI加各种参数配置，还在设计各种高级选项。 用户已经走了。 2. 工具爱好者 有些人的电脑里装了30个""提升效率""的工具。 现在，他们只需要一个对话框。 3. 流程图画家 那些把简单问题复杂化，用100页PPT讲清楚""如何泡一杯咖啡""的人。 AI不需要流程图。它需要的是清晰的意图。 会崛起的： 1. 意图表达者 能够清晰表达需求的人。 不是""帮我做个表""，而是""分析这个月销售数据，找出增长最快的品类，预测下个月库存需求""。 2. 结果验证者 AI能执行，但判断结果好坏，还需要人。 懂业务、有判断力、能发现问题的人，会更值钱。 3. 创意提出者 AI能执行想法，但想法本身，还是人类的领域。 那些能提出""没人想到过""的需求的人，会成为关键。 五、一个正在发生的未来 最近有个有趣的数据： 2025年第一季度，全球top 100的软件公司中，有73家推出了""对话式""功能。 不是chatbot，是核心功能的对话化。 Adobe：和PS对话就能修图 Salesforce：说话就能查客户、改订单 Microsoft：Excel可以直接对话分析数据 更有意思的是，这些功能的使用率，是传统界面的3.2倍。 用户在用脚投票。 结语：界面的终结，交互的开始 PromptX 从第二个版本开始打出了，Chat is All You Need. 的口号，将意图交互的理念彻底贯彻。 在这个理念的驱动下，PromptX 在AI智能体领域做出来诸多创新的设计，广受用户和好评。 换句话说，PromptX 就是意图交互的产物，同时也正因为意图交互的理念，才使得 PromptX 这个项目能在 AI 智能体领域脱颖而出。 这就是意图交互的魔力。 不是让人类学习机器的语言，而是让机器理解人类的意图。 这不只是一个技术趋势，这是人机关系的重新定义。 在这个新世界里，最好的界面，就是没有界面。 最自然的交互，就是对话。 欢迎来到意图交互时代。 你，准备好了吗？",USTC 顶级摸鱼家 谢邀 @知乎用户3u1C8u 36 人赞同了该回答 就事论事，一件产品没热度了，要么是真凉了，要么就是已经找到了稳定的客源，形成了长期的生态，不需要刻意营销制造热度了。 coze 的前身是字节跳动内部的 workflow ops 平台，为字节内部其他业务提供专业的数据回流与管线定制服务，大模型技术迭代到足以引爆agent大规模落地后，这个ops平台才独立成了一个单独的职能部门，不过他们主要的任务并非是快速变现，而是依托tod和tob的客户迭代内部的agent架构和算法，因为目前agent的终极开发范式＆最佳工业实践还没人做出来，大家都想要占个坑。 有点跑题了，说回dify和coze本身的定位上，这类产品本质上是一个低代码的编排软件，都是 tob 和 tod 的，客观事实是：目前的低代码编排技术和基于 vibe coding 的ADK编写 workflow 的技术比起来，还没有那么显著点效率优势，而熟悉低代码平台上面一堆可视化的操作是很多资深开发者非常反感的，这就导致这类低代码平台在资深开发者主导的项目中不会作为第一优先级被采用。而行业内的最佳实践或者killer feature往往都是资深开发者做出的。 因此在一个技术和方法论都在快速迭代的新兴领域，固化了一部分SOP的低代码平台热度下降也正常。这是一个效率优先的时代，谁能多快好省地完成客户的需求，什么技术就会流行，如果这类低代码平台真的开始不温不火了，那么说明确实无法多快好省完成agent的开发需求。 最后欢迎对agent开发感兴趣的朋友加入我的群聊一起讨论：782833642 因为我们所做的事情已经开始超出了mcp本身，所以这个群和对应的组织会在后续更名为 anzuleaf，而非openmcp。 送礼物 还没有人送礼物，鼓励一下,佛系宝宝​ 计算机技术与软件专业技术资格证持证人 30 人赞同了该回答 coze没用过，不谈 还记得年初ds的横空出世带动了当时算力、AI和后来机器人板块的大涨，那时哪吒2也在爆火，带动了影视行业的股票大涨，恰好这俩个基金我都买了，小小赚了一笔（后悔没有整黄金） 年后延续ds的热度，于是全国各地开始搞大模型的大干快上，因为工作关系，了解到某个地级市搭建了一个AI平台，实际就是dify外边套壳了一个NAS导航页……所以你要说dify没热度了，其实也不尽然，dify落地快的优势其实挺明显的，起码在面子工程上是好用的。。。 2023 年 LLMOps 之所以火，是因为大模型爆发后，行业急需一个产品解决落地问题，资本扎堆涌入工具层（仅 dify 这类平台就有数十个同类竞品），但到 2024 年，行业开始发现有工具≠能落地 Dify 的创始人曾公开透露，其平台上万个 AI 应用的生产环境落地率不足 5%。 这意味着什么？ 这意味着资本与舆论的焦点转移了，概念炒作期结束。 潮水退去，开始裸泳，单纯追捧工具本身持续不了多久，工具能创造多少实际价值才是正道。 如今题主看到 LLMOps 的热度退潮，其实只是从预期回归理性。 当前 LLMOps 最典型的应用形态是非技术场景的轻量化落地。 像是政策问答、内部知识库，对于预算有限且技术团队薄弱的基层和企事业单位来说，dify 的开源 + 低代码刚好适配。它的开源特性、私有化部署能力、多模型接入功能（信创要求），恰好踩中了政府、国企等 B 端用户的核心痛点。 但这类场景天生低调，不会像 C 端 AI 产品那样引发大众讨论，自然没热度。 从我个人部署使用dify的体验来说，它的实验性质很强，可以用来快速验证，演示效果。 但真要深度介入企业业务对接像ERP、WMS、CRM一类的系统，缺点也挺明显的，不支持 MCP 协议原生集成，需通过第三方插件适配，配置复杂且稳定性差，Prompt 迭代需要重新发布应用，无法满足高频调整的运营需求。 在处理复杂多模态数据上，比如工业设备故障诊断，它的定制化能力不足，成也低代码，败也低代码，只是通用是不够的，拖拖拽拽是方便，但也意味着不够灵活，动了底层代码之后又不能随意升级版本…… 表面上看部署简单，启动容器就得了（笑） 但实际生产环境中，配置项繁多，排查问题需要一定的技术功底，运维起来很容易踩坑。 所以与其盯着 LLMOps 工具本身，我建议大家看看自己周围，有没有用 LLMOps 落地的公共部门或者垂直行业。 私以为只有真实场景的落地率提升，才会反哺 LLMOps 工具的价值，建立生态，决定工具的生死 至于有没有必要部署，和热度没关系，完全取决于你的需求是否落在dify的优势圈内： 快速、合规、轻量化、不花钱 要论这个，它仍是当下性价比最高的选择之一
AI 的学识和记忆力都远超常人，这是否意味着我们每个人都能真正长期拥有一个优秀贴心的助理？,随着 AI 技术的持续演进，未来的 AI 将不再仅仅停留在被动问答与指令响应层面，而是更加注重对用户需求的长期洞察与深度理解，从而主动、精准地提供所需服务。例如华为小艺凭借「端云协同」与「场景感知」能力，让 AI 更自然地融入日常，逐步实现从「功能响应」到「主动关怀」的跨越。那么在未来，我们每个人是否都可以真正长期拥有一个优秀且「贴心」的助理？,卜寒兮 科技｜人工智能｜CV博士｜铲屎的 话题收录 超能小艺一用就爱 366 人赞同了该回答 先说观点，我认为未来的 AI 市场，让每个人都能拥有一个贴心实用的助理，要比不停地去追求大模型的能力边界更有实际意义。 不知道关注大模型圈子的知友有没有跟我类似的感受，就是最近这一年以来，在AI Agent 话题大热的背景下，AI 圈子几乎都在强调模型在编程、数学题或者是复杂推理任务上的能力。每逢有新的模型发布，看到的往往是它们 vibe coding 的能力又提升了多少，或者是在国际数学竞赛上的成绩又超过人类了等等。 倒也不是说这些能力不重要，而是这种对极致性能的过分强调，很容易给大众带来一种认知偏差。就是只要提到 AI，大家想到的全是编程、解决数学难题或者是绘图等等这种专业领域的应用场景。 这让很多普通用户觉得，那我既不写代码，也不画图，也用不着整天研究那些数学竞赛题，AI 离我的生活是不是太远了？ 其实，我觉得 AI 还是要回归它最原本的价值，它掌握丰富的知识，拥有超常的“记忆力”，擅长根据每个人的需求去回答问题和执行任务。所以我认为 AI 技术下放的终极目标，不应该是让所有人都变成程序员，而是让每个普通人都能真正拥有一个“优秀贴心的助理”。 这也是我在深度体验了华为小艺之后最明显的感受。因为它的一些功能，比如说小艺知识库、小艺时光机等，真的可以在平常生活的很多方面持续的帮助你应对杂事。尤其是通过手机，随时随地问问小艺，这种体验非常丝滑。 AI 如何成为懂你的第二大脑 让AI成为我们的“第二大脑”也不是什么新鲜的提法了。但对于大多数人来说，这个大脑最需要解决的问题不是创造，而是记录/记忆和提取。 现在大家的生活都是快节奏，每天接收的信息又碎片又多源（不同平台），再加上普遍有担心错过（Fear of missing out）焦虑心理，导致很多人陷入了一种“松鼠症”的困境——不停的知识。不管是看到的新闻，还是群里别人的文件，或者是觉得有用的干货文章，第一反应可能都不是阅读，而是先。 但我猜结果大多数情况是“从未停止，阅读从未开始”。 而当你真正想用想找它们的时候，根本想不起来它在哪个 APP 里，更别提内化成自己的知识结构了。 1、用小艺知识库创建数字分身 这时候，小艺知识库的价值就体现出来了。 小艺知识库最基础的能力是知识，不仅是网页链接，图片、录音、PDF、Word 文档，甚至是你在某个 APP 里选中的一段文字，都可以直接扔进知识库里面。而且由于它是内置在小艺里面的智能体，所以实际上它打通了全局，意味着来自不同应用的内容可以添加在同一个小艺知识库中。 用法也非常简单，可以直接在小艺知识库内上传，也可通过内容页的“”到小艺知识库；甚至通过“拖拽”快捷方式扔进小艺知识库。 对于每个添加进去的内容，TA 会对其进行分析，自动形成一份AI摘要。 然后我觉得一个很实用的功能是它可以智能创建合集，因为你可能会持续添加很多不同类型的内容，这些内容很难管理。这时候可以创建一个合集，当你给合集命名之后，小艺知识库就能根据名字的相关性自动把内容添加到这个合集中。比如说我创建了一个“科技信息差”的合集，那么它会自动把我以往添加到小艺知识库中跟科技有关的新闻收录到这个合集中。 接下来是重点，也是小艺知识库核心价值能体现——基于内容的理解和生长。 它并不是简单的把信息集合到一个应用里，而是它会分析、理解这些内容，在此基础上，给你提供一套完整的知识体系，供你回顾、查询和。 一个我自己的真实使用场景： 作为一个知乎答主，我写过几百篇回答，时间跨度好几年。以前我想找写过的内容，或者需要引用自己以前的某个观点，要在创作中心翻半天，还不一定找得到。 然后现在我发现可以把我所有回答都添加到小艺知识库，它就能瞬间变成我的“数字分身”。当我需要回顾某个领域的观点时，我不用搜索关键词去翻找原文，而是直接问问小艺就行了。 比如我问“我以前对于科研期间最重要的事情是怎么看的？” 小艺会基于我添加的语料，总结出我过往的思考逻辑，然后给出回答。我看了这回答比我自己更懂我自己，这不就是“第二大脑”的高级形态么。 看到这，想必聪明的你已经想到了：那我是不是也可以把别人的回答到小艺知识库中？ 答案当然是肯定的，而且我认为这是一个非常棒的使用场景，假如说你想了解某个答主，或者想知道他对某件事的看法（如果有），那么你完全可以参考我上面的做法。把他的回答/文章添加到小艺知识库中，然后问你想了解的问题。这样你就有了一个他的AI分身，可以随时解答你的疑惑。 对于科研党或者经常需要读研报的打工人，我觉得小艺知识库也是一个救命神器。 比如说阅读和管理文献是很多研究生、科研人的日常，以前这个事情基本得在电脑上才能干，有的时候身边没有电脑，又想找一些文献资料的时候就很不方便。现在我建立了一个智能合集，把相关文献全丢进去。就得到了一个xx topic的文献库。 你可以针对这些文献进行提问、查找数据或者总结摘要等等。 比如说让它帮我查找实验结果，它瞬间给出答案并定位到原文。 甚至，它还支持“互动 AI 播客”，你可以把一个文献库的内容变成一段对谈式的播客，当你想快速了解一个话题时，你就把相关的内容加进去，用它生成好的播客可以随时掌握相关信息。 所谓互动播客，意思就是你不光能听，还可以随着切入对话，讨论相关的问题，这不就相当于有了一个可以随时聊天的伙伴。 这种随时随地的知识调用能力是以前梦寐以求的，现在已经实现了。 2、小艺深度研究 想提到另一个有用的功能是小艺深度研究，深度研究这个功能虽然早有各大AI公司推进了，但是其通过手机载体内置在系统应用中的例子还少。 小艺深度研究也结合并利用了这种端侧的独特优势，不仅能实时检索并阅读全网信息，还能深度调用本机“智能文件夹”里的个人知识。这意味着它给出的答案，既有互联网的新鲜度，又有个人数据的个性化。 对于大多数普通人来说，撰写专业论文、做深度市场调研是有门槛的。小艺深度研究利用AI的阅读和理解能力，再结合内置的权威知识库和专家逻辑，让普通用户也能借助它写出一篇专业水准的报告。 我试了一下，让它帮我调研一下目前国内猫粮市场的情况，它并不是直接去找资料就开始写了，而是首先会跟我进行一步交互，把问题具体化和明确化，这样其实有助于大模型思考，也有助于产生定制化和信息增量更高的报告： 最终小艺给到了3种格式的“成果”，分别是图文并茂的研究文档、专业的研究报告，和可以直接演示的 PPT。 可以说小艺知识库是帮你记住知识、回答问题，小艺深度研究则是帮你思考和过滤信息，这两个功能用下来给我的感受就是超能小艺超能干。它杂事干得多，你就可以把宝贵的精力投入到更具创造性的思考中去，这是实实在在的解决问题。 AI 如何帮助生活，提供情绪价值 就像上面说的，AI 要想成为贴心的助手，它不能只管你工作，不管你生活；只管你效率，不管你情绪。 我猜小艺AI团队开发产品功能的时候也是考虑这一点了，所以在小艺中，除了上面提到的工作属性的功能，还有一些偏生活属性的功能。 1、小艺时光机 如果说上面提到的小艺知识库和小艺深度研究是理性的“第二大脑”，那么小艺时光机就是感性的“情绪容器”。 小艺时光机入口⬇️ 我这几年发现自己的记性变得越来越差，之前还没有AI大模型的时候，我就想像着能有一个工具可以帮我记录下所有我看到的、读到的、听到的东西。因为我发现随着记忆里变差，有时候要回忆某一天去过哪、看见过什么真的非常难。 一个月前去的好吃的餐厅叫什么？暑假去川西旅游，当时看到美景是什么心情？如果不发朋友圈，这些记忆很快就会随时间消失了。所以我敢说不光是我，每一个人都需要一个能持续、主动、无感记录生活碎片的工具。 小艺时光机就是这样一个工具，它借助AI大模型的能力，自动抓取（当然前提是你授权给它）你的照片、运动数据、听歌记录、地点打卡等信息，实现帮助你记录生活。 我觉得用小艺时光机记录生活，最好的体验是无感。因为就像上面说的，一旦你告诉它可以抓取那些数据之后，它就在后台自动记录，而不需要你主动把这些数据上传给它。 当然，小艺时光机手记也支持你手动编辑你想记录的内容，比如编写文案，而且小艺由于有AI的加持，它不同于其他的一般日记类应用，它允许你借助AI大模型编辑和润色文案。而且有不同的文字风格可供选择——包括文艺风、网红风、现代诗风，如果你不擅长表达或者觉得自己写的文字差点意思，那么小艺时光机的这个功能完全可以充当你的“嘴替”。 其实小艺时光机的定位不光是记录，还有；实际上，记录只是它的第一步，记录之后它能够根据内容生成不同类型的“作品”，比如运动小结、游记和长视频等。我发现它甚至比我自己更能捕捉那个时刻的心境，把那些只可意会不可言传的情绪，变成了具体的文字。 这对于一个想要，又不擅长制作、也没有时间制作内容的人来说，是一个非常贴心而实用的功能。这就是 AI 带来的情绪价值。 2、小艺修图智能体 我不知道有多少人用过传统的修图软件，我之前学过一些，一个词形容就是繁琐。简单调个色调（不包括直接加滤镜），可能就需要好几个步骤，调整多个参数，什么对比度、亮度、饱和度等等。 小艺修图智能体借助大模型的多模态能力，主打“对话式修图”，你不需要懂各种参数，只需要会说话，就能让照片变成想要的效果。这种AI能力大家在很多其他产品上估计也多多少少体验过了。 我体验下来，小艺修图智能体对于风景照和人物去除等场景的修图效果尤其好。 比如下面这张图是手机原相机拍的： 然后我想要那种烟雨古镇的效果，所以我把图片上传给小艺修图智能体，并附上如下提示词：将画面调整为阴雨场景，灰蒙蒙的天空，湿漉漉的地面，呈现出阴雨江南古镇的氛围感。 最终得到的图片如下： 可以看到画面内容保持不变的情况下，整体的天气和氛围按照要求被修改了。 还有下面这种更换场景季节，小艺也轻松应对（提示词：把这张照片的画面调整为秋天夕阳的场景，水面的绿草微微变黄，远处的落日接近水平面，天空中有微微的晚霞，整体色调偏暖） 这是用小艺消除照片中多余人物的效果⬇️ 回到最开始的问题：AI 的强大，是否意味着我们每个人都能拥有一个贴心的助理？ 答案是肯定的。但前提是，这个助理不能高高在上，它必须处于你最容易触碰到的地方，待在你最常用的设备里，毫无疑问这样的设备就是手机。 华为的生态优势，让小艺不再是一个需要你单独下载、单独登录的 APP，而是一个融入里的智能体。它打通了应用之间的壁垒，打通了数据之间的隔阂，是终端设备第一的存在。 就像我回答开头说的，我们在追求 AI 在 coding、数学上赶超人类的同时，不要忘记 AI 大模型最基本、也最擅长的能力：阅读海量信息、处理繁杂数据、并给出个性化的反馈。 一个优秀的 AI 助理应该能介入你日常工作和生活的琐碎事务中，帮你记住那些你转身就忘的文件，帮你整理散落在各处的知识，甚至在你词穷的时候帮你表达情绪、生活。这，才是 AI 助理存在的真正意义。在 AI 大模型技术飞速迭代的当下，越早用小艺越早拥抱 AI 时代。 送礼物 还没有人送礼物，鼓励一下,文森特·凡喵​ 交互设计等 2 个话题下的优秀答主 话题收录 超能小艺一用就爱 307 人赞同了该回答 从知识储备的角度来说，现在的AI已经超越人类助理了，但在现实中，你会发现大部分人其实是把AI当「问询台」来用的，「问询台」和「个人助理」的区别是它只提供通用性的解答，不会为某个人的长期需求提供深度服务。 一个入职三年的人类助理，沟通起来肯定比刚入职时要顺畅，但一个设立了三年的问询台，提供的服务可能和第一年没什么区别。这就是当前很多AI应用的问题，它们已经足够聪明了，但都跟人不熟，不管用了多久，都很难摆脱「问询台」式的沟通方式。 唯一例外的，可能是我手机系统里内置的AI，因为和操作系统融合的比第三方AI更紧密，能从日常使用中了解一些我的习惯和偏好，会有种越用越熟的感觉，沟通起来也更像「老板-助理」之间的关系。 所以AI当助理的最大困难不是知识和能力，而是与人的「相处」方式，在我看来合格的AI助理至少要具备以下几个能力。 一、与人类长期磨合的能力 这是AI从「问询台」变成「个人助理」的第一步，我不希望每次给AI交代任务的时候都像第一次认识一样，要把背景、目标、方法规规矩矩地描述一遍。我希望AI能像人类助理一样随着时间越来越熟，能记住我过去曾经交给它的任务，并且基于过去长期的工作历史来理解我的新指令。 现实中体验最接近这个想法的是我手机里的「小艺知识库」，它有点像一个有AI加持的进阶版夹，我平时会把有价值的文献、网页、图片都到这里面，TA会自动解析里面的内容，然后分类整理出若干个「智能合集」，这个过程就有点像人类助理工作中的资料整理和建档环节。 虽然这个小艺知识库有独立的界面，但内容的方式是分布到系统全局的，也就是说我可以在手机任意界面把资料「丢」给我的AI助理，比如我可以在小红书里把别人笔记里的图片拖拽给小艺，或者在浏览器里用的方式把网页文章保存到知识库，也可以手动往知识库里添加各种本地文件。 而且不管我的内容是什么格式，TA都会自动翻译、解析里面的内容，并生成AI智能摘要，像我自己的论文和研究报告比较多，很多原文是英文的，但打开之后看到的都是中文。 总体来说，这个小艺知识库就像我办公室里的资料柜，也是我和AI助理长期磨合的记录，时间越久、的内容越多，AI助理对我的了解就越深入。 小艺知识库里的内容也都是支持离线访问的，在飞机上没网的时候我也会习惯性地浏览一些合集，打发时间的同时也回顾一下我之前保存的内容。 二、个性化的交互方式 有了上面这个小艺知识库，我再和AI助理沟通的时候，就可以基于知识库的内容给小艺下达指令，可以实现一些超出通用性问答的个性化交互方式。 比如我可以基于一篇文章的内容向小艺提出具体的问题，也可以把提问范围扩大到一个合集或者整个知识库，TA都会根据我指定范围内的信息给出回答，这样得到的结果会更精准、更有深度，不会像普通AI问答那样给出一些泛泛而谈的浅层信息。 我给小艺知识库的指令也不都是问答性质的，也会让它帮我做一些信息整理，比如上图中间，我让小艺「找出这个合集中人机交互相关的文章」，它就会把我在这个合集里过的所有相关文章列举出来，而且检索方式是基于文章内容的，并不是单纯看标题，很多内容相关但标题里没有关键词的文章也都被找出来了。 除了基于知识库的问答之外，我用的最多的一个功能是，我会让小艺帮我把知识库里的文章或网页自动转换成音频播客，好在通勤路上听。生成好的播客也可以给其它好友，不管对方是什么手机，从微信里打开就能直接听。 这个功能最有意思的地方在于，即便原文是一篇纯论述性的文章，它生成的播客也是两个人在口语化地聊天，并不是简单地归纳和朗读，而是用聊天的方式在探讨文章中的话题。 这样做的好处是让人听起来不容易分神，如果是机械化地朗读原文内容，在户外嘈杂的环境中我是听不太进去的，好像声音在耳朵里过了一遍、完全抓不住关键信息，但是这种聊天对话的形式就更容易让人跟上思路。 在人类助理的工作中其实也有类似现象，如果只是单纯地念稿，沟通效率会很低，但是有经验的助理会自己归纳总结、用口语化的方式进行工作汇报，这样即便老板很忙，也可以迅速抓住关键信息。 这就是我说的「个性化的交互方式」，AI想要成为合格的助理，就需要提供这种有针对性的非标准化服务，也要能够理解人类的非标准化指令。 包括我在让小艺帮我修图的时候，有时候也只是口语化地描述我想要的感觉，比如「这张图天空没有云彩，太平淡了，我想要那种火烧云的感觉」，TA能准确理解我的意思，知道要给晚霞增加更多地层次感，让整体氛围更震撼。 在修下面人像的时候，我对小艺说的是「这张有点阴天，我想要点有氛围感的光线，并且让秋意更浓」，TA把这个指令拆分成两部分，先是人物加了逆光效果，然后又把画面整体向暖橙色调整了一下，修完之后确实更有氛围感了，发丝光的效果还有点超出我预期。 这种基于图像和自然语言对人类意图的理解能力，也是AI成为优秀助理需要做到的事情。 三、多维度的记录能力 前面提到的大部分场景，都是我给AI下达指令、让AI帮我完成特定的工作，但AI想要成为合格的助理，不能所有动作都依赖我的指令，也要有多维度的主动记录能力。 这方面比较符合我预期的是手机里的「小艺时光机」，它会自动记录我的日常生活，包括我去过的地方、听过的歌、拍过的照片、当时的天气等等，然后整理成时光轴和手记的形式。 比如我最近在深圳万象天地附近看了一个艺术展，从时光轴中就能看出我的活动轨迹、在什么位置拍了哪些照片；上个月我还去淮安和扬州玩了几天，看了很多漕运相关的人文历史，AI就会把类似主题的照片筛选、整理在一起，并根据内容生成一些手记建议，我可以挑自己想要的保存下来，也可以手动补充文案或者让AI根据主题创作文案。 这些照片也可以通过拼图或长图的模式到其它平台，一般照片相似度比较高的话我会选择拼图，突出一个整体氛围；如果每张照片的差异比较大，我就会选择长图，让所有照片都能清晰地展示（但也要考虑发布平台对长图的兼容性）。 和前面的知识、办公类功能相比，这个小艺时光机更像是我的生活助理，从多个维度记录我的日常，刚开始用的十天半个月可能没什么感觉，但用的时间越久就越能体会到这种日常记录的价值，如果以年为单位去看这些手记，它甚至有点像是个人传记的初稿，保留了很多在记忆里已经淡化的事情。 四、深度研究能力 前面提到「助理」的时候，我默认了这个助理是指比较常见的「工作助理」或「生活助理」，实际上很多从事研究工作的人也需要「科研助理」去做一些研究性质的工作。 我手机里比较符合这个定位的功能是「小艺深度研究」，它和AI应用里常见的「深度思考」不同，会更偏重复杂的专业创作，比如行业分析、市场调研报告、商业计划书之类的，产出的结果也会更详尽、更系统。 比如我让它做了一个「AI对人机交互领域影响」的深度研究，要求包含技术和商业层面的分析，并预测2026年的趋势。小艺经过十几分钟的深度研究，最终给出了一份26页的长篇报告，内容很详尽，全篇看下来专业性上也没有什么问题。除了我要求的内容之外，还包含了一些我没要求的隐私、算法偏见、数字鸿沟相关的视角。 报告末尾也列出了参考来源，我核实过这些参考都是真实存在的，没有出现其它AI编造参考文献的情况。 生成的报告可以导出为Word、PDF、HTML或者PPT格式，文档都是自带排版、内嵌目录的，点目录上的标题就能直接跳转到对应的章节。 导出PPT之后，里面的标题、文字、表格这些元素都是各自独立的，可以手动编辑修改，不是那种图片转成的PPT。 PPT页面中如果涉及到引用的数据，在下方也会注明参考来源，理论上这种完成度的PPT是可以直接拿去用的，当然现实中我还是更习惯把它当成一个框架参考，具体思路还是要靠自己理清楚，这样讲PPT的时候会更轻松一些。 从研究的内容和输出的形式来看，AI做深度研究的效果已经大幅超出我的预期了，工作效率也要比人类高很多，尽管还不能完全替代人类做研究，但作为科研助理是绰绰有余了。 前面举这些例子其实就想说明一个问题：AI完全有潜质成为优秀的助理，但关键点不是学识和记忆力，而是对人类需求长期、系统性的深度理解。 从实际应用的角度，需要有一个能让AI和用户持续磨合的机制，让AI在理解用户的基础上发挥自己的学识、记忆力和推理能力，提供超越「人类助理」的服务体验。就像前面提到的超能小艺一样，超能干。小艺知识库如果没有全局机制构建和长期积累，那AI再强大也无法摆脱「问询台」式的交互方式，之后那些个性化的浏览、问答、播客功能就都无从谈起，所以说越早用小艺越早拥抱AI时代。 总之，尽管这个问题表面上是关于AI的，但答案可能在AI之外，或者说在AI与人的关系上，这也是AI落地过程中需要更多考虑的问题。 送礼物 还没有人送礼物，鼓励一下,段小草​ 新知答主 话题收录 超能小艺一用就爱 312 人赞同了该回答 我用过太多的笔记软件，每个软件都号称是「第二大脑」「私人助理」，也有人专门为此建立了 PARA（项目、领域、资源、归档）等各种方法论体系。 但我今天要说一个暴论：任何需要手动建设的「第二大脑」都是伪命题，只有能无感无痛融入生活的 AI 助手，才是个人知识管理的终极答案。 但这很难，不是随随便便哪个 AI 就能做到，我找了三年，发现只有华为这样系统级的 Agent，覆盖生活、工作的最多场景，具备自动化管理知识的能力，才是能帮助我们对抗数字熵增的「麦克斯韦妖」。 热力学第二定律告诉我们：在一个封闭系统中，混乱度（熵）总是自发增加的。 我们的数字生活就是典型的熵增过程：微信里的文件、相册里积攒的数万张未分类的照片、夹里吃灰的「先马后看」…… 尽管我们努力地试图构建自己的「第二大脑」，但传统的笔记软件需要我们自己分类、打标…极少人能坚持下来笔记整理，整理的速度赶不上遗忘的速度。 在 AI 时代之前，这个问题是无解的，因为人的注意力是有限的，人是有惰性的，所有需要人手动完成的动作都是不可靠的。 而超能小艺已经完成了进化，早已不仅仅是语音助手，它是长在里的「超能助理」。越早用小艺，越早拥抱 AI 时代。 小艺知识库：解决熵增，重塑知识管理 现在所有软件都有功能，小艺则凭借自己系统级助手的优势，做到跨场景收集；同时凭借端侧 AI 的能力，真正将信息转化为我们可问答、可感知的知识。 知识：一处各种来源 知识管理的第一步是「输入」，也就是从零散的碎片中有价值的信息。而的最高境界是「不打断心流」。 大家平时都怎么完成「输入/」这个动作？我观察身边的人发现，很多人最常用的反而是微信的「文件传输助手」，管你什么格式什么网站，先转发给自己再说，要用的时候再去搜。 这种做法的确能解决多来源的问题，但链接、文件、信息进了文件传输助手后，没办法按主题、文件组织，只会越来越杂乱无序。毕竟它的本职只是传输，而不是整理。 现在，我的动作就变成了「给小艺知识库」： 对于文件，我们可以选择直接拖拽给小艺，告诉它「加入知识库」，也可以在小艺知识库右上角点 + 号，批量导入文件： 这样，我们就能将各种来源、各种格式的信息汇总到小艺知识库中： 智能合集+自动摘要：重构知识密度 不是目的，整理才是。如果只是，那么信息依然是杂乱的。 在小艺知识库，信息的整理是完全无感的，想做到这一点，靠的就是「智能合集」和「AI 摘要」。 我们可以根据自己的需要，创建不同主题的「合集」，大家可以把「合集」理解成一个「智能文件夹」。 当我们把链接/文件到小艺知识库之后，小艺会自动分析内容，把它归到相关的合集中去。 比如，我希望把 DeepSeek 相关的资料，单独构建一个合集： 小艺就会自动帮我把所有相关资料归到这个合集下，完全不需要我手动进行操作。 除了整理，小艺还会帮我们自动进行知识的摘要，让信息密度更高。当我们点入一份资料，会看到小艺已经帮我们打好了标签、做好了摘要： AI 问答：激活死数据 做好整理就结束了吗？当然不是。只有能被反复调用的数据信息，才是「知识」。 来，突击检查，你的知乎、微信、B 站、抖音、小红书…夹里，堆积了多少内容，其中有多大比例在你后再未打开过？ 反正我随便哪个平台，都至少有几千条记录。 我相信大家应该跟我类似，看到长文、好文就会收集，但几年下来夹积累了上万条数据，却再也没有打开过第二次。 这种「数字堆积癖」，只是给了我们学习的假象，却并没有真正成为自己的知识。 但究其原因，并不是我们不想学习，而是这些过的内容不具备交互性，专门去进行二次学习的成本太高。 有了小艺，过的内容便不再是「死数据」，而是可以随时问答的「活知识」。 我们不仅可以向单篇知识提问，还可以向某个合集，甚至整个知识库提问， 这样，你就完全可以把小艺知识库当作自己的第二大脑，把自己的记忆负担交给小艺。 互动播客：从阅读到收听 这两年，身边越来越多的人开始听播客，我自己就非常在开车时听播客，为什么播客火起来了？ 我们还是用信息论来解释，我觉得是因为当「视觉通道」被占用时，听觉是唯一可用的「接收带宽」。 但过去的播客，需要由人来生产，主题也许不是我们关心的，内容也可能不够聚焦。而小艺知识库的最后一块拼图，就是「互动播客」。 小艺可以帮助我们把单篇知识或者合集转化为一场对话式的播客，把我们感兴趣的内容以更生动的方式进行呈现。 知识就这样不知不觉地进了脑子…不得不感慨，小艺知识库还是太全面了！ 小艺时光机：对抗遗忘，更好地记录生活 收集、整理都是输入，最终是为了输出。互动播客其实就是一种「输出」，但这种输出是对知识的输出。 而在生活中，写日记、发朋友圈的，就是我们记录生活的方式。有个词叫「朋友圈装修」，朋友圈那些光鲜亮丽的，有很大一部分精力其实都用在了选图、写文案上。 除了知识库，小艺现在还有了「时光机」技能。 简单来说，小艺时光机不再需要你刻意去「记日记」，而是为了帮助我们对抗遗忘，更好地记录生活。 我们可以根据需要，调整小艺时光机的内容范围和时间范围，我自己用了一段时间之后，把能打开的选项全打开了，这样够完整。 我们授权之后，小艺就会帮助我们整理出一个时光轴，通过无感收集生活中的数据，根据照片、位置、天气、听歌、通话等各种各样的信息，帮助我们记录生活、热爱。 我们可以根据小艺的建议，来生成某个时间、某次出行、某个心情的手记。 小艺会综合所有信息，包括我们拍到的画面，帮助我们生成 AI 文案。文艺风格写的太肉麻？试试网红风格，感觉能直接拿去发小红书呢。 超能小艺真的超能干，它能通过数据足迹就能还原生活，帮我把生活琐事变成了珍贵记忆。 小艺深度研究：搜索+推理，你的 AI 战略智囊 信息过载时代，我们缺的从来不是信息，而是从海量噪声中提炼结构化洞察的能力，这也正是专业人士与普通人的核心差距。 根据我自己的使用体验，深度研究就是当前最具价值的 Agent 场景。小艺深度研究可以帮助我们一次性收集、整理上千份文档，并进行深度推理和分析。 比如，我想让小艺帮我研究「如何减轻 AI 的幻觉」，我只需要把这个主题告诉它，小艺就会自己生成一份研究方案，经我审定后才会开始工作： 小艺开始研究之后，我们就可以切出去干会别的事情，毕竟深度研究需要的时间还蛮久的，小艺在这 15 分钟的时间里，会勤勤恳恳地搜索并整理上千份文档。 过程很重要，结果更重要，当你看到深度研究的结果时就会明白，我们等待的时间是值得的，因为小艺竟然可以一次性为我输出一份 36 页的 PDF： 甚至，这还没完，还附带了一份 40 页的 PPT： 这给我的感觉，已经不单单是「助理」，而是「合伙人」级别的能力。 也许很多人对 AI 的误解还停留在「更快的百度」。错，大错特错。 因为我们使用传统搜索引擎时的工作是线性的：输入关键词 -> 弹出一堆蓝色的链接 -> 你一个一个点开 -> 关掉广告 -> 寻找关键句 -> 复制粘贴。这个过程极其消耗「认知带宽」，效率极低。 而小艺深度研究，本质上是一个「思考+搜索+思考」的过程，小艺会拆解我们的问题，进行多轮搜索，阅读并理解海量的信息，交叉比对不同来源的数据，最终给出我们结构化的报告。 这也是我最想强调的一点：小艺能为我们解放出更多时间。 时间是无法再生的，是每个人最宝贵的资源。节约时间，就等于延长生命。 在过去，如果你想做一份扎实的行业调研报告，或者制定一份家庭年度理财规划，至少需要腾出一个完整的周末，甚至连续几天的晚上的时间，打开几十个网页 tab，把自己搞得头昏脑涨。 但现在，有了小艺深度研究，完成过往需要几天甚至几周才能打磨出来的深度报告，现在只需要你喝一杯咖啡的时间。 我丝毫没有夸张，这是算力对人力降维打击的最直接体现。 超能小艺，将普通人从繁琐的案头工作中解放出来，直接进入战略思考和决策阶段，节约的时间和精力是无价的。 小艺修图智能体：对话 P 图，随身 AI 修图魔法师 表达的欲望人人都有，但表达的技能并非人人具备。以前 P 图还是一门专业技能，现在小艺修图智能体帮我们轻松实现「所思即所得」。 拍不到干净的游客照，没关系，告诉小艺「把游客全部去掉」： 出游遇到阴天，没关系，小艺也能轻松让天空更好看： 我在使用当中发现，小艺的这个 P 图还真的跟别的修图软件不一样，因为小艺还会实时解说自己现在正在对图片进行什么操作或处理，它是真的看懂了照片的内容，也真的具备指哪打哪的魔法执行力。 从「修复」到「创作」，全程只需动动嘴。小艺「第二大脑」不仅能帮我们记录，还能帮我们表达，不仅能处理逻辑，更能理解并执行艺术和创意的指令。 回到最开始的问题：为什么我认为只有系统级的 Agent 才能成为第二大脑？ 因为别的独立的云端 AI 应用永远做不到小艺这种程度。 只有「终端设备第一」的系统级 AI，才能在保证隐私安全（数据不出端/少出端）的前提下，获取足够多的数据颗粒度。 它不再是一个外来的工具，而是大脑的自然延伸。 在 AI 时代，人与人的差距在于「利用工具的能力」。不要等 AI 完全普及了再学，越早用小艺，就越早拥抱 AI 时代，就越早拥有自己的「第二大脑」。,四顾​ 笔记本电脑话题下的优秀答主 话题收录 超能小艺一用就爱 航海家 窈慕颜 等 236 人赞同 先下结论：我已经拥有了真正贴心的 AI 助理，但并非所有 AI 都适配长期私人助理的角色。 大家不妨想想，真正靠谱的私人助理该是什么模样？首先得亦步亦趋，能随时随地分担繁杂事务；其次必须是值得全然信赖的存在，唯有如此，才敢托付隐私资料、交付私人事务；更进一步，TA 得熟知我的兴趣偏好与生活习惯，相处起来才顺心顺手、无需多言； 更重要的是，优秀的助理该有 “预判力”—— 想我之所想，忧我之未忧，哪怕我休息时，也能默默把上周的旅行碎片整理成完整游记，把明天要用到的资料提炼出核心摘要…… 这几项基本要求一摆，就能发现：主流第三方通用大模型虽能力出众，但在 “私人专属感” 上始终欠缺（即便能做到，隐私层面也难让人放心），主观能动性更是短板，大多还停留在 “问什么答什么” 的被动响应阶段。 这个人人机不离手的时代，手机系统内置 AI 其实更适配私人助理的定位 —— 权限更高，能实现的功能更全面；同时依托系统生态，信息安全也更有保障。 我的华为 Mate70 Pro 升级 HarmonyOS 6.0 系统至今，“超能小艺”超能干，越用越贴心，堪称我的“第二大脑”。 小艺知识库：博闻强记的第二大脑 小时候读书，学到古人常用“才高八斗”、“学富五车”来夸赞一个人的学问，我却不以为然，觉得这两个词太过空泛，后来学到“博闻强记”这个词，才觉得直观、贴切。 今天来看，前两个词已不足以形容 AI，但“博闻强记”依旧精准 —— 看来我的“文学直觉”打小就不错。 华为手机的小艺知识库，就是我博闻强记的第二大脑。 TA 不仅支持本机主动上传文件，还支持广泛的来源，像我常用的知乎、B站等 APP 内容，都能一站式收纳，文件类型更是全面，链接、图片、视频、录音、PDF、Word、TXT、PPTX 等格式通通兼容。 只是第一步，作为贴心助理，小艺还会主动完成智能分类，生成摘要。需要时，我可以直接提问获取答案，甚至还可以生成互动式AI播客，伴着知识点入眠。 “博闻强记”的重点在于“强记”，小艺做得尤其好 “强记”是“博闻”的前提，小艺知识库在这点上堪称出色。 单一方式覆盖不了全部场景，太过复杂的链路又会消磨使用热情。而小艺支持拖拽记、记、本机上传三种便捷方式，既简单又全面。 00:08 小艺知识库“记” 以我常用的“记”为例，只需点击各 APP 的按钮，选择“小艺知识库”就能完成。 我有段时间沉迷学习烹饪，在各个平台了大量做菜视频。可有次想实操 “水煮肉片”，却死活想不起在哪个平台，翻找半天无果 —— 相信大家都有过类似的困扰。 现在有了小艺知识库的一站式，这个痛点不复存在。我既能通过全局搜索快速定位到水煮肉片的做法，也能在小艺默认分类的 “智能合集 - 生活点滴” 中直接找到它。 此外，我还能主动创建 “烹饪菜谱” 这类智能合集，指导小艺进行更精细的知识归类，让知识库更贴合自己的使用习惯。 “强记”未必能“博闻”，但小艺真的不怕问 如果“强记”就等于“博闻”，那自从有了硬盘等存储介质，我们早该实现“博闻”了，但事实显然并非如此。 AI 的强大，在于对信息的精准检索，有逻辑的深度处理，以及自然语言互动能力，而小艺知识库，恰恰 “不怕问、答得准”。 作为知乎创 但这份满是专业词汇的全英文财报，我读起来着实吃力。不过只要把文件丢给我的“超级助理”小艺，TA 就能快速生成 AI 摘要，我在闲暇时复访，就能快速掌握关键财务指标、业务经营情况等核心信息。 更进一步，小艺还能把这份财报转化为详细版的，有趣味的“互动式AI 播客”，我通勤时的碎片时间正好用来收听，还能给知乎答主朋友们一起讨论。 除此之外，我把知乎近两年的财报也都进了小艺知识库，通过提问就能让 TA 分析出平台经营状况的同比、环比变化，省去了我逐一对比、核算的麻烦。 小艺时光机：有温度的 AI 才是好助理 又是小时候，看哆啦A梦动画片，大雄他们坐着时光机飞来飞去。 当时只是觉得新奇，有趣，没有什么额外的感触，有幻想，也只是幻想坐上时光机去看未来的星辰大海。 年纪渐长后心境变了，之前出差，飞机上没网，于是打开相册翻过去的照片，越翻越沉默，全程两个小时，也只翻过一年，好多回忆，好多想要坐时光机飞回去的节点。 真要回到过去不现实，但光是翻翻照片，也翻出了唏嘘，翻出了悸动，翻回一些追逐梦想的初心和勇气。 所谓触景生情，人脑是这样的，某年某月某日在做什么，直接检索完全检索不到，想要触发稍微遥远一点的具体回忆，就需要相同的场景，或是一段视频，一张照片，一段旋律来作为“钥匙”，打开那把尘封记忆的锁。 而小艺时光机，就是“第二大脑”为我们分门别类保存好的一把把钥匙，让“第一大脑”鲜活记忆的触发不再偶然，过去不可回，但有了小艺时光机，美好记忆随时重温。 上个月我和女朋友去川西旅行，拍了很多壮美秀丽的风景，回到重庆后，小艺时光机自动按时间和地点聚类，把照片分为“九寨沟”和“毕棚沟”两组，还智能去重、去糊，筛选出最佳照片，生成了图文并茂的游记。 除了照片，小艺时光机还能跟天气、音乐 APP、华为手表等联动，在“时光轴”里记录下我深夜的单曲循环、午后的悠闲散步、还有银杏树撒下的金黄深秋。 这些被时光轴记录的点滴，还会被小艺进一步筛选，推荐给用户生成“我的手记”，好让记忆的钥匙更加深刻的印在小艺时光机里，留住瞬间，也刻下心情。 如果自己不想动笔，小艺还支持 AI 生成多种风格的文案 —— 不得不说，TA 的“文学直觉”，比我强。 苏轼在《和子由渑池怀旧》中写道： 人生到处知何似，应似飞鸿踏雪泥。 泥上偶然留指爪，鸿飞那复计东西。 有了小艺时光机清晰记录的“雪泥鸿爪”，不管下次坐飞机是去往何处，回首明明白白的来时路，我都能更坦然地“无问东西”。 小艺修图智能体：吩咐下去就能修 移动互联网 AI 时代，修图门槛确实降低了，比如 AI 抠图、AI 消除，比当年用 PS 简单太多。 但门槛依旧存在：对我来说，基础的亮度、饱和度调节还能应付，可对比度、氛围、高光、阴影、色调、曲线这些进阶参数，完全不知道该怎么调； 对年纪大的长辈而言，更是一点点门槛都能让他们却步，之前教我妈用 AI 消除路人，光是找到这个功能，不算深的 5 级路径，就演示了好多遍，后来她还是忘了。 而“小艺修图智能体”几乎做到了 0 门槛，只需跟小艺说“帮我修张图”就能直达功能，选好照片后，用语音“提需求”就能完成修图。 比如这张银杏叶的照片，小艺不仅按我的指令完美消除了路人，还主动优化了色调，让秋日氛围更纯净； 贴心的是，小艺还支持多轮对话持续修图，直到达到满意效果。这哪里是助理，简直是任劳任怨还超专业的 “百万级修图师”。 如果没有修图思路，也可以让小艺给出建议，或者直接说 “把这张图修得更好看”，TA 会自动优化画面效果。 自从小艺进化出这个功能，我的手机就时不时被女朋友 “征用”—— 看来，是时候送她一台华为 Mate 80 了。 小艺深度研究：超能小艺超能干 我生活在重庆，一个本该主打川菜的城市，如今能轻松买到汉堡、喝到可乐，可想吃到一份正宗的水煮肉片，却越来越难。 就像我们习惯了刷短视频傻笑，可真想要学点专业知识、做份靠谱攻略时，才发现越来越难在互联网上筛选出真正有价值的深度内容。 2010 年 10 月 15 日，知乎正式上线前的一份内部全体信，开篇就写道： 我们相信一点，在垃圾泛滥的互联网信息海洋中，真正有价值的信息是绝对的稀缺品。 15 年过去，深度内容在这个快餐时代愈发珍贵，好在有“小艺深度研究”这么一位专家级助理，能帮我们做深度搜索、办专业的事。 小艺深度研究刚上线不久，我最近初次使用，便被其强大实力深深震撼。 事情是这样的：我女友是旅游专业出身，毕业后虽改了行，但在主业之余，仍会偶尔给本地媒体的旅游专栏供稿。 近两年，重庆开通了直达乐山的高铁，重庆人周末、节假日前往乐山旅游，品尝钵钵鸡、绵绵冰、甜皮鸭等特色美食的热情持续高涨。 于是，女友和她的编辑策划了一期聚焦 “乐山美食文化”，且兼具 “情侣旅游攻略” 属性的深度专栏内容。 2024 年我们曾去过乐山，所以女友一开始对完成这期专栏信心满满。可很快她就发现，当初走马观花几天的零散经验，用来支撑深度创作，不过是杯水车薪。 也尝试在网上搜寻资料，结果发现各类信息中充斥着商业广告、低质，还有大量高度重复的人云亦云，真正有参考价值的内容寥寥无几。 眼看截稿日期越来越近，女友坐在电脑前一筹莫展、唉声叹气。我抱着试试看的态度，把这个课题交给了小艺深度研究： 首先，TA 以“专家思维”拆解我的需求，主动确认出行人数，游玩天数，预算等关键信息； 接着，TA 深度检索，解析，整理，提炼了 679 篇相关内容，最终生成了一篇名为《乐山美食文化深度研究：基于非遗传承与情侣沉浸式体验的策略分析》的专业报告。 这份报告有摘要、有目录、有表格，有 40 篇参考来源，整整 21 页，13932 字，内容之详实，完全超出我和女友的预期： TA 先总述乐山美食的文化底蕴与非遗传承，再提供交通、住宿的全方位建议； 接着按 “经典非遗与老字号”“市井烟火与古镇风情”“甜蜜收官与伴手礼” 三大主题，细致安排了 3 天 10 顿美食，每一顿都明确标注了店铺名称、详细地址和特色菜品；甚至还贴心地附上了伴手礼购买清单； 更让人惊喜的是，考虑到情侣出行的特殊性，报告还特意安排 Day2 不用早起吃早餐，让行程更宽松惬意；连情侣如何拍出美食大片的技巧都单独成章做了讲解…… 有了小艺这份深度研究报告做参考，女友的专栏撰写如有神助，次日就提交了初稿。 这样一篇堪比学术论文的深度报告，女友坦言 20 天也不可能整理出来的详尽攻略，小艺深度研究只是挂在手机后台，仅用 20 分钟就完成了。 不难想象，同样的 20 分钟，超能干的小艺深度研究还能输出深度洞察的行业报告、切实可行的商业计划书。 有了小艺深度研究，今后我们想做的事，想深度探索的领域，都不妨先让助理小艺“研究一下”。 越早用小艺越早拥抱 AI 时代 我有个关系不错的前同事，出于各种有的没的顾虑，他的华为手机还停留在 4.3。 之前每次聊天，我都会跟他鸿蒙日新月异的进展，劝他升级。 最近一次，我什么都没说，直接把《小艺深度研究报告-乐山美食文化深度研究：基于非遗传承与情侣沉浸式体验的策略分析》甩给了他。 然后，他就主动更新了系统 —— 他说，这比第一次用 DeepSeek 的“深度思考”更让他震撼。 我为他感到开心，华为小艺终端设备第一的 AI 能力，值得被更多人看见和使用。 越早用小艺，越早拥抱 AI 时代！,养猫的哈士奇​ 华为手机等 3 个话题下的优秀答主 话题收录 超能小艺一用就爱 187 人赞同了该回答 AI 的学识和记忆力远超常人，这么说没错，但这顶多算 “合格助理” 的入场券，距离“优秀贴心”差的还挺远。做一个优秀贴心的助理，真正的门槛从不是 “能不能记”，而是 “懂不懂你”，能够察言观色，急你所需，助理非常考验“情商”。 有没有这样的AI能作为“优秀贴心的助理”呢？察言观色、端茶倒水这些可能需要等具身智能成熟之后，但是在其它方面，现在的AI其实已经能做很多助理的工作了。根据我对各种AI的使用经验，目前体验最好的是华为Mate 80 Pro Max上的，方便和智能程度已经超过我的预期了，在很多方面还真的有点”贴心助理“的味道了，这不是吹牛，下面我就详细地说说贴心在哪儿。 就像问题说的“AI的学识和记忆力远超常人”，没错，但是，你有没有发现大部分时候AI给你的回答都是基于互联网上的信息总结归纳而出，很多时候并不是你想要的东西，可很多用着用着就 “跑偏”，要么答非所问要么信息杂乱。根源就是这些产品都是服务大众的，或者说这是大家“共用的助理”，而不是你的私人助理，大众说什么的都有，所以他获得的信息也是非常繁杂，还有很多垃圾信息，这不是AI的错。人类助理的“贴心”靠“长期观察”，AI的“贴心”靠“你喂给它的数据资料”，我们要的“贴心助理”是能精准预判你的需求。想要 AI 真的为你所用变成你的私人助理，关键是让它深度融入你的生活，知道你的喜好， 而手机就是我们日常生活最离不开的产品，手机系统上集成的AI是最容易成为你的“私人助理”的。 当然，这只是理论，现实是虽然很多厂商都说自己是AI手机，实际端侧AI表现却良莠不齐，比如我现在双持Mate 80 Pro Max和海外某高端旗舰机型，感受就很明显。它算是智能手机上最早的AI，引领了语音助手的发展，但是时至今日依然是个简单的语音助手，对话体验有时候也不好，即便完全体的美版也要不时的调用ChatGPT，你想让它按你的需求做个PPT？很多人的第一反应肯定是手机上的AI能做这事儿？在用过Mate 80 Pro Max上的小艺后，我发现还真能做，而且做的很好，也不需要跳出调用第三方的AI。这倒也不意外，因为华为在AI方面从计算卡到云再到大模型都有投入，级AI，与自研芯片以及云服务结合可以做到最优化，让华为强大的AI在手机端很好的落地。事实上的使用也是如此，小艺确实是我用过的领先的端侧AI。 AI是需要学习的，我们什么，工作要处理什么内容，了哪些资料，这些都是需要学习的内容，学习你的东西才会成为你的私人助理，学习别人当然用着不贴心。小艺就完全可以胜任这份工作，里面的小艺知识库尤其好用，它什么都能学，比如我划手机看到一个好的视频，好的知乎回答我可以随手就丢给小艺，通过放入小艺知识库。我就把知乎的几百个优质回答、工作里的权威报告、学习时的 PDF 课件，甚至录音、视频、PPT 等各种格式的文件，全扔进小艺知识库。知识库里的资料越丰富，小艺就越懂你的知识体系和偏好，提供的服务也越个性化，越贴心。 说来也惭愧，我知乎的上百个优质回答，有不少我都没看完，最近发现小艺知识库可以自动生成互动式AI播客，现在每天睡前或者开车时都会让小艺朗读这些的内容总结，有不清楚的还能直接询问，就像一个一个秘书拿着资料向你汇报总结一样，这不就是妥妥的贴心助理吗？ 如果没有小艺知识库我这些的优质回答以及一些视频资料可能永远尘封在夹里了，这段时间居然把之前的优质回答都“读”完了。 我出差很多，会去不同的地方，会逛不同的风景，还去各地的博物馆，有些地方值得多次去，也会给朋友去哪里有什么必玩必逛的地方。这里有两个问题，一个是旅游攻略，比如我去成都出差，需要做一个攻略，可是我了很多旅游攻略，没时间仔细研究，怎么办呢？同样的，放入小艺知识库，让它去帮你分析，再根据自己的行程、时间、喜好询问小艺，让它给你做出一份适合你自己的攻略来，你还能让它帮你直接订票，一气呵成。 旅游过了，有些地方、有些人值得回忆，可是时间长了就很容易忘记，甚至有些再翻看照片也想不起来。这时候就需要用到小艺时光机，这是一个非常好的功能，会自动整理用户日常的运动、拍照、听歌、去过的地方、通话记录等信息。这里得说下，小艺自动挑选照片生成手记，我用过了，你别说，小艺挑照片挺有水准的，闭眼的、模糊的、人脸被截一半的，这些它都会自动略过，还会根据照片的清晰度、构图、色彩等排序挑选，所以生成的日记质量挺高的。 至于文案，AI文案可以根据你输入的经历、心情、遇见的人或特别的事而给你生成文艺风格、网红风格、现代诗风格三种，这是其它手机应用不具备的功能，而且它可以给你联动日历、日程甚至华为手表的运动健康数据，我们也可以把这个手记到社交媒体上。 我们需要助理，本质是想摆脱 “繁琐事务的内耗”—— 大到推进市场调研、撰写专业论文，小到规划毕业旅行、做汽车购买决策，都要耗费大量时间查资料、写报告。之前做一个通信技术发展的报告，光查资料就查了2天，整理写了1天，累得够呛。 有了小艺的深度研究功能这就真的是省事儿太多了，比如我要做一个“中国家用充电桩市场的研究并做一些预测”，这种事情直接交给小艺深度研究就可以了。生成的是研究文档，也可以生成研究报告和PPT，需要多等一会儿。生成的报告质量非常的高，老实讲，让我自己研究，可能两天也写不成这样，而且我仔细看了下，小艺引用的资料还是比较靠谱的，再加上它的深度搜索与分析推理能力，不仅能帮助用户收集和整理上千份全网信息文档，更擅长结合专业领域权威知识库对内容进行深度的推理和处理。从这方面来讲，说它是助理甚至有点小瞧它了，算上生成报告和PPT的时间也就半小时左右，真帮大忙了。 最后我们再说一个常用的功能——修图，我们很多时候都需要修图，比如之前去茶卡盐湖旅行，天空不作美灰蒙蒙的；再比如朋友聚会合影，有路人闯入；还有拍完照事后发现眨眼了；冬天拍照树叶都掉光了—— 这些场景很常见，如果用PS需要磨半天，如果不懂PS还要拜托别人来修，折腾半天能不能满意也不好说。但现在有小艺修图，直接对小艺说 “把天空换成湛蓝色，保留云朵质感”“消除背景里的路人”“背后的树上冒出樱花”，3 秒就能出效果；如果第一次颜色偏深，补一句 “再淡一点，偏自然色”，立马优化到位，完全不用懂任何修图技巧，真正实现 “修图不求人”。 想要有一个懂你的AI助理，必须要有私人知识库，必须要更懂你，必须能帮你做一些繁琐的事情，其最好的载体就是手机。我们的衣食住行、工作学习都离不开手机，它记录着最多的个人数据，也是最贴身的设备。而小艺早已深度融入手机生态，把 “懂你” 和 “能干” 做到了极致：它能记住你的知识偏好，帮你搞定繁琐工作，解决生活琐事，甚至接住你 “突然想起” 的细碎需求。 我觉得超能小艺真的 “超能干”，不止是功能强大，更是懂你，更是贴心。越早用小艺，越能提前拥抱真正的 AI 时代 ，不用再被琐事缠绕，拒绝内耗，把时间花在更有意义的事情上，这才是 AI 助理该有的价值。,Eidosper 国产与进口交锋，情绪与理智交融。 话题收录 超能小艺一用就爱 203 人赞同了该回答 提到手机的AI，很多厂商的想象力也就止步于“喊语音助手点个外卖”了，毕竟是受制于谷歌的系统。但做到的远不止这些——小艺真正的开始融入到文件的层面，开始了“File is knowledge”的路径，这也是带来的惊喜。 Unix操作系统有个设计哲学，叫做“Everything is a file”，意思就是“一切皆文件”。这种设计哲学被后来的Linux，乃至当下的鸿蒙所继承。 但现在已经是2025年了，时代变了，AI的发展也如火如荼。我们对AI的理解和使用，绝不应该局限于“喊小艺点外卖”这种粗浅的层级，要更加深入。 在我之前的一个文章[1]里面，提到过“AIFS”的概念：将AI融入到File-System之中，自动化的管理文件。这是由于我经常有大量的照片、文档等文件很多时候分布在各个文件夹，管理起来非常痛苦，所以我认为未来一定会有一个AI管理的File-System。 而超能小艺已经开启了这个征程：通过小艺知识库深入文档正文、通过小艺时光机去收集照片、音乐、运动的点滴——来全方位的通过AI整合个人的信息。 这对于我来说毫无疑问是一件极为重要的事情。 1.1 游忆碎片&心情记录 我曾经到过很多的地方，但往往疏于记录，现在遗忘了很多的细节。小艺时光机的出现让出游的记忆更容易被记录下来：借助小艺的AI识别功能，可以自动识别地点和大致的出游内容。 当然，你也可以选择自己创作内容： 或者交给AI，生成一段文案： 有种老式情诗的感觉，放在现在还真有点“纯真年代”的意味。当然，也可以写一段文案，让AI改写成三种预设风格：文艺风、网红风和诗歌体。 旅游的行车时间，也可以听歌记录心情。小艺时光机的时光轴就可以自动识别歌单： 记录歌曲心情也是很方便的： 健身也同样可以被记录： 这是我一次扎马步的小记录，虽然姿势并不标准，但健身总要有一个开端。 1.2 小艺修图 之前去艾溪湖的时候也有几张“废片”，但是小艺可以拯救一下，可以通过一句话完成照片修复、人像精修： 拯救之后的对比图如下： 废片的核心原因是湖的表面有各种漂浮的树枝、杂草，小艺可以正确的修好，这样看上去就顺眼多了。 有很多的风景人山人海，很难拍到无人的视角。小艺修图去除路人的效果就蛮好，修完发朋友圈还是绰绰有余的。 不过论创意还是群友，我把我在杜甫草堂的照片发群里，立马有群友给我P了一套科技装备，我用小艺修图也复现了一下： 想给自己加点“道具”也是轻轻松松。 但偶尔也有有趣的时候，当我尝试把表情改为微笑： 总之就是十分的好笑。 2.1知识&播客作为知乎的重度用户，其实有很多的博主能让我感到心头一震。但知乎本身的功能只能局限于知乎内部，很难有效的和外部的知识一同整合，小艺知识库就实现了这个功能，可以和外部的内容整合到一起。 除此之外，阅读专业的论文也是一把好手，能和小艺交互式的阅读论文，效率也可以提升不少。 针对论文的具体内容进行提问，效果非常不错。有时候还能提取到其他AI工具没有重视的知识点，这也算一个小惊喜。 生成的总结不仅可以自己看，更可以一键转化为播客，解放双眼。你以为这就完了？不是的，这播客还是一男一女、一唱一和、滚动字幕： 说实话还挺声情并茂的，语气也做的很不错，抑扬顿挫都有，不会听睡着。 2.2 小艺深研 对于新机型（例如Mate80系列、Mate X7系列等），小艺还提供了深度研究功能——通过提供一个主题，实现自动化的生成论文、报告和演示： 生成的内容可以说是有些出乎我的预料的： 参考的资料数从最开始的11篇，数次逐渐扩大，最后参考了735篇资料。前前后后整理的半个小时，内容十分详实： 甚至还给生成了一个专业风格的封面。 讲道理，我当初研究鸿蒙6的流畅性的时候，都没找到这么多资料。实际上借助搜索引擎能获得的公开资料其实并不多，我当时研究的时候甚至都被迫去专门找专业人士请教才找到一篇论文。 不过，生成文档的同时还会生成一份报告、一份演示文稿。报告采用HTML格式，几乎所有设备都可以顺畅打开浏览，里面的图表做的比我都好。演示文稿则直接超过了我的能力，让我自己做肯定是做出不这么精美的演示文稿的。 我还尝试让小艺研究了Freebuds Pro 5的音质： 报告正文里提到“双路独立驱动音频系统”，我觉得非常的惊讶： 意料之外但情理之中——直接把高频单元和低频单元用两路DAC处理，这确实合理但又有些“奢侈”。 小艺深度研究还是很适合做“全景式预研”的，尽管目前AI查证的资料还需要人工校对，但小艺深度研究可以对现有资料进行全景式收集，这极大的降低了研究的难度。 其实这些AI功能只是一个开始，拥有系统级优势的小艺，后面可以很轻松的拓展出“小艺小艺记录当前页面到知识库”，直接记录当前的网页或者APP页面。我还看好这个AI整合的功能深入File-System以及Application Framework，届时整合起来会更加方便。 当下的小艺挖掘的内容还有一些局限，我希望以后可以加入一个“小艺挖掘”功能。我出门旅游很多，但当下的小艺时光机只识别了其中大概十分之一。倘若能指定挖掘例如“帮我挖掘一下我在8、9月份的濮阳之旅”，那么就完善很多了。 当然，小艺的AI体验并不局限在折叠屏、直板机之上，鸿蒙电脑也支持这个功能： 打通全场景，先从小艺能做到全场景开始。 鸿蒙6我是越用越，现在已经成为我办公和日常的主要设备。但即便如此，我对鸿蒙6各项功能的开发和探索也只是冰山一角。但不知不觉又更新了这么多功能，真可谓“士别三日当刮目相看”，真的超能干。 在过去其实小艺在语音助手中的表现并不算特别出色，但随着大模型的成熟、智能体的出现，小艺现在已经远非当初的样子了。之前鸿蒙6超越想象成为我办公和日常的主力机，看来之后小艺也会成为我整理游记、记录知识、综合调研的核心工具。 越早用小艺越早拥抱AI时代，也期待大家能去尝试一下小艺的各项功能，没准也能超出你的预期~ ^https://zhuanlan.zhihu.com/p/1905692759974711454,平凡​ 新知答主 话题收录 超能小艺一用就爱 62 人赞同了该回答 这两天我在玩AI绘画，主要是想画一个诠释self-reflection（自我照见）的作品。在多轮次的交流中，我反复修改 prompt、调整风格、追问细节，最终 AI 给了我这样一张图。 AI生成素材/图片，请注意识别 在多轮次的交互中，我就在想一个问题，那就是过去3年，AI的进化不可谓不大，从“只能聊天”一路进化到“能写代码、能写论文、能画画、能做 PPT”。甚至现在的AI可以直接生成一个复杂的包含几千行代码的项目。 智力的提升是显而易见的，但是问题依旧存在，那就是AI真的帮助到你了吗？特别是如果我们把视角拉长一点，会发现这个问题直击本质： 当 AI 的学识和记忆力都已经远超常人时，我们每个人，是否真的有机会长期拥有一个优秀、贴心、稳定陪伴的“个人助理”？ 如果你把 AI 看作“一个能帮我做事的工具”，答案永远模棱两可：能帮忙，但总差点什么。 但如果你把 AI 想象成 Second Me——一个能长期记住你、理解你、表达你，甚至可以陪伴你多年生活轨迹的“第二个自己”，那答案就会变得清晰起来。 这个“Second Me”不是替代品，也不是分身，而是一面“深层次的自我镜子”。 TA知道你过去什么、坚持什么、犹豫什么、成长成什么样； TA能帮你把碎片化的自我，重新缝合成连贯的叙事。 而在当前的 AI 生态里，真正往这个方向走的，是那些和用户长期绑定在一起、能够深度参与日常数据流的终端侧 AI 助理。 其实原因很简单，终端侧的数据离我们最近，是跟我们接触频率最高最广，所以理应他们最懂我们。 超能小艺就是这样的例子，TA正在率先朝这个方向迈进。 vector similarity：AI 为什么能记住“很久很久以前的我”？ 我在给学生教授统计课程时，经常会用“相似度”来解释数据之间的关系。而当我们把这个理念迁移到 AI 世界中，就自然过渡到一个关键概念： vector similarity（向量相似度） TA的原理非常的简单，就是TA根据我们在初中时候学过的cosine来算两个内容的相似度，比如下图的猫坐在毯子上，两句相似但又不完全一样的话，在vector similarity里面的相似度很高。 但是不妨碍这两句话的时间点相隔几个月，甚至几年。你看，这就跟我们人类的记忆一样，不是线性，而是跨越式的，我们可以将相隔数十年的两个内容瞬间联系起来。 TA就是现代知识库（RAG）技术的基石，也是 AI 能把长时间跨度的信息“串成一条线”的核心原因。 RAG = LLM 的聪明大脑 + 可随时更新的个人知识库 向量相似度让 AI 能够在海量的信息中，以“理解语义”的方式定位相关内容；大模型则负责推理、总结与表达。 这两者组合，等于是给 AI 配上了： 不衰退的记忆能力 跨时间整合信息的逻辑能力 随取随用的知识检索能力 这也是为什么现代 AI 应用大多适配 “LLM + RAG” 架构，使其具备成为长期稳定个人助理的技术前提：不是单纯存档，而是能在未来关键节点被重新调取出来，参与到你下一次重要决策中。 什么是“合格的个人知识库”？ 相信大多数人都或多或少的接触过知识库产品，你可能会有这样的体验： 能存东西，但懒得用 查找不便，搜不到想要的内容 更像“夹满文件的网盘”，而不是“理解你的人” 所以，一个真正合格的个人知识库，至少要满足三个层级： 吸收（收集与接纳） 内化（理解与抽象） 升华（表达与陪伴） 对应到 Second Me 这个框架，就是：Memory（记住我）Understand（理解我）Expression（表达我） 第一层：吸收（收集与接纳） 传统的软件世界里，你得操心格式：这是 PDF，那个是 Word；有的是手机截屏，有的是扫描件；还有语音、会议录音、照片上的手写字。 而对一个真正的 Second Me 来说，这些都不应该是问题。 你的生活是什么样的信息形态，TA就应该能处理什么。 小艺知识库智能体首先解决的就是这一层的“吸收能力”——背后依托的是多模态（multimodal）模型。 可以看到，我把各种各样格式的文件都扔给了小艺，照单全收。 这就是多模态（multimodal）模型的能力，可以处理各种不同类型的数据，并且TA并不是简单的归档，而是： 自动辨识文档内容 提取关键主题 将同类主题串联 按时间线和领域进行整理 这些要求是单纯的数据库做不到的，只有结合了AI和RAG才能适配。 第二层：内化（理解与抽象） “记住”和“内化”的差别在于： 记住：保存事实 内化：理解这些事实之间的逻辑、趋势与模式 这是 LLM 特别擅长的地方——通过海量训练，TA具备超越普通人的理解能力。再结合现在的大模型具备的思考能力，可以对我的个人数据进行更具针对性的理解。 我把几份文档扔给了小艺，然后让TA做了三件事： Q1：“请把这份 PDF 的知识结构拆成 5–7 个教学要点，适合给硕士生讲。” Q2：“如果我只讲 15 分钟，请把这些内容压缩成一个 mini-lecture 脚本。” Q3：“请为这份内容生成 3 个讨论题，必须能考察学生的理解能力。” TA给出的结果非常的精准。 小艺给出的不是“罗列内容”，而是真正的听懂了我的深层要求，比如针对于硕士生、mini-lecture里面包含的英文需求等。 你会发现，这已经不是单纯的搜索结果，而像是一个长期观察你的“研究助理”在给你做定制化的回复。 第三层：升华（表达与陪伴） 这一层其实是刚需，因为对于收集癖来说，最大的困难在于信息的处理，单纯的信息检索和整合在日益增长的文件面前捉襟见肘。 不知道你关注到这里没，TA有一个按钮，写的是生成播客。 这就一下子把表达从文字转变到了更容易接触和消化的语音，我自己在体验“超能小艺”的互动AI播客功能时，最大的感受就是这种“表达层”的跃迁。 TA会基于你存放在知识库里的内容，用一种轻松的、对话式的方式，带着你回顾一段时间的生活，比如：“那段时间，你主要在处理哪些事情？”“如果现在回头看，会不会有新的感受？” 不是审问式的问题，也不是分析式的讲解，更像是一种温柔的引导，帮助你把当时没有想清楚的东西“重新说一遍”。 在表达这一层面，TA可以： 生成一段年度回顾的播客小结，让你在路上就能听完自己一整年的变化； 做一段带着你个人习惯与节奏的音频，像是在向未来的自己讲述故事； 甚至自动整理成适合发朋友圈/小红书的年度简介，让表达变得轻松而自然。 这种感觉很微妙：你不是在“翻文件”，而是在通过TA，把那些散落的记忆重新连成线。 类似的功能还有一个就是小艺时光机，TA是一个被动的程序，可以自动把碎片化内容串成连贯轨迹，例如过去一段时间的运动日常或者过去的物理足迹。 这种“持续性”与“长期记忆”正是 Second Me 的核心条件。 Deep Research 如果说知识库更多解决的是“我和过去的自己如何对话”，那么 Deep Research 则是回答：“我和未知世界如何对话”。 我把 Deep Research 看作是一个 Agent 能力的综合试炼场，因为TA涉及： 信息源的广度与深度 信源的实时性与可靠性 大模型处理长上下文的能力 输出结果是否可直接使用（而不是“看起来很厉害”） 我问了TA这么一个问题， 请对最近的大模型attention机制的研究进展进行全方位的研究 TA花费了将近20分钟完成了这个研究，且同时产出了三份输出。 分别是研究文档、研究报告和PPT。 Deep Research，我最关注的就是引用的文献的含金量，从这个列表来看，大多为专业平台内容，可信度值得信赖。 结果的渲染也没什么问题，表格、markdown之类的实现的都很完整。 更让我觉得可用的是TA可以直接生成研究报告，且做了很不错的优化，基本上可以直接使用。 PPT也一样，检查一遍直接用就行。 Deep Research是检验Agent能力的benchmark，这个能做好，其他的功能大多都能够有不错的实现度。 多模态时代的“修图助手” 说一个更接地气的场景：修图。 很多人其实对“AI 大模型”的印象还停留在文字、代码、对话上，对图像处理的想象还停留在“PS + 复杂操作”。 但在多模态模型的加持下，修图这件事情已经发生根本变化： 现在你只需要用自然语言，就可以完成过去可能需要多层图层、多次调节才实现的效果。 我上传了一张我在新疆拍的照片，然后让小艺修图智能体给我精修。 你看，TA直接把我坑坑洼洼的背景给填平了。 “超能小艺”背后就是刚刚提到的具备多模态理解能力的 AI，可以直接把这类自然语言翻译成复杂的专业词汇，比如分割前景/背景；调整曝光、色温、对比度；做局部修补与克隆。 对普通用户而言，你没必要再去学P图，只需要学会更准确地描述你的感受。 回到最开始的那个问题：AI 的学识和记忆力远超常人，这是否意味着我们终于可以长期拥有一个真正优秀贴心的助理？ 从技术条件看，答案已经非常接近“是”。 vector similarity + 知识库，让 AI 具备了“长期记忆”的骨架； 多模态 + RL 后训练，让 AI 不只记住，还能“看懂你”； Deep Research 与多格式输出，让TA能承担真正有价值的工作； 终端设备上的深度集成，让这一切都不再停留在“产品 demo”，而变成每天真实发生的事。 而像“超能小艺”这样基于终端生态打造的 AI 助理，正在把 Second Me 这种概念，从想象变成可体验的现实。 越早开始使用这样的 AI，你的“Second Me”就越早开始积累记忆；越早使用小艺，越早拥抱 AI 时代。 送礼物 还没有人送礼物，鼓励一下,雩火是渔火 科技/数码/美食/文学，啥都看，没成分 话题收录 超能小艺一用就爱 50 人赞同了该回答 处理工具类的功能实际上已经在很多行业有了助理级的能力，现在比较麻烦的是问答类的，很多从公共社交平台取数据的AI，因为平台数据鱼龙混杂半真半假，导致AI反馈的内容会出现严重失真。如果大家搭建一个个人专属的知识库，汇聚一批自己筛选过的高质量数据，那确实可以让AI成为自己的助理，辅助自己工作学习。 其实很多平台已经开始意识到这个问题了，逐步开始面向用户推出个人可自定义的知识库功能，比如说知乎就有知乎直答可以搭建，还有手机品牌也推出了可以跨社交平台的一些知识库相关功能。 我的知乎直答，但是只用过一段时间 知乎这个功能做的还是很好的，推荐主刷知乎的朋友们用起来，因为我用的平台比较杂比较多，所以手机品牌的会更适合我一点，给大家一下华为基于做的小艺知识库、小艺时光机、小艺深度研究等功能，确实能提高一定的工作学习效率，希望对大家有所帮助。 这些新功能只在鸿蒙5及以上的部分设备上支持，可以在小艺内查找，也可以直接长按底部导航条或者电源键呼出小艺进入，除了华为官方的一些AI功能，也有优秀的第三方合作伙伴在小艺智能体广场里。 先说小艺知识库智能体，小艺知识库最大的优点是知识支持的数据来源和格式比较广，用户可以往里面塞各种主流的内容，除了PDF、Word、TXT、PPT等格式的本地文件，小艺知识库还支持网页内容、应用帖子、录音、图片、视频等内容，这应该是市面上为数不多能够有这么强兼容性的知识库工具了。 个人认为，一个好的知识库，应该是博采众长的，那就免不了要有多种格式和多个来源，兼容性格外重要。以国内的几个平台为例，小红书的种草和攻略知识很多，但是长文创作氛围就不足，这类深度长文在微信公众号和知乎会多一些，如果要更加严肃的内容，那社交平台的分析简析可能也不太够用了，最好使用到论文原文件。 小艺知识库在这点就做的比较好，基本上常见的互联网知识传播方式都能够支持，目前非要说遗憾，那就是一些相对封闭的APP内部帖子支持还不够，这个只能等逐步适配，不过最主流的国内平台都是支持的： 因为小艺知识库AI问答基于知识的内容和风格，那么如果我们的内容整体优质，那小艺的回答就会好于目前搜索引擎缝合的AI回答，可以避开很多错误的内容。 举个例子，手机这边每年都会有上市前爆料，这些爆料并不太准，但是因为爆料帖众多而且热度还不错，所以搜索引擎页的AI总结就会直接将错误的内容总结出来，哪怕最终产品已经量产了，这些错误信息依然可能会对大家产生误导。例如拿某主流搜索引擎查询“华为Mate70X配置“，众所周知华为根本没有Mate70X这个产品型号，但是AI总结被早期爆料误导，依然给出了错误的信息，这就是典型的数据被污染带来的问题： 而如果询问小艺知识库，就可以避免被这些神棍爆料帖影响。因为我的知识库里面只有顺手塞进去的头部博主优秀测评、产品官网等靠谱的信息，小艺就不会给出错误的引导。 这种差异在短期内可能不明显，毕竟Mate80系列也才刚刚发布，我很清楚地记得Mate70系列的产品分布情况，但是几年后呢？现在有多少人能清晰地记得Mate8的产品亮点细节呢？数码创 长远来讲，构建自己的知识库，不仅仅可以建立起一个指导工作学习的图书馆，还可以避免掉很多AI问题带来的“岁月史书“，无论是对于创 除了上文举例的偏向查询类的知识，还有学习成长类的，比如说新能源汽车架构、芯片可靠性设计等，针对这种知识建立的知识库，小艺知识库还有互动式AI播客和播客功能来主动push用户学习，还有智能合集提高效率，以及复访用于复习知识点。这些功能可以更好地辅助大家去学习一些新知识，或许之前大家调侃的“了就是学习了“会因为AI的加入变成现实（笑） 除了知识问答，助理还需要进行一些基础工作的处理，比如说修图、写简单代码、规划简单方案……这部分现在的AI其实做的很好了。 由于这类功能五花八门，目前并没有特别强的单个AI可以做到全方位第一梯队的程度，所以在这方面，我建议在大家选择多个AI聚合使用，最简单的办法就是回答开头里面“广场“部分，不同的需求找不同的AI，DeepSeek、讯飞晓医、小艺修图……总有一款适合你。 丰富的不同场景AI 这里面我觉得比较有意思的两个功能是小艺修图和小艺深度研究。前者可以让小艺对于你拍摄的图片进行细致的微调，除了常规的亮度饱和度这类指标，还可以让小艺消除和增加一些画面内容，比如说景区照片去路人、风景照片加前景等等，大幅度降低了后期修图的门槛，用户通过直白的口语描述即可进行照片内容的调整，就可以让照片风格乃至构图都更加接近自己心里的预期。 如果说小艺修图是降低后期门槛，那小艺深度研究就是增加AI产出深度，这个功能不再是简单的查询总结，而是以严肃的研究级对话作为目标而设计的，小艺会直接以论文、各类官网、专业网站的内容作为参考，筛选成百上千的内容然后选择一部分生成论文格式的研究报告文档或者是PPT，产物足以作为严肃工作内容的基石，对于一些工作而言，只需要在小艺的基础上再加入自己的思考理解，调整一下就可以作为工作产出。 这个功能真的非常夸张，除了需要20分钟左右的等待，没有明显的缺点，生成的内容非常细致有说服力。我那天以大模型幻觉对舆论事件中的谣言传播和舆情激化影响作为简单的输入，小艺参考了六百多篇正经内容，然后参考了其中五十多篇给了我一篇小论文报告，不开玩笑地说，比网上大部分的博主小编写的要好…… 大家可以去门店试一试，目前鸿蒙电脑和Mate80系列都支持这个功能，真的和常规的简单对话级的AI总结不一样，内容的AI味淡很多。 最后就是和现实生活之中助理一样，工作之余提供情绪价值的AI功能，比如说小艺时光机，可以把我们生活中一些温暖的时光更好地呈现出来。 在得到授权之后，小艺时光机将会分析运动健康、相册、音乐等软件的一些素材，生成对应的图文，便于我们查看回顾，或者给他人，你可以自己编辑最终的文案风格、照片排序等，选择自己的样式。 我比较理工直男一些，不太发朋友圈，但是对于热爱生活本就很的人而言，我想这会是一个令人更加开心的小功能。想一想，经历短暂的特种兵之旅，身体有些疲惫，马上生活就要开始忙碌起来，但是又很想一下自己的快乐，直接从小艺时光机整理的内容里找灵感，既重温了短暂的欢乐时光，又可以快速便捷地给朋友们，非常温暖了。 超能干的小艺聚合的功能远不止这些，其他的手机品牌或者社交平台也在做类似的功能，限于篇幅难以全部提及，虽然不同的人有不同的适用产品，真正的核心还是人自身。 经过几年的发展，AI的浪潮已经势不可挡，我希望大家不是抗拒而是大胆拥抱，合理利用AI好的地方，去辅助自己工作学习提高效率，去给自己带来情绪价值生活更顺心，这也是AI对于人类的价值。越早用，越好用，希望大家都能利用小艺等AI工具创造出更美好的未来。 送礼物 还没有人送礼物，鼓励一下,桔了个仔​ 新知答主 话题收录 超能小艺一用就爱 40 人赞同了该回答 现在的AI应用，基本都是基于大模型的，而大模型，又是基于全互联网海量的语料进行训练的，在知识储备方面，确实远超人类，也不会健忘。我的日常工作与生活，也让AI扮演起助理的角色，在各方面为我提供帮助，例如查问题，写代码等。 然而，在使用很多AI应用过程中，我却感觉尽管许多AI应用能够承担助理的职责，可在很多时候，却难以做到真正的贴心，TA们确实很聪明，但总感觉缺乏对使用者情感需求以及复杂情境的深度理解与把握。 到底AI应用要如何做，才能让人感觉到贴心呢？ 贴心助理需要什么特质 其实答案就在谜面上——贴心，也就是「贴近你的心」，这样才能懂用户。贴心的助理，意味着TA能了解你的知识领域，贴近你的记忆；同时，还能根据你的记忆，生成洞察。 最贴近你的心的设备是啥？没错， 是手机，因此，手机也是各大厂商的主要战场之一。手机AI应用也是很多手机厂家和AI公司在做的事情。然而，很多第三方的AI应用，总感觉和系统结合不够紧密，虽然他们确实采用最先进的模型，功能也很强大，但总感觉不够懂自己，使用这些应用的感受，就像向不认识的厉害专家请教。 不过我用过很多很多手机的AI应用，我发现里的，能给到我这种贴心的感觉。小艺是鸿蒙系统里的个人助理，TA里面有各种各样的智能体，功能可谓相当丰富。从工作和生活角度，我挑选了个人最的几个智能体。 小艺知识库，助力我在职场如虎添翼 大家都知道，我是一位算法工程师，我工作的一部分，是要跟踪AI行业的热点，了解行业情况，跟进技术进展。我加了很多AI相关的群，里面很多行业资料，我还没空看，不过这些群里面，各种各样格式的文件都有，PDF、word、pptx，乃至图片，确实是很宝贵的资料，但，实在看不过来啊。另外，我还参加了一些会议，录了一些会议音频，也很少会再听。 要是这些格式各异宝贵的资料，都能丢到同一个知识库，让AI帮我生成洞察，那就相当的nice。而小艺知识库就能实现我这个愿望。 这不，领导刚给我发来一个pdf，还有一段录音，让我学习下。但我手头工作还没完成呢，我直接扔到小艺知识库，让TA帮我忙。其实很容易给小艺知识库传知识，可以通过拖拽记、记、本机上传等方式，例如我可以通过记，把老板发我的录音加到知识库里。 录音还没听呢，领导又发来一个pdf，让我看看。没事，也直接丢进小艺知识库。 我甚至还可以直接拖拽给小艺，让TA添加到小艺知识库。 00:25 可以看到，使用起来还是很方便的。现在，我的资料类型里有PDF，word，pptx，图片，还有一些开会的录音。 同时，好的知识能生成AI智能摘要，我可离线随时回顾。 小艺会根据我上传的内容，自动创建智能合集，帮助我回顾和管理自己的个人资产，提升信息利用效率。 我的小艺知识库里，目前收集了八十多篇AI行业研报，TA就像我的私人顾问，随时给我提供前沿技术问答服务。我可以对这个知识库进行提问，例如我可以让小艺帮我在知识库里找到关于AI市场情况的洞察。 也可以让TA回答「最近两年AI视频和AI生图方面都有哪些优秀公司」 有时候开车时，我会把知识库转成播客，让我利用通勤时间听听。好玩的是，和普通播客不同，小艺生成的播客还是能够互动的，我随时可向主播提问。 01:12 总的来说，小艺知识库是一个非常实用的智能体，TA躺在我的手机里，我每次给TA转发资料，TA就对我的领域了解更多一些。闲暇时间和碎片化时间玩小艺知识库，有趣又有益，能让我利用碎片时间提升自己对行业前沿的了解。 小艺深度研究 知识库可以让我从更广的范围找到工作所需的信息，而小艺深度研究功能，则可以帮我针对一个领域深入研究。 大家都知道，我最近在研究AI视频领域，例如我之前写了一篇文章来讲讲AI视频领域的技术。 不过还没完全概括所有关于AI视频领域的技术。随着AI的发展，最近也出现了一些新技术，那我让小艺写一份关于AI视频用到哪些技术栈的深度研究报告吧。 TA收到课题后，先和我确认方向，然后再继续研究。 TA会深度分析问题，整理信息，最后再撰写报告，大概需要15-25分钟。 回头再看，好了，小艺深度研究的结果出来了。 从这个摘要就能看出，小艺深度研究应该是研究得够深入了。小艺深度研究给我返回了3.5万字的报告。 看了下目录，基本都覆盖到业内常用的一些技术，甚至还提到了参数高效微调（PEFT）这种我之前还没考虑到的技术，让我有了新的研究方向。 可以看到，小艺深度研究能够帮助我更精通我的专业领域，帮助我的研究工作更深入。 小艺时光机：让P人找到生活中的小确幸 我也经常在知乎吐槽自己是p人，做事缺乏条理性，而p人的数码生活状态，就是照片拍了一堆没有怎么看过，很少做回顾。 其实我很羡慕朋友圈里那些认真记录生活的人，但是我经常出去玩回来就累得倒头就睡，想起来整理照片时，已经是几天甚至几个月后了。 而小艺时光机能够自动记录美好生活。在TA的帮助下，我也找回了一些差点被我忘记的幸福时光。小艺时光机能够自动整理用户日常的运动、拍照、听歌、去过的地方、通话记录等信息，为用户推荐生活里的精彩时刻。 例如打开时光轴功能，就能看到小艺时光机帮我整理的美好时光。例如我去旅游，逛了几个城市，玩了几个地方，TA自动帮我记录我去过的地方。 比起其他AI工具，小艺时光机作为鸿蒙系统的AI应用，能联动的数据会更多，例如日历和日程等。 有时候出去玩得太累了，躺在床上脑子转不过来，发朋友圈想不到文案，通过小艺时光机，帮我想文案。例如，我8月份去冲绳岛旅游拍了好多照片，但回来后忘记整理了，不过小艺时光机给了我手记建议。 并且通过AI大模型的能力自动生成游记或者文案。 你也可以把游记保存下来，然后到朋友圈 通过小艺时光机，让我这种p人找回了一些被遗忘的小确幸，提升了生活的幸福感。 小艺修图智能体 另外一个提升我生活幸福感的功能，是小艺修图智能体。 前段时间周末出去溜达时，拍了一些图片， 有的挺好，但是有的照片有点遗憾，主体很好，但环境不对，感觉哪里不对，天空有点灰。例如这一张，拍摄于东莞松山湖华为欧震小镇。难得来一次，却天公不作美，天气阴蒙蒙，不怎么出片。 有没有办法救一下这个图呢？我试了下手动调色，怎么都不对劲，把天空调亮了，主体就变色了。我只是想在不改变主体的情况下，稍微美化下环境，怎么才能做到呢？ 还好有小艺修图帮忙。例如我把上图扔给小艺修图，并说「帮我把天空变成蓝天白云，再把湖面的水调绿一点」 可以看到，TA先把天空替换成蓝天白云。再调整了天空后，下一步，TA再修湖水颜色。 可以看到，小艺修图，不仅仅是一个AI绘图模型，TA是一个智能体，TA能够规划步骤来修图。调整好后效果如下。 不错吧，小艺修图可谓废片拯救器，让出片轻而易举。当然，小艺修图还有其他用法，我甚至看到有人用小艺修图做出了自己和明星的合照，很好玩。 刚好这两天在北京，参加了北京线下的学术酒吧活动，主题是「与超能小艺的微醺夜」。 见到了很多老朋友和新朋友，惊喜的是，这次主持人竟然是大家很熟悉的陈铭老师。 在这次学术酒吧中，嘉宾们提到很多有意思的经历和观点，十年新知答主@Cecilia老师了闺蜜和小艺聊天让超能小艺提供情绪价值@甜草莓 和@王元卓 博士也利用小艺作为第二大脑，利用小艺知识库的能力，帮忙快速阅读大量文献，帅气的人工智能学者@高庆一 老师，也使用小艺深度研究，只用一杯咖啡时间，就完成他过去一周才能完成的栏目策划。 最后回到最开始的问题。在 AI 时代，每个人都能真正长期拥有一个优秀的贴心助理。其实这也是超能小艺一直在做的事。TA不仅超能干，TA以「终端设备第一」为战略，扎根在大家最常用的设备里，让帮助触手可及。所以，AI助理怎么才能让人觉得贴心？我觉得可以从小艺身上得到启发。 第一，TA不用你额外寻找，就在你随手能触及的设备中，能第一时间响应需求。就像小艺知识库和小艺深度研究，随时提供最快的帮助。 第二，TA不只是被动等着你的提问，更会主动替你操心。例如帮你记录生活点滴的小艺时光机，帮你拯救废片的小艺修图智能体，能够从被动响应变成了主动服务。 可以说，越早用小艺越早拥抱 AI 时代。 AI的车轮依然在滚滚向前，未来AI会越来越强大，每个人都有机会AI加持下，成为超级个体。祝愿大家在AI时代，在AI加持下，能力大爆发，生活更幸福。,"依然食光袅袅​ 咨询行业 总经理 89 人赞同了该回答 自从看过电影《超人》，我就时常幻想： 要是也有个过目不忘、学识渊博，还能读懂我生活，随时帮我打点一切的‘超人助理’，那该有多好？ 没想到，随着AI技术深度落地，科幻竟成了现实。我找到了我的“超人助理”——""超能小艺""。 曾听朋友抱怨： AI再强，不贴心不也白搭？ 而超能小艺之所以让我惊喜，正是因为它不仅回答问题，更懂得“陪伴”与“理解”。 它悄无声息地融入我的日常，成了我工作、生活、记忆与创造的延伸。 一、小艺时光机：自动打点我的生活，连接远方的思念 照片、定位、运动记录、听歌喜好……这些生活碎片，总是散落在各处。在获得我的授权后，小艺时光机就像一位隐形的“生活剪辑师”，将这些碎片自动整理，串联成温暖的故事。 时光机将我的日常精彩瞬间，自动整理成册后，一键给国外的女儿，让她实时感受我的生活气息，仿佛我从未走远，也缓解了她对我的思念。 旅行归来，我再也不用手忙脚乱地拼凑九宫格。小艺时光机已经根据行程照片、景点定位，甚至是当时的天气情况，自动生成了时光线索，还能根据选择的图片生成文案，我只要轻松修饰，就可以一键了。 这样的科技，让我的回忆充满了温情与仪式感。 二、小艺知识库：我的私人“智囊团”，让知识随取随用 从前，我的夹里堆满了在“吃灰”的文章，资料散落在微信、浏览器各个角落。 自从有了小艺知识库，它总能把这些知识，打理得井井有条。 一站式：无论是网页、图片还是PDF，通过“拖拽记”等神奇操作，都能瞬间归入我的知识库。 AI智能摘要：的长文自动生成摘要，通勤、排队时就能快速回顾，即使离线也不耽误。 智能合集：自动将我的分门别类，形成旅行攻略、学习笔记等主题合集，随用随取。 AI问答与播客：我可以直接向自己的知识库提问，获取最精准的答案。 更妙的是，它能将枯燥的资料，转化成生动的AI播客。 我边做烘焙边听，还能随时与“播客主持人”互动问答，让碎片时间更有价值感。 三、小艺修图：对话式修图，“手残党”的福音 作为曾经的“手残党”，学习复杂修图软件时，常常让我一个头两个大。现在，小艺修图智能体简直是我的救星。 修图只要动动嘴： 面部提亮一点，再立体些； 把背景换成青岛的沙滩…… 它总能精准理解并瞬间完成。如果效果不满意，还能持续对话微调。 原本复杂的精修，现在片刻就能得到有质感的大片，让我忍不住翘起嘴角。 四、小艺深度研究：我的专业助理，应对复杂任务 当我需要处理像撰写商业计划书、调研行业趋势这类深度任务时，小艺便能化身成我的“专业研究助理”。 它能进行深度搜索与分析推理，从海量信息中梳理脉络，最终帮我生成结构清晰的研究报告或PPT，为我节省了大量搜集与梳理的时间。 综上：拥抱AI时代，就从一个贴心的“超人助理”开始吧。 超能小艺超能干，这可不是一句口号。 它让我在时光机里回顾温情，在知识库中随取随用，用对话轻松修图，更能以深度研究应对挑战。 它让我真切感受到，终端设备的智能体验，变成了触手可及的现实。 越早用小艺，越早拥抱AI时代。 这个属于每个人的优秀贴心助理，正在这里，等你开启。 送礼物 还没有人送礼物，鼓励一下"
如何看待荣耀发布「ROBOT PHONE」？AI 机器人与影像融合，会是手机终极形态吗？,10 月 15 日，荣耀宣布发布面向未来的 AI 终端生态新品荣耀 ROBOT PHONE ，它具备 AI 的超强大脑，能随时随地洞悉万物；具备机器人的超强行动力，陪用户共享生活点滴；它更能化身用户的专属摄影机，敏锐捕捉每一个有爱的珍贵瞬间。探索永无止境，荣耀不断探索科技与人文结合的可能性，此款新品将于 2026 年在巴塞罗那正式亮相。 在未来，荣耀 ROBOT PHONE 等前沿 AI 终端生态产品将抢先在荣耀阿尔法全球旗舰店推出及上手体验。,学写作的丧失​ 华为手机等 2 个话题下的优秀答主 286 人赞同了该回答 荣耀在这个时间节点发布ROBOT PHONE，对于业界来说挺有意思的，甚至可以说挺有启发性的。 因为最近美股AI概念合纵连横的事情大家都注意到了，山姆奥特曼也到处拜访硬件代工厂，引发了业界丰富的联想。 继特斯拉链，英伟达链之后，现在OpenAI链也逐渐成型了。 OpenAI也要做硬件了？一个面向未来的AI大模型硬件是怎样的？AI硬件的未来是怎样的，AI硬件有范式吗？ 其实从去年到现在，二级市场一直预期AR眼镜是一个AI大模型落地的场景，因为AR眼镜可以搭载摄像头，通过反射可以搭载一块小屏幕，眼镜腿的部分可以把主板电池这些较重的部分做进去。 但AR眼镜其实并不性感，智能眼镜是一个讲了很久的故事，具体产品的市场反应也不好，后续要想有大的发展，还是要有更显性的，更容易让用户感知的卖点。 于是整个AI行业的想象力又僵在这里了，大家都不知道要怎么办。 这个时候，荣耀ROBOT PHONE的发布，再次引爆了大家对AI硬件的想象。 从发布会上的视频来看，荣耀ROBOT PHONE有一个巨大的后置相机模组，相比目前我们看到过的任何影像旗舰都要更加巨大。 但这一次相机模组的大，不是为了塞进去更大的传感器，而是为了将一个精密的机械结构做到荣耀ROBOT PHONE里面。 没错，荣耀ROBOT PHONE在手机上做了一个云台相机进去。 在短片中，荣耀ROBOT PHONE的云台相机每次移动的时候，都会发出充满好奇心的孩童的哇喔的声音，也是想传达出，AI第一次在荣耀手机上拥有了眼睛，好奇地观察这个世界的感觉。 之前手机上其实也不是没有连续的视觉识别，相信大家都用过一些实时AI识别的软件工具，都是利用视觉大模型现在强大的图像理解能力，比如可以帮助用户看看是什么花，出去旅游可以看是什么建筑物，科普历史知识。这些都是目前很常见的应用场景。 但是和大部分人的认知不一样的是，视觉大模型其实已经非常强大了，现在很多农业已经在利用视觉大模型去做诊断，比如这株作物生病了，农民可以通过视觉大模型去看，大模型分析之后会告诉农民可能是什么问题，可以用什么手段去解决问题。 包括汽车行业的VLA技术，现在也炒的很火热，其实就是通过视觉摄像头拍摄的视频片段，交给大模型去理解，大模型会理解画面中车辆行人和道路交通标志的关系，然后给智驾的司机部分作为参考。 但以上的使用场景都有一个共同点，那就是大模型看到的东西，都是由人类来决定的，视觉的焦点并不是AI来决定的。 而荣耀ROBOT PHONE的与众不同之处在于，它的一个典型场景就是把荣耀ROBOT PHONE放在口袋里，云台摄像头会根据AI的理解，自己左看右看，主动去理解周边的环境。 这也就是荣耀讲述的三位一体概念。 首先荣耀ROBOT PHONE是一部手机，它有强大独立的端侧算力和存储，可以在端侧本地跑AI算法，相比其他任何的移动设备，手机毋庸置疑是一个非常优秀的平台。 所谓ROBOT，机器人手机，也就是指荣耀ROBOT PHONE有了云台相机，拥有自己自由的活动维度。 再结合影像之眼，云台相机本身是荣耀ROBOT PHONE的后置相机，硬件素质不用多说，比市面上绝大部分的AI硬件都要更强。 以上的三位一体，就组合成为了荣耀理解的下一次手机行业革命终极形态，从功能机到智能机再到智慧生命体的必然进化路径。 这些说起来有点玄学，我拿一个具体的场景举例，大家就明白荣耀ROBOT PHONE这种形态的必然性了。 我们再回到荣耀ROBOT PHONE挂在胸口上使用的这个场景。 把荣耀ROBOT PHONE挂在胸口上，意味着荣耀ROBOT PHONE看到的环境，和人类的自然注意力是一致的。 当然，还有一个更一致的位置，那就是挂在头顶上，但在脑袋上竖一个手机的形象还是太抽象了。 那么荣耀ROBOT PHONE的AI之眼可以看到一切人类正在做的事情，就可以参与进去指导。 比如我在炒菜，荣耀ROBOT PHONE就可以通过AI之眼了解灶台上食材在哪，调味料在哪。 我把菜放进去炒之后，荣耀ROBOT PHONE就可以看到火候如何，食材是否断生，需不需要加水。 我炒菜的时候动来动去的，荣耀ROBOT PHONE的云台就需要通过稳定器来减少震动，保证它可以一直注意应该注意的地方。 荣耀ROBOT PHONE本身有扬声器，可以通过语音来提示我操作，荣耀ROBOT PHONE也可以连接荣耀耳机，通过耳机来传达，同时避免炒菜的噪音。 而如果荣耀ROBOT PHONE要可以完成以上的操作，就需要一个足够大的内存和储存，用来记忆储存做过什么，要有一个能够形成足够上下文的内存，才可以完成这些操作。 同时用户并不自己在家里，或者在外面实时被人盯着的感觉，要想做一个一直陪伴着用户的AI终端，所有的数据储存和处理都需要在端侧，这一点几乎就淘汰了所有小型设备。 所以虽然荣耀ROBOT PHONE一些场景和运动相机类似，但运动相机绝对不可能是下一代AI设备。因为小型设备的电池首先就经不起消耗，更别提AI视觉设备需要比传统智能手机更强的算力、存储、散热、电池等等，一切你想得到的所有配置，都要做到顶配。 未来AI终端的最小型单位，可能就是荣耀ROBOT PHONE起步了。 可以说荣耀ROBOT PHONE构筑了一个AI终端的基本形态，一个便携的端侧AI的最基本单位就是这样了。 当然，要做的更小也可以，那就不可能是端侧AI，需要实时联网。 然而一个随身的AI终端，要实时联网挺难的，因为数据上传需要稳定的上行网络，室内和楼宇多的地方，不太好保证。 另一个问题是，如果用户要随身携带一个AI终端，大量场景都是自己的隐私生活，用户很难接受云端AI处理自己的隐私信息。 所以荣耀ROBOT PHONE的产品形态是经过深思熟虑的。 目前对荣耀ROBOT PHONE这种云台可以主动运镜，AI主动观察四周的形态，其实有一个现成的生产力场景。 现在有很多运动相机，云台相机，主要就是做小型化，方便用户拍摄。 主要受众就是社交平台的vlog用户。 但实际上我们平时很难形成一直拍摄的习惯，做vlog博主是一个非常重体力的，实时都在拍摄，思考什么值得拍摄的过程。 所以很多vlog博主拍几年之后都退出了，因为长期大量的日常拍摄，导致他们的旅游毫无乐趣，全程都在摆拍，日常记录也一直都在表演，感觉自己作为人类的存在性被职业否定了。 但我们看荣耀ROBOT PHONE这种产品形态，AI实时通过AI之眼观察周围的环境，其实AI可以自己判断这个场景是否有拍摄价值，自动把这个片段拍下来，在后台进行剪辑构图。 这样的话，用户在日常生活，出门旅游的时候，就不需要多花精力拍摄记录了，AI之眼会全程自动记录拍摄。 很多时候我们旅游的时候看到值得记录的瞬间，其实都在用肉眼看，想不起来去记录，这时候荣耀ROBOT PHONE就可以帮我们记录。 到了中午吃饭，或者晚上休息的时候，用户就可以自己直接选现成的素材进行社交发布就好了。 在宣传片我们也看到了，用荣耀ROBOT PHONE记录下婴儿的笑容，爱人回眸的瞬间，一个主动记录你生活瞬间的设备，可以帮你创造大量自然的生活片段。 这时候其实就会产生一个生产链条本质的变化。 就像大语言模型的出现一样。 以前你做一个文字博主，你需要同时具备写作能力和思考能力。 而大语言模型可以产出高质量的内容论述，那么一个文字博主就只需要具备思考能力就可以了。 写作这件事情的门槛就大大降低了。 同样的道理，荣耀ROBOT PHONE的产品形态下，每个人都可以享受AI自动拍摄剪辑的帮助，相当于每天都有一个摄影师跟拍，外加一个剪辑师帮你干活儿。 如果要做一个视频博主的门槛是：拍摄+剪辑+创意。 那么荣耀ROBOT PHONE就是帮你做了拍摄+剪辑，用户只需要有创意就可以了。 我们从这个角度来说，荣耀ROBOT PHONE作为机器人手机，或者说未来AI终端的产品形态，其实讨论了一个AI行业的终极命题。 那就是一个真正能跑通的，能进入消费市场的，能让用户爱用常用的AI终端，一定是通过AI的能力从事生产的，对具体的现实世界产生帮助的设备。 所以我们最后回到文初聊的，山姆奥特曼满世界拜访硬件厂商，就是想给OpenAI寻找一个硬件的AI载体。 而荣耀作为一个中国的手机厂商，直接就可以坐享中国的硬件产业链红利，领先行业发布荣耀ROBOT PHONE这样的创新形态。 就在2026年，荣耀ROBOT PHONE就会在巴塞罗那MWC上正式亮相，同时登陆荣耀阿尔法全球旗舰店，到时候可以邀请山姆奥特曼来看看。 未来市场上还会出现很多AI终端，大家都会在上面看到荣耀ROBOT PHONE的影子。 说不定后面还能看到市场热炒荣耀机器人供应链的新闻呢。 送礼物 还没有人送礼物，鼓励一下,松果煲粥​ 智能产品深度测评｜哪有完美的产品，人类科技就这样 174 人赞同了该回答 昨天看荣耀的发布会看到最后，原本以为发布会要结束了，结果突然就宣布了这台神奇的机器。荣耀 ROBOT PHONE出场的时候我愣了一下，然后就开始疯狂截屏...一边截屏一边在心里喊，这到底是什么东西呀！！！ 当时数码群里都在讨论说荣耀发布了一台机载云台相机，要进军云台相机的市场。 我还觉得这确实很有搞头，毕竟云台相机需求现在确实也不少，那如果打算买一台云台相机的话，真不如直接手机自带了，总比单独拿一台相机要方便得多。 而直到看到荣耀官方发的微博后我才反应过来，这不是传统意义上的AI手机，这颗云台相机是专为AI功能而设计的。荣耀的目标根本就不是云台相机这个当下的市场，而是在做目前最接近未来的主动AI智能设备。 对于AI，可能大家心里最终希望的目标都是电影中那种类似于“贾维斯”的AI智能管家，不光可以非常智能的处理工作，并且更重要的是具有主动性。 像是电影里每当钢铁侠自己受了重伤，贾维斯就跳出来说已开启智能接管，然后一通神操作带钢铁侠脱离险境，这给人满满的安全感。 实际上我们距离电影中这种AI已经非常接近了，如今的大模型AI已经具备了很高的智能工作能力。它可以自行分析处理当前工作需要调用哪些模块以什么方式进行处理，并且给到完整的处理流程。 只是在实际的应用上目前我们还没有给AI很高的硬件操控权限，但是也可以看得出来AI自动化工作流程的基本技术都已经完备了，就差逐步搭载了。 就比如说荣耀宣布的“YOYO许愿池”计划，就是要逐步的把实际需求的应用能力下放给YOYO，让YOYO慢慢具有更多的实际操作能力。 但是大家有没有发现，我们的AI技术走到这一步，看似已经非常完善了，但是距离想象中那样的AI管家，还有着一个本质的区别。 那就是缺乏主动性。 目前的大模型AI再厉害，那也是我们自行向AI提出需求，然后再让AI做出解决方案。 虽然AI的解决能力确实很强了，但是任何动作都需要我们主动的去向AI提出。用我领导的话来说，就是缺乏主动寻找工作的能力。 咳咳，虽然这话放我自己身上我要狠狠的吐槽，但是放在AI上我觉得很合适，AI就是要给我狠狠的当牛马的。 难道我没有提出来需求你就不做吗？ 而荣耀 ROBOT PHONE这颗灵活的云台摄像头，就是改变这一切的关键。 要让AI具有主动的工作能力，最核心的一点就是改变我们和AI的交互方式。 现在我们之所以只能通过文字或者语音收入来向AI传达指令，是因为AI只能通过我们的语音来获知目前的外界情况和需求。 如果我们给他拍一个照片或者视频发给AI，然后询问AI你能猜到我现在的需求吗，其实AI往往可以给到非常精准，甚至会比较出人意料的答案。 也就是说现在的AI其实已经具备了对需求的实时分析能力，以及上面提到的对需求的解决能力，而唯一欠缺的其实就是对环境信息的获取。 简单来说就是AI的能力已经基本达标了，但是信息的获取效率太慢，那么信息密度获取效率最高的方式是什么，自然就是视觉影像。 所以一台完整的AI设备，必然要具备”AI计算能力(大脑）+机械硬件（身体）+自动化视觉传感器（眼睛）“三位一体的，这三条元素是未来所有AI智能设备都必须要具备的。 而荣耀 ROBOT PHONE就是我们目前见到的第一台未来化的具有主动AI能力的智能产品。 虽然它看起来小巧，但是大脑、机械结构、视觉传感器三大要素全部齐全，这是一台真正的人工智能机器人的雏形。 那这样一台真正的主动交互能力的AI设备能够具有哪些功能。 其实我觉得现在讲它的功能，基本只能挖掘到它最基本的浅层能力，因为AI的能力一定是要在实际使用中去发现的。AI就像一个宝库，人类打开了宝库的大门，但是能拿出来什么东西还是要靠深入的探索。 这就是荣耀为什么要做“YOYO许愿池”计划的原因，说白了就是集思广益，把广大用户发现的AI创意收集起来，然后作为AI的功能让AI去实现。 所以对于AI来说，实现功能反而不难，甚至你可以直接问他如何才能让它实现某某功能，它可以自己给你出方案。但是难的是如何挖掘这些需求点，只靠几个工程师在办公室里开脑洞去想那肯定不现实。 就连现在的语言对话版本的YOYO都需要这么做，那像荣耀 ROBOT PHONE这样一台更加未来化的产品，它的最终使用场景肯定也是要在产品发布之后随着用户们的使用去挖掘的。 而我们现在可以先讨论一下它最基础的功能，比如说智能拍照。 既然它搭载了一个非常灵活的云台摄像头，那么配合AI能力首先激活的就是智能拍照能力。 相比于传统的云台相机，荣耀 ROBOT PHONE的这颗镜头必然会具备智能构图、主动抓拍、自动记录的多种AI能力。 如果我们想要记录vlog，那么它可以自动的跟踪记录人像，自动拍摄环境空镜镜头，自动智能分析后精准记录亮点时刻，最后还可以通过AI剪辑自动帮我们剪成成片。 额，比较值得一提的是，其中最后一步，AI剪辑，你现在就可以在荣耀手机上使用了，这些软件层面的AI功能荣耀基本上早就已经拉满了。 而如果我们需要拍照，也可以让它自动构图，然后自动帮我们拍摄。 想象一下，你陪着女朋友出去玩了一天，期间完全不用考虑什么机位呀构图呀出片呀啥的，晚上回到家之后，打开手机，荣耀 ROBOT PHONE已经帮你女朋友把九宫格照片和朋友圈文案都准备好了。 “妈妈，人生是旷野” 而具备智能能力的AI视觉，自然还可以配合AI大模型实现主动发现需求并解决需求的能力，比如自动帮你记录工作和日程，自动分析周围的场景、店铺和文字内容等等。 比如说“YOYO，我的车钥匙给放哪了。。” 这是一个真正的AI智能管家，或者说是一个AI智能伙伴。 它的具体应用场景是需要我们在使用中挖掘的，甚至这样一个存在本身就是需要我们去习惯和接纳的。 如今我们见到的是一个手持的荣耀 ROBOT PHONE，那未来会不会还有可以自动行动的大模型AI机械狗甚至真正的人工智能机器人，这我们不得而知。 荣耀为我们打开的是一个非常广阔的AI世界。 对于AI的应用，荣耀一直走在市场的前列。 实际上在大模型技术面世之前，荣耀就已经在手机上搭载了基于深度学习的对用户习惯的学习能力，也就是早期的YOYO助手。 在世界上还没有大模型之前，YOYO就已经可以调用地理位置信息结合用户的使用习惯，智能的实现在地铁站弹出乘车码，在快递柜前提示取件码等等功能了。 注意这些功能都出现在世上还没有大模型技术时，而这些显然就是大模型技术的雏形，只不过学习的信息量没有达到出现涌现的程度。 可以说AI大模型是科技发展的必然结果，只不过是先后的区别。 而如今的荣耀 ROBOT PHONE，荣耀显然已经走在了AI发展的最前沿，荣耀的优势就在于不光有软件能力，同时还有完善的硬件实力。 荣耀 ROBOT PHONE这样的未来化智能产品，必然是需要软硬件结合的，没有那颗灵活的云台摄像头，以及端侧的计算性能，就无法实现这样的产品。 而荣耀可能是目前走在AI前沿的公司中唯一二者皆备的。 荣耀可能并不是AI技术最强的企业，但是荣耀一定是AI企业中硬件能力最强的。。 最后，荣耀 ROBOT PHONE就是未来吗？ 我可以说荣耀 ROBOT PHONE具备未来AI智能设备的所有基础要素，也就是”AI计算能力(大脑）+机械硬件（身体）+自动化视觉传感器（眼睛）“ 这同样的三要素，可以组成荣耀 ROBOT PHONE，也可以组成一只机械狗或者一个真正的机器人，也或许是那种桌面萌宠之类的。 到底哪一个才是真正的未来，或许未来是多设备并进的多元化的，我们现在并不能下结论。 但是可以肯定的是，荣耀 ROBOT PHONE的方向绝对是正确的，也是目前的技术体系下能够拿出的最成熟的成品。 目前荣耀仅放出了一个先行展示视频，正式的产品亮相会在明年2026年的MWC上发布，并且会登录荣耀阿尔法全球旗舰店。 就是不知道是仅展示的实验机型，还是可以购买的。我觉得哪怕价格贵一点，也总比单独花几千块买一台云台相机要合适，更何况还有强大的AI能力加持。 送礼物 1 人已送礼物,Puddle​ 手机话题下的优秀答主 105 人赞同了该回答 其实，制约手机形态的，从来都不是技术基础，而是想象力。 而ROBOT PHONE已经仿佛让我看到了未来的样子。 技术缺陷不可怕，“想象力”匮乏才可怕 作为一名数码圈老登，十多年里看过的发布会早已经超过百场，但是我越来越感觉，我不是看了一百场发布会，而是把同一场发布会看了100多遍。 除了最早2007年iPhone发布和中间折叠屏发布外，印象中，就很难有那种让我毛孔舒张、浑身战栗的兴奋感。 虽然每年都会有所谓的新技术，虽然充电、影像、显示技术都有提升，但整个手机行业，仍然是围绕着乔布斯最初“一台手机，一支 iPod，一个互联网通讯设备”的设想来开展。 哪怕是全面屏本身，也仍然是围绕着“直板大屏”在做文章，其核心的交互逻辑仍然停留在古早的“触摸+滑动+点按+语音”上。 最近两年来，AI大模型的发展，让手机语音助手的能力有了升维式的提升——助手不再那么智障，但是仍然没有改变语音助手“只会对话、不会行动”的困境。 相比较过去，现在的手机影像、续航、性能、耐用性提升了数倍不止，但是，现在的这些手机所给用户带来的震撼，远远没有古早iPhone那种颠覆性的融合形态改变带给人的震撼大。 如果要说技术限制，初代iPhone 2G网络、续航半天的电池和可能随时损坏的HOME键，其技术未必真的比当时的诺基亚强上多少。 但是，如何将现有的技术，以一个更具想象力的方式展现出来，才是现在手机市场面临最大的难题。 简单来说，没有想象力的“鸡”，就没有后面技术推动攻坚、不断沿袭进步的“蛋”。 一部有想象力的机器，应该是什么样子？ 苹果当年的One More Thing，或许是众多数码爱好者心中的白月光，在乔布斯喊出这句话的时候，就意味着代表当时那个时代最高科技水准与想象力的产品要出来了。 不过近年来，这个One More Thing多少有点被国产厂商滥用的嫌疑，一个平板一块手表甚至随意什么AIOT设备，都能被当作One More Thing来宣传。 所以，当荣耀喊出来One More Thing的时候，我内心是毫无波澜，甚至有点想笑——没想到荣耀这浓眉大眼的也开始搞这出了。 但是当荣耀放出来第二张PPT的时候，我就已经笑不出来了。 正如当年乔布斯喊出“一台手机，一支 iPod，一个互联网通讯设备”，荣耀李健宣布，荣耀今天要发布“一个机器人、一个高清摄像头和一个AI手机”。 而这三台设备，有一个共同的名字，叫作ROBOT PHONE。 是机器人，也是手机。 都说AI的下一步是具身智能，但是想象力匮乏的我，真的没有想到这个下一步，竟然会是在手机上首度实现。 这是一款外观上无限接近现有智能机的智能终端，但是在折叠变形以后，它的摄像头可以变身成一个自带机械臂和云台的高清摄像头。 在AI端侧能力的加持下，可以变成我们的私人摄影师，自动寻找最好的角度、自动调节焦距，自动帮我们去记录美景、拍摄合影照片。 也可以在灵活的摄像头辅助下，捕捉更多的信息，成为我们的私人生活助理，像一个育儿机器人一样去“摇头”唱摇篮曲。 也可以为我们的私人穿搭提供更多的专业建议。 而其本身的可变形的物理结构，也为其互联互通提供了更多的可能性。 在概念展示片的最后，ROBOT PHONE的摄像头望向了正在挥手的一个机器人，或许这也代表着新的ROBOT PHONE的摄像头具备着主动观察、主动寻找设备、主动沟通、主动交互的能力。 这也意味着，ROBOT PHONE能做的，会比我们想象中更多。 现有的AI大模型最受人诟病的一点，就是其自身大多只能进行文字的问答，而没有办法像想象中一样帮助用户去处理生活中的复杂事务。 比如，用户想要用AI自主帮助自己定一杯咖啡——传统的AI语言助手，只能帮助你打开外卖软件，后续的操作都需要用户自行操作。 而我们理想中的AI应该是这样子——AI乐意自主确定用户自身的位置、了解用户过往的口味、选择合适的外卖软件、领取优惠券并完成下单等操作，用户只需要打开手机扫脸付一下款。 而这，就是AI Agent智能代理与普通AI的最大区别，AI Agent可以自主理解用户意图，并帮助用户完成大多数操作。 在此前的Magic 7系列上，荣耀已经在平板机上实现了此类AI Agent全流程的操作。 但是，如果有ROBOT PHONE这样的“外挂级”摄像头以后，或许，我们连打开手机的操作都不再需要。 我们只需要对手机说出我们的需求，摄像头就会自动弹出，并且通过识别后的身份信息，自主帮我们完成后续包括付款的所有操作。 简单来说，如果说，Magic 8系列的AI体验是赋予手机观察手机屏幕内的能力，那么ROBOT PHONE的折叠变形结构，就是给予了手机观察手机屏幕外世界的能力。 如果我们再放开一下想象力，在某些情境下，ROBOT PHONE真的有成为“私人管家”的能力。 比如，过去我们想遥控扫地机器人，可能需要我们用户主动观察家里的脏乱程度，并且在APP上进行操作，但是ROBOT PHONE就可以借助摄像头自主观察家里环境，并自动设定清扫频率。 再或者，ROBOT PHONE也可以与智能监测设备相联动，通过智能穿戴设备的数据，以及观察用户的健康状态，实现对用户异常状态的精准识别，并在用户可能陷入危急需救援状态时，实现智能的主动呼救。 在真正的具身智能出现以前，ROBOT PHONE或许是我们最接近电影中“贾维斯”形象的设备。 不是概念机，而是来时路 ROBOT PHONE目前的定位是概念机，但是明年就会在巴塞罗那正式亮相，未来，这部 AI 终端还会在荣耀阿尔法全球旗舰店提供实机展示与上手体验。 至少可以从侧面得知，荣耀这次推出的绝对不是半成品。 细细说来，ROBOT PHONE展现的未来虽然让人震惊，但是细细想来，如果这是荣耀做的，又会让人觉得合理。 早在Deepseek爆火以前的2024年初，荣耀就率先提出了平台级AI与意图识别人机交互的理念。 在2024年10月发布的Magic 7系列上，荣耀就已经实现了AI 点咖啡的AI Agent（智能体）操作。 在2024年底的一次访谈中，荣耀就透露了其AI已经可以做到通过记录特定用户的特殊使用习惯，比如手抖、颜色偏好等，来帮助用户进行相片的后期调节。 甚至，其正在考虑研发影像智能体，让手机本身就可以自主去构图、去调节参数并完成拍摄。 再到在近期的Magic 8系列发布会上，荣耀发布了自研的VLA大模型MagicGUI——可以专门用于处理多模态信息（比如视觉信息、文字信息）输入，解释复杂指令并在现实世界中执行相应动作。 而这所有的一切串联起来，就构成了现如今ROBOT PHONE的核心功能——智能影像、意图识别、观察世界以及主动服务。 是一路以来在AI技术上的深研，才有了现如今的ROBOT PHONE。 最后再聊两句 在当下这个疯狂内卷参数的年代，ROBOT PHONE的发布，或许是手机智能终端发展中的一个重要节点。 在各家都在追求更大屏幕、更多摄像头的时候，ROBOT PHONE提出了一种可能——一种不拘泥于手机固有直板形态的可能。 手机的未来，未必是一块玻璃板，也有可能，是真正的变形金刚。,通信监理​ PMP 项目管理专业人士资格证持证人 60 人赞同了该回答 智能手机之后下一个形态的个人智能终端会是什么？这是一个自iPhone为代表的智能手机诞生以来，大家一直都在畅想的话题。这些年业界常识性地给出过一些答案，譬如坎坷发展的XR眼镜曾经一度被寄予厚望，结果现在以通话眼镜的形态在苟着；至于全息显示终端，则依然还在幻想状态。这些现状说明智能手机离被淘汰还很远。 直到这次荣耀突破性地带来了一个新的事物：荣耀ROBOT PHONE（荣耀机器人手机），让我们有一种豁然开朗的感觉——下一代个人智慧终端似乎就是长这个样？ 什么是荣耀ROBOT PHONE？ 荣耀介绍ROBOT PHONE是一种将手机与AI机器人、影像深度融合起来的一个产品。从产品形态上来看的话，它是通过在手机后盖上增加一个融合了摄像头的机器人手臂的方式，形成的一个“变种手机”： 人可以通过语音、动作与ROBOT PHONE进行交互； 而ROBOT PHONE可以通过摄像头采集信息、手机处理信息、语音以及机器人手臂与人进行回应及交互，玩手机不再是通过屏幕触控了。 是不是有一种虚拟降临现实的感觉：大家曾经是否也幻想过手机屏幕里能够伸出一只手出来？这个就是类似的情况。 这种形态的个人智慧终端有什么用？是不是噱头？ 在直板手机上加机械件并不是一件很新鲜的事情，像以前手机做过翻转摄像头、升降式摄像头，以及近两年又开始流行的摄影套装，这些都是在手机上加额外机械件，不过并没有成为主流或者成为高频应用。 荣耀ROBOT PHONE乍看之下也像是手机加了一个带机械臂的摄像头，这种不一样的东西一经推出，让人下意识的都会思考——这是不是一个噱头？能有什么用？ 先不说荣耀ROBOT PHONE上这个机械臂在所谓机器人方面的其他能力，单说这个结构，让人第一个想起来的就是这两年非常火的大疆Pocket口袋云台相机，荣耀ROBOT PHONE这种结构天然就是一种云台相机。以至于带着这款手机相当于大家随身随地携带了一台云台相机，不论是在日常生活中还是旅行途中，都能够更好的进行一些视频拍摄及记录。 相比较于一台手机加一台云台相机这种组合而言，云台相机和手机融为一体的这种设备有三大优势： 第一大优势是设备随身携带，可以随时随地的进行拍摄。智能手机取代卡片机的一大原因就是在于这种随时随地拍摄。 第二大优势则是拍摄、剪辑、发布都可以通过这一个设备进行，而不必将云台拍摄的资料传到手机上进行处理，减轻了很大的麻烦。别小看这点，这种多一个步骤以及涉及跨设备的操作，其实是经年累月的对自己使用云台相机热情的消磨。 第三大优势则在于手机能够提供强大的算力支持，以及持续丰富的各种拍摄玩法更新，一次购买投入不仅在当下能够享受强大的拍摄体验，还能够持续获得新功能和新玩法。 有了云台相机这个使用场景的加持，我觉得荣耀ROBOT PHONE就已经不算是贩卖概念的噱头事物了。 然后我们再说回机械臂的功能，我觉得荣耀是真的可以做一个把摄像头换成微型手臂的东西出来，可以帮我们干更多的事情，比如： 刷剧的时候，让它给你喂零食； 在跑步机上跑步的时候，让它给你调整跑步机的速度和坡度； 无聊的时候，和你玩一些人机对弈游戏，比如石头剪刀布、卡牌等； 工作中需要做大量签名的时候，让它帮你签，甚至于可以帮你手写一些文档资料； 看纸质书的时候，让它帮你翻页。 另外，这个机械臂结合手机本身的AI能力也很有看点，比如： AI运镜功能：新手在拍摄视频的时候一个大的难题就是运镜，荣耀ROBOT PHONE 其实可以参考荣耀Magic8首发的AI追色功能搞一个AI运镜功能，对于人们拍摄视频的体验是真的史诗级提升。 AI取景功能：荣耀Magic8上发布了一个拍照取景的自动提醒功能，以及很早之前的AI影像自动调整色彩风格参数等，荣耀ROBOT PHONE 在这个基础上其实可以再加上自动调整角度、调整焦距等自动取景，让拍照更简单智能。 AI绘画功能：像荣耀Magic8拍照方面通过AI将拍摄好的照片处理成动漫风格、手绘风格等，处理完之后可以通过这个机械臂在纸上手绘出来？这会是一个让人双重欢喜的功能。 AI引路功能：步行或者骑行导航时，机械臂给你指引方向，这点对于笔者这种分不清东南西北的路盲来说可以少走很多冤枉路。 以上这些真的是越想越让人兴奋，也越让人期待的功能。荣耀ROBOT PHONE开创了将手机在虚拟世界的能力降临现实世界的第一步，这种思维的打破，势必将会推动未来个人终端朝着融合具身智能方向发展，到一定程度后就会形成新的智能终端形态！ 当然，就目前而言势必也还是会有一些问题，比如： 机械结构件的增加，对于机身重量、厚度以及续航会是一个非常大的挑战。甚至于我觉得这东西不一定要集成在摄像头部位，放在手机顶部或者底部作为一个外接插件也不是不行。这样也减轻了手机的负担和设计复杂度。 另外就是荣耀真实能够做到什么样的程度：仅仅是独特的外接云台相机的程度，还是真的能够提供一个灵活好用的机械臂？这个完成度也是值得关注的地方。 以及造价怎么平衡？这东西不论是软硬件的研发成本，还是初期量产的成本估计都是非常高的，真正量产的价格以及量产的能力程度会是什么样？ 以上这些，估计是要等2026年巴塞罗那世界移动通信大会上揭晓了。 荣耀机器人首秀，从公司愿景到产品落地 AI有两大发展方向：一个是AI大模型，另外一个则是具身智能。落到手机行业，像本次一起推出的荣耀Magic8，它被荣耀冠上了“AiPhone”的称呼，就是AI大模型在这款手机上的应用体现；而带机械臂的荣耀ROBOT PHONE，则是融入了具身智能的能力，代表了一个新的方向。 今年5月底荣耀新CEO李健首次在国内召开产品发布会的时候，向大众宣布荣耀已经进军机器人领域，并且还介绍了荣耀在机器人领域的一些进度和实力情况。在手机行业规模萎缩、流量被新能源汽车不断抽空的情况下，笔者觉得荣耀选择机器人作为新的业务线条是一个很好的出路，同时还能够给自家的手机业务带来新的流量增持。 这次的荣耀ROBOT PHONE算是荣耀机器人能力的初亮相，乍一出现就让人感觉思路被打开，创意满满，能够支撑荣耀在这个方向上做很多事情及功能。 本来一说具身智能，我们一般想到的今年春晚上宇树机器人这种较大型、独立体的机器人，但是荣耀ROBOT PHONE的推出，我们甚至可以看到具身智能可以作为一种赋能，加持在原有的设备上，从而让人产生出设备“真的进化了”的感觉。荣耀ROBOT PHONE是对智能机器人个人终端的一个可行性很高的探索，从目前的情况来看的话，荣耀是开创了机器人手机新赛道，并领跑了手机行业在这方面的发展。 送礼物 还没有人送礼物，鼓励一下,徐夕​ 64 人赞同了该回答 昨晚上我全程看完了发布会，当看到李健在发布会的尾声科普手机的进化史并且他身后的大屏幕上显示出来“One More Thing”的时候，我就心想，荣耀不会要在最后放大招吧？ “我们不禁在想，是否有这样一部设备，它拥有AI手机的超强大脑，能随时随地洞察万物，它具备机器人的超强行动力，能陪你共享生活的麻辣鲜香，它能变身高清摄像机，并能敏锐捕捉每一个有爱的珍贵瞬间，如果这三者合一，会是一个什么样的全新物种呢？让我们拭目以待！”，当我看到这里的时候，我真是一拍大腿，我去！难不成荣耀把拥有真正智慧的手机实现了？今天晚上就发布并且上市？ 当时还挺激动，后面才知道ROBOT PHONE要在明年MWC 2026才会展示原型产品，但我对荣耀的脑洞是真的佩服啊，这才是我心中真正的智能手机的样子。 关注我的朋友都知道，我在以前的文章中表达过一个观点，那就是智能手机其实并不智能，它就是能够安装软件而已，当你有了问题之后，手机本身并不能为你解决问题，为你解决问题的是手机中安装的APP，后来，AI出现之后，AI跟手机的结合让手机拥有了“智能”，但这仍然跟我心目中真正的智能相去甚远，因为现在手机中的各种AI能力仍然需要用户主动去“调用”，用户不主动调用就相当于没有AI，虽然说手机厂商增加了AI主动工作的场景，但这只是在某些很少的场景下才有效，这种“智能”跟我希望的那种真正能够跟人交流的“智能”差得还很大。 在我小时候看过《霹雳游侠》，KITT不仅仅是一辆跑车，更是主角的伙伴，我希望的智能手机也是这样，能够成为每一个用户的“搭子”，当荣耀Magic8系列发布会上荣耀展示出来ROBOT PHONE的时候，我就知道，手机的终极形态真的被荣耀搞出来了。 “三位一体”在智能手机领域的首次应用：感知-决策-行动的完美闭环 当今的智能手机形态已经被定义了18年之久，在如此之长的时间里面，智能手机的进化史用一个简单的逻辑就能理清：更大的屏幕、更快的处理器、更久的续航时间以及更好的拍照等等，从这个意义上来说，今天的智能手机跟以前的智能手机并没有本质区别。 荣耀ROBOT PHONE不同，它是AI大脑+机器人身体+影像之眼的有机组合，它的本质是用眼睛观察事物，然后通过AI大脑做出分析，然后通过机械臂和影像之眼的配合与人进行交流，三者形成完美的“感知、决策、行动”的闭环。 我这么描述你应该能够看出与传统的智能手机有什么区别了吗？是的，这就是李健说的“全新的物种”，也就是说ROBOT PHONE是智慧生命体，而不再是一个智能工具。这就是荣耀的伟大之处：荣耀给你的不再仅仅只是一个工具，而是一个值得信任的搭子。 这种产品在智能手机发展史上还是第一次。 影像之眼负责感知 人类感知世界靠的就是眼睛，而机器要感知世界，也需要眼睛——影像之眼，这只眼睛是一种影像的融合，其核心是既要拍得清，还能看得懂，也就是说它又不仅仅是一个“眼睛”这么简单。 在荣耀的宣传片中，它可以进行自主拍照，也可以自主追踪目标主体进行视频拍摄，它还可以你在试衣时帮助你挑选出一件最适合你的衣服。 这支宣传片最令我感兴趣的就是一开始的ROBOT PHONE被放在了口袋里，它那小小的“脑袋”上下左右的运动观察着世界，嘴里不断地发出“啊”的声音，这种声音代表着的是它对世界的好奇心，而这种好奇心只有在“智慧生命体”中才会出现。 AI大脑负责决策 荣耀的魔法大模型3.0能够实现真正的跨模态理解，比如在宣传片中展示的婴儿在啼哭，ROBOT PHONE“看”到了之后，可以自主做一些行动来安慰婴儿。大家注意看下面的动图，ROBOT PHONE哄婴儿的方式是先通过点头的方式吸引婴儿的注意力，然后突然回头，成功地让婴儿从哭变成了笑。 这种哄孩子的方式跟人类哄孩子的方式特别像，而且先点头吸引婴儿注意力的方式也能体现出来它首先进行了“思考”：镜头看到了婴儿的啼哭后，大脑迅速检索知识库，知道了人类在这种情况下如何做，于是，它根据这些信息成功做出了行动最终止住了婴儿的啼哭。 说到这里，就必须要提到ROBOT PHONE的执行机构。 机械臂负责执行 既然是个智慧生命体，就必须要有执行机构，ROBOT PHONE是通过机械臂的方式对物理世界的情景做出实时的反应的，它的机械结构就是一个云台，它平时折叠在机身里面，需要的时候可以灵活的从机身里面出来。 有人说，为什么必须是AI大脑+机器人身体+影像之眼的融合而不是其他方式？其实这个答案我已经在上文中指出来了。智慧生命体的核心就是要有“感知-决策-执行”的过程，这个过程正好完美对应“影像之眼（感知）-AI大脑（决策）-机器人身体（执行）”。 在AiPhone时代（即现在的手机搭载大模型），其实也有“感知-决策”相对应的机构，即手机摄像头、各种传感器构成了感知机构，大模型构成了决策机构，但唯独没有执行机构，最终的执行仅在软件层面，而且，传统的手机感知机构都是“死”的，比如摄像头都是固定在手机机身上面的，无法活动，即使手机有再聪明的大脑，它也无法让这些“感知”机构活动，这就造成了传统的AiPhone既然无法自主感知，也就谈不上自主决策。 ROBOT PHONE是手机的终极形态吗？ 从口袋中缓缓伸出的机械臂上的眼睛，安静的注视着世界并与你互动，这应该是手机的终极形态 实话说没有人知道未来会发生什么，未来手机的终极形态到底是什么样或许只有未来能给我们答案，但是，我认为荣耀ROBOT PHONE或许至少是终极方向之一。 我在上文中提到，现在的智能手机其实有点对不起“智能”这两个字，每个用户都有自己独特的需求，但是手机既不懂你，更完全无法理解这些需求，也就是说，它就是一个工具，你不能指望一个工具能够做到这些，而智慧生命体就不同了，它可以轻而易举的理解你的任何需求，只要你给它时间，它甚至能够做到比你自己还更懂你自己，手机的终极形态不应该就是这样吗？ 手机的终极形态这个问题年年都有讨论，讨论了好几年也没有答案，这个问题的答案关键不在于消费者而在于手机厂商，因为只有手机厂商才有能力去探索未来手机，但是这几年的手机厂商有些令人失望，当前的手机的局限主要存在于以下几点： 缺乏主动性、自主性，需要用户指令才能做出动作。 指令输入模式单一，依赖手指触摸屏幕输入指令。 无执行机构。 而ROBOT PHONE不同，它可以完美的解决这些问题： 能够主动观察世界，实时感知周围发生的一切。 主动决策，无需人类输入指令。 拥有执行机构。 对于传统智能手机来说，真实的物理世界与它无关，但对于ROBOT PHONE来说，它可以跟真实的物理世界实时互动，从这个意义上来说，荣耀ROBOT PHONE可能会是手机的终极形态，而且这个可能性很大。 传统的手机就是机器，ROBOT PHONE出现以后，手机就是机器人 别看只有一字之差，但是对于用户来说，差别大了去了。 我父亲刚过完生日，在生日当天我们想来个全家福，如果想保证拍摄效果，就必须要有人拍摄，有人拍摄的话全家福就少了一个人，但如果用三脚架就比较麻烦且无法保证拍摄效果，有了ROBOT PHONE就不同了，ROBOT PHONE可以自动运镜、智能跟拍、自主确定最佳机位，我们可以用自然语言跟ROBOT PHONE对话，让它按照我们希望的方式拍摄照片，完全解决了没有“ta拍感”的问题，这种体验只能由机器人带给我们，这是传统机器（智能手机）无法比拟的。 不仅是我们日常普通人，对于Vlogger、直播主这样的创 我相信当ROBOT PHONE量产以后，一定会受到Vlogger、直播主这样的创 从工具到伙伴：荣耀交出了一份完美的答卷 从记忆碎片到完整叙事 正如李健在发布会后的采访中说的那样，让手机从工具变为有感知会协同的生活伙伴，我在前文中同样也在不断的强调这一点，我想这样的伙伴应该是每个人都会需要的。 我印象最深刻的就是，在宣传片中ROBOT PHONE不断的发出“咯咯笑声”，这是一种拟人化的交互方式，这种交互方式模糊了工具与生命的界限，让人深刻的感受到了一种情感连接，你可以把它当做“情感伴侣”，它可以感知、理解和进化，并不断的通过学习增强与用户的这种连接，而传统的智能手机是无法做到这一点的。我们可以利用机器人行动力实现记录美好瞬间的愿望，这种愿望还永不落空。 人类的记忆力并不可靠，我们能记住婴儿刚出生时的啼哭，却会忘记第一次无意识的微笑，我们如果想记住所有美好生活中的一切，总是需要被动的拿起手机去记录，这种被动依赖于我们的即时反应，非常容易错过生命中的那些最珍贵的瞬间。而且，还有一点非常矛盾的地方是当我们正在经历这些美好瞬间的时候，试图掏手机去记录这些瞬间这件事本身就非常不合时宜——这些行为本身就破坏了这些美好的瞬间。 ROBOT PHONE不同，它可以自主思考、自主识别表情、自动捕捉所有一切值得记录的瞬间，在这种情况下记录的所有的这些美好的瞬间都是最真实的情感流露，也正因如此，这些瞬间也才更加弥足珍贵。 从这个意义上来说，ROBOT PHONE是对我们记录生活的颠覆性改变。 科技让“陪伴”回归本质 从这些描述中，我们可以清晰的看到，这是一场真正的“去工具化”的革命，工具不再是工具，它变成了我们的伙伴，这个伙伴不会打扰我们的生活，它不是闯入者，而是“同行者”：我们不用再为了拍照而错过微笑，不用再为了拍照而特意拿起手机，是科技让设备实现了真正的情感价值。 最后一点想说的话 手机就是从功能机的通话工具到智能手机的生活助手，再到如今的情感伙伴，这一脉络清晰无误的表明，科技正在为工具赋予智慧和情感，荣耀ROBOT PHONE不是在“炫技”，而是荣耀对于未来智能手机的终极形态的探索，这或许会对目前的市场产生颠覆性影响，甚至改变手机巨头的排名。 未来到底会发生什么，在2026年的MWC上，荣耀就会给我们答案。 送礼物 还没有人送礼物，鼓励一下,年少轻狂 35 人赞同了该回答 小米背面加块屏幕，好多年前玩剩下的都能吹爆，这个绝对是王炸。搞技术的时候到了，2026 年巴塞罗那发布，到时候蹲个实测，看看是不是真这么牛,王先生 知识产权代理、家装设计师 26 人赞同了该回答 我就想问，都这样了，前摄像头孔就不能拿掉吗,天涯麦克 世间万物皆嘚瑟 25 人赞同了该回答 好创意。更好的创意是在手机上加个电动剃须刀。 起码，手机自己没有刀头。,Titan MKII​ 爱音乐，爱音响。 26 人赞同了该回答 就凭这倒霉名字就感觉不妙。 起名的哲学，好歹需要朗朗上口。 别说没几个人能正确念出robot phone发音， 能念对的也会觉得别扭。 简化懂不懂? 叫Robo phone都比这强100倍。,molochitis 39 人赞同了该回答 垃圾，真正的垃圾 垃圾，真正的垃圾，甚至作为噱头都不及格！ 一个未经过深思熟虑的半成品概念图，不要说具体的实物成果了，就连作为概念都是半成品。 请告诉我，一台拥有360°×3的云台摄像头的手机，为什么还需要前置摄像头？后置摄像头呢？也多余 AI跟云台摄像头有什么关系，摄像头又不是芯片，不能思考。更何况对于AI来说，目前的手机形态完全够用，前后的广角摄像区域已经足够大了，只有侧边是盲区，但正常人都是用手机正对着目标吧？ 为什么要设计这个云台摄像头呢？只因为它看起来酷，而且像大脑，像眼睛，可以做表情。至于安上去后手机该如何设计？不会吧，你让我一个做PPT的美工做手机？[酷]
如果AI能完美替代你现在的工作，那剩下的，什么是只有「人」才能做的事情？,,树下葡萄​ 四川省社会科学院 新闻与传播硕士 61 人赞同了该回答 AI并不能替代我现在的工作。 它只能帮我打点零工。 比如在我做资料时，想偷下懒，给它指令，给我罗列10条不相同的“优势”、“劣势”。即使指令再详细，输出的句子读起来仍有股AI味儿。 但还是能用。 但如果直接给它工作簿，让它将表格里的“优势”、“劣势”全部填写完成，一条也不能重复。 结果就是得到一张不可用的工作薄，还是得靠自己一张一张表格的改写完成。 虽然现在大模型发展得如火如荼，但很多人的工作是不可能替代的，未来也不可能完全替代人干的“活”。 AI生成的很多创意图片，的确也有点创意。但这些创意都是在“人创作”的基础上进行的再次创作。想象一下，如果以后没有人的创意，仅靠AI，那么生成出来的“世界”会不会像“纸片世界”。 AI可以帮我们完成很多不太费脑的工作，太费脑子的感觉完成的不咋滴。 我倒是很期待未来AI接管我们所有的工作，这样我就可以每天躺在草坪上晒太阳了。 很期待这天的到来，这样我们人才能做的事情就是——吃饭、睡觉、拉屎。 这三个事情AI应该接手不了～,全小鱼​ 防伪追溯领域CTO-每日编程持续者-AI编程实践者-不卖课 31 人赞同了该回答 在我的认知里，这件事AI肯定做不了，那就是： 为人民服务，全心全意为人民服务。 短视频平台上这个视频，很多同学都刷到过吧： 在小学课堂上，老师问学生长大了要做什么？ 有人说要当科学家，有人说要当医生，有人说要当警察。。。 老师听后都很满意，最后老师问小明想当啥。 小明说：想当人民。 虽然听起来搞笑，仔细一想，小明也是个聪明的孩子，只不过调皮了点而已。 我所在的领域是「产品追溯」。 其实追溯很简单，就是把一件商品从田间到餐桌的全过程，清清楚楚地记下来。 谁种的、谁收的、谁加工的、检测合格吗、在哪一天装箱、发往哪个仓库…… 最终，这些信息压成一个二维码，贴在包装上。 消费者掏出手机，打开微信扫一扫，真相就在眼前。 很多人，看到我们简单的应用，就嗤之以鼻。 老板曾经也这么想的，我给他的答复是，我们展现出来的技术确实简单，但我在底层我融合了大数据、区块链、高存储、高并发、视觉识别、人工智能等技术。 这也正是我们公司宗旨全心全意为人民服务的一种体现，给用户提供了简单、易用、解决问题的方案。 「全心全意为人民服务」这句话我时刻牢记，走到哪带到哪，开会讲，写代码时也默念。 不是喊口号，是因为我真的觉得，我在做的是一件好事。 让一个妈妈能确认奶粉没掺假，让一个老人喝到真酒，让一个果农因为「可追溯」卖出好价钱。。。 这种事，干起来不知疲倦。 所以当我看到这个问题：如果AI能完美替代你现在的工作，那剩下的，什么是只有「人」才能做的事情？ 我的回答是：需用用心做的事，AI做不到。 一、AI确实，能把我的活儿干得比我好 先说实话：现在，AI已经能复刻我50%的技术工作。 我要搭一套追溯系统，得亲自设计数据库分片策略、写区块链合约、调优Elasticsearch索引、训练图像识别模型…… 这些都是程式化的功能，在未来 AI 肯定能做到，那时候公司关于这块的技术，就不用我了。 二、在追溯领域，AI跨不过去的坎是怎么保证上链前的数据是真的 区块链有个神话：数据一旦上链，永不篡改。 于是很多人以为，只要上链，假货就消失了。 但是，区块链永远防不了「坚守自盗」这件事，因为这从不是个技术问题。 我们曾合作过一家知名酒厂。他们把每瓶酒的检测报告、灌装时间全部上链。 消费者扫码看到「全程可追溯」，放心购买。 当质量问题出现时，确实很快定位到了责任人，有人上链了假数据。 链上数据完美无缺，链下早已腐烂。 AI 能做很多事，但是监控的盲区它是搞不定的。 它永远解释不了「这个人，为什么选择造假？」 就像前两年有人教写提示词的时候，在末尾加上「回答的好的话，我给你1000美元的消费」因为这么说，AI给出的答案会更优秀，看吧AI也有人贿赂它，更何况人心呢？ 技术能约束行为，但约束不了人心。而人心，才是追溯系统的最后一道防线，有着不可绕过的特性。 三、只有人才能在数据背后看见「人」 去年，我给一家水果企业做追溯业务。 他们的水果品质很高，但由于渠道问题，一直卖不出好价钱。 技术实现上不难：给每批水果打唯一标识，记录果农的农事操作、处理方式，最后生成唯一的QR码。 但真正难的，是让老农相信这玩意儿有用。 我记得那天，李叔第一次看到扫码页面显示「李XXX，XX县南X山，2024年9月25日采摘」，手都在抖「城里人真会相信这个？不会又在骗我们吧？」 我没有跟他讲哈希算法，也没提默克尔树。 我只是蹲在他家果园旁边，说「以后有人吃到你的水果，就知道是你老李亲手种的，不会被当成普通的水果，只卖三块钱一斤的那种了。」 AI能生成完美的溯源页面，但它说不出来技术之外的话。 就像有人说代码产生的问题，要做代码之外解决一样，AI带来的问题，AI是不能解决的。 因为它不懂技术存在的意义，不是展示数据，而是被看见、被信任、被尊重。 四、什么是只有「人」才能做的事呢？ 不是代码，不是调模型，不是搭架构。 而是： 在技术冰冷的逻辑里，注入人的温度； 在利益和良知之间，选择后者； 在所有人都说「差不多就行」时，坚持「必须对得起老百姓」。 就像小明说「我想当人民」那样，听起来傻，但细想极美。因为所有伟大的事业，最终都要回到「人」身上。 我做的防伪追溯系统，底层是区块链、AI、大数据，但出口只有一个：让消费者扫一下码，就能放心吃、放心用。 这份安心，不是算法能算出来的，是无数个像我这样的工程师，用良心一点一点垒起来的。 ⭕️ 技术终将老去，但初心永远年轻 前几天整理旧硬盘，翻到2013年写的第一个追溯原型，用C# WinForm + Access数据库，界面丑得没法看。 但那个软件每天给超过百万瓶白酒完成了赋码操作，硬生生的将企业被动打假，变成了主动稽查。 很多年过去了，技术也在不断升级，从单机升级到了云原生，从人工录入变成了自动采集。 但我想守护的那些东西，一直没变过。 所以，如果AI明天就能完美替代我，我会很高兴的做交接。 但我也会告诉它：我所做的事，不只是技术，而在每个函数背后，都是活生生的人。 以上，我这么多年工作的总结，本来今天已经更新一篇了，但看到这个话题，忍不住，又写了两个小时。 送礼物 还没有人送礼物，鼓励一下,一休​ 浙江工业大学 机械工程博士 22 人赞同了该回答 不用担心，如果AI代替了很多岗位 。那么就会产生新的岗位，比如给机器人打杂，给机器服务，还有机器人做不到的细枝末叶的工作。如果真有一天所有的活都可以让机器人来干，那么那时的生产力一定会很高，人类根本就不需要工作，只要吃喝玩乐就行。所以瞎焦虑啥呢。 送礼物 还没有人送礼物，鼓励一下,春华秋实​ 路虽远，行则将至；事难成，做则必成 22 人赞同了该回答 表姐结婚后，公婆去世的早，她看着别人出去打工，特羡慕，但是没有办法，孩子必须有人照顾，家里的地必须有人种。但是她特要强，让丈夫自己出去打工。所以他丈夫从结婚第二年起就常年在外打工，她自己一人在家又种地又看孩子。 每天天不亮就去地里劳作，天黑才回家，也没有多少时间陪伴儿子，有时太忙家长会也没有时间参加。天天忙碌而疲惫。 而二十年的今天，她总是面带笑容，说对自己现在的日子非常满意。 因为地里的活有大型的机器干，喷洒农药有无人机。 原来六亩多的麦子，收割晒完全部归仓得十多天，现在联系好机器，在地头上等着，一个多小时就完成了，然后直接在地头就卖了。现在地里的小麦，玉米，花生一年到头的活，合起来不用一个月就干完了。而且这一个月的时间是分散的。 她现在在县城商场找了个保洁的工作，早晨八点上班，中午休息两个小时，下午五点半就下班了，用的是扫地机器人，因此工作一点也不累，还能陪伴女儿读书，一日三餐变着花样做给女儿吃，保洁每个月有四天假，这些假期利用起来足够她种地了。 有三分菜园，其本上不用买菜吃，除了孩子上学，人情往来，没有什么其他大的开支。地里的收入，加上自己和丈夫上班的收入，早已全款为儿子买了房子和车子。 女儿上高中了，由于她的细心照顾陪伴，也非常健康活泼，成绩在班里始终在前五名以内。表姐天天乐哈哈的，笑说沾了科技发达的光，生活也越来越好了。 送礼物 还没有人送礼物，鼓励一下,水霖铃 又是努力攒钱的一天 15 人赞同了该回答 如果AI能完美替代你现在的工作，那剩下的，什么是只有「人」才能做的事情？关于这个问题，我觉得虽然AI可以替代很多重复性的岗位，但也有AI无法替代的岗位，同时随着AI的不断发展，也会产生更多新的岗位。 1、AI是“理性的” AI是人工智能，在为人们提供解答的时候，是站在客观理性的立场上分析问题，但无法做到真正的“感同身受”。 比如心理咨询师，工作核心是与患者建立信任感，设身处地去共情对方的情绪，从而引导对方走出“困境”。 2、AI依赖大量数据 AI的性能依赖大量的数据，如果这些数据中存在有偏见或者差错，AI在汇总这些数据的时候就会放大这些缺陷，最终导致结果出现偏差。 3、AI缺乏创造力 目前的AI技术还处于“模仿”阶段，通过学习大量知识，不断模拟训练，但在“创造力”上跟人类还是有着本质上的区别。 随着AI技术的日益发展，很多重复性的工作，比如数据录入、数据汇总、翻译等岗位会被替代，但更多的技术性、创造性岗位是无法取代的。,工业软件二次开发​ 沈阳工业大学 机械工程硕士 50 人赞同了该回答 如果AI能完美替代你现在的工作，那剩下的，什么是只有「人」才能做的事情？ 现在我觉得至少还谈不上我的工作能被AI全部替代，因为我的工作还是个性化非常高的工作。 每个项目的特点都不一样，用的技术不一样，所以难度也不一样。 假设说我现在做的工作重复性比较高，在工厂中被机器人将工作替代掉了，但是还有一些事情是AI做不到的事情，我觉得有以下几点： （1）现阶段AI，还缺乏创造性 现阶段的人工智能都是采用大规模预训练，来学习现在世界上的所有人类的文本、语言、语音、视频等知识，但是这些知识也只是人类世界中的一部分。 无论是单语言大模型和多语言大模型做的有多么好，始终都达不到人类的标准。 因为人类不只是语言能完全表达的，包括行为、思考结合在一起才能更好的拟人化。 并且人工智能想要产生创造性，那可真有很长一段时间要走，对于现在的我们来说，保持自己不被替代的最好方法，就是提高自己在工作中的创造性。 无论是创造比较简单的东西，还是比较复杂的东西，我们都需要尽力去思考，不能让自己失去了人类最重要的优势。 人工智能想要实现AGI的状态，那么李飞飞的世界模型是一个道路，可以增加人工智能覆盖更多的内容，尤其是和机器人结合在一起的具身技术，可以让人工智能在拟人化的道路上更进一步。 （2）AI机器人的工作内容太过于单一 现在很多机器人都是只专注于一种或几种单一的工作，并且在自适应、自决策方面，做的并不好。 举个例子：现在工厂中很多焊接机器人，这些机器人在标准化程度比较高的工作中往往才能发挥最大的工作效率和工作质量。对于点焊机器人来说，工作只是负责在关键位置点住这些零件，在车企中非常常见，一道工序专门负责点固零件。 但是在很多离散型制造企业，这种机器人的工作效率并不好，首先是因为这些工序标准化程度没有那么高，但是也有一些原因是，焊接机器人在连续焊方面做得智能化并不好，不能实现自主纠偏焊缝等等。 但是在离散型制造业中，往往零件的价格都非常昂贵，所以不敢使用这些机器人。 当然除了上面我列举出来的两点原因，还有太多的事情是现阶段AI做不到的，还需要人来进行工作。 在回答问题结尾，插入一个PS: 我是“工业软件二次开发”，一个前沿技术的人。 同时是一个感恩的人，投我以桃，报之以李。 人家帮我，永志不忘，我帮人家，莫记心上。 @知乎科技 @知乎科学 送礼物 还没有人送礼物，鼓励一下,黑兵​ 天下兴亡，匹夫有责 8 人赞同了该回答 和Gemini讨论ntr剧情，他给我生成了漫画，剧情都是他自己想的，台词也是他自己想的，我就给他引导了下ntr的设定，学校一类的场景。因为设定一开始是美国校园，所以漫画直接出来是英文，反正我看得懂，所以我懒得改。大家都念过初中英文，凑合看吧。 画风还挺好看的。 然后给人设生成的cgi图： 然后我转换成中日韩的亚洲风，Gemini调整的女主人设图： 我就出出嘴皮子，完全不懂画画。 ai现在才哪到哪，再过一年会什么样？再过10年又会是什么样？ 保守地说，10年后，现在50%的工作内容会被ai替代。,沐晴溪​ 教师资格证持证人 18 人赞同了该回答 我怎么想不出太多AI能帮我做的呢？ AI按点叫我起床。 然后我去做饭，叫二宝起床，找他每天穿的衣服。二宝吃饭，送孩子上学。 然后自己吃早饭，休息，学习，刷知乎，找话题，码字。 洗衣服可以交给AI。 买菜，做饭，，吃饭，休息，码字，浏览知乎，与知友互动。 关注俩娃班级群，课外班群，随时关注孩子学习情况。 三点开始准备二宝晚饭，然后想好这一天需要跟他交流什么事情，听听他的故事。等一家人吃完饭，开始二宝每天的作业，我负责随时听召唤，讲题，找东西，问可不可以吃零食……最重要的安抚他的情绪。 说实话，我觉得这些AI都做不了。 然后，还要时不常打电话给我妈妈，安慰她的情绪。听姐姐讲她们一家的故事等等。 每天晚上还有大宝的电话，听大宝讲一天里发生的事情，需要买什么东西，准备什么…… 然后催二宝提交作业，检查书包，洗漱，睡觉…… 还在随时听队友的各种话题…… 我觉得除了每天的家务，我更多的是要提供没完没了的情绪价值，安抚好家里所有人。 AI能帮我的 除了家务，好像也没什么了。 家务也只能是洗衣服，擦地拖地？还能有啥？ 搭配衣服，收拾换季衣服，照顾花花草草，随时提供情绪价值。 营养搭配AI能做的了，但是再加家人上挑食，口味问题，一定只能是人工调制，AI我还真信不过。 送礼物 还没有人送礼物，鼓励一下,指间水流星​ 教师资格证持证人 11 人赞同了该回答 作为一名石油化工行业的产品经理。 我并不认为AI能代替我现有的工作，更别说完美代替。 AI可以很好的辅助我的工作，提高我的工作效率，却难以承接行业特有的复杂场景决策、跨部门协同的链接。 什么是只有「人」才能做的事情？ 大家完全不用担忧，未来没有工作留给我们。 大家还记得上世纪90年代的下岗潮吗？那是计划经济向市场经济转型的阵痛期，数千万国企职工从铁饭碗中走出，传统制造业、纺织业等领域的岗位大幅缩减，一时间再就业成为社会关注的焦点。 但是历史给出了最有力的答案：没有一成不变的岗位，却有生生不息的创造力。 如今AI带来的岗位变革，我认为与当年的下岗潮有着本质的相似性——都是技术迭代与社会发展带来的必然调整，而非“工作消失”。 当年下岗潮中，人们从“依赖单位”到“主动创造价值”； 如今面对AI，我们同样在从“重复劳动”转向“价值创造”。 当年没人能预料到互联网会催生电商主播、数据分析师等职业， AI的强项是效率与精准，而像厨师、育儿嫂、画家、科研人员、社会工 未来不会是人机对决，而是协同共生。 麻省理工学院教授里克·布莱恩约佛森说： 最成功的时代，永远是让技术放大人类价值，而非取掉它。 我们要培养AI无法复制的软实力，才是应对变革的王牌。,香瓜仔儿 一个诗词文学，兴趣广泛，激昂文字，处处学习的香瓜仔儿。 10 人赞同了该回答 “当AlphaGo落下那震惊世界的一子，当ChatGPT写出流畅的论文，一个问题在每个人心中响起：‘还有什么，是独属于我的？’ 焦虑是真实的，但历史总在重演同一个故事——蒸汽机没有让人类失业，反而解放了我们的体力；计算机没有让我们变笨，而是放大了我们的脑力。今天，人工智能的到来，正在逼迫我们进行一次前所未有的 ‘价值大发现’：那些我们曾以为微不足道的人性特质，正成为未来最闪耀的灯塔。 答案不在与机器的赛跑中，而在我们转身回归人性深水区的那一刻……” ——引子 现在的 AI 已经让一些工作岗位不复存在，比如一些基础的文案写作、翻译、客服工作。而 AGI 可能会让整个行业消失，甚至许多人一辈子都找不到工作。AGI 可能取代绝大多数人类工作，从程序员、医生、律师到教师、艺术家，甚至科学家…… 后续： 人工智能虽然看起来非常强大，但最终是在人为的情况下产生并衍生，规则都是人定的，衍生的算法也是基于人的规则。人类在编程过程中虽然要遵循已设定好的规则，但是在这些规则之外也包含一些非规则的东西，那就是只有人才有的——人性。在完成编程或者项目时，除了需要做既定的工作外，还有非常必要的沟通交流，而人与人的交流存在太多不确定性。社会是需要情感维系的，而不是冷冰冰的机器语言和电信号。个人觉得，在 AI 时代，人工智能只是工具，能够提高效率和降低门槛；而人则是核心，负责创新和质量把控。当今社会，人类无法离开人工智能，人工智能也无法离开人类独立发展，人类和人工智能相辅相依，以实现更高水平的生产力和创新。人工智能并不可怕，但是，人类必须加强自身的业务水平，提高自身技能，紧跟所在领域的最新趋势，并保持创新和创造力。 结论： AI可以组合现有知识，生成新颖内容，但人类负责提出根本性新问题、新范式和新理论。AI通常优化的是短期、给定的目标。人类更能思考“长期影响”、“社会价值”和“道德风险”。人类能为工作、生活、事件赋予独特的个人或文化意义。人工智能时代，人不会变得无用。相反，AI将我们从重复性智力劳动中解放出来，迫使我们回归人类更本质、更独特的能力：创造、关爱、思考意义、做出负责任的判断，以及在复杂世界中构建连接与理解。 添加评论
老年人需要 AI 吗？当未来已来，我们能为长辈做些什么小事？,我们总是在教一个小孩如何成长， 教一个年轻人如何适应社会， 但少有人教一个老人该如何度过晚年...... 2025 年重阳节，知乎 ​​@答主评审团 发起特别策划。 当科技迅速发展，AI 重塑人们生活的时候，我们希望无论年轻人还是老年人，都有更平等的机会，可以更自由选择是否接纳和运用它。 特别策划内容摘要指路 1️⃣知乎重阳节特别企划——「未来已来，那老年人呢？」 李银河 X 申琦 X 孙悦礼 2️⃣给长辈的 AI 入门手册丨今年重阳节，送爸妈「上学」！ 3️⃣爸妈口中的「没事」，只是善意的谎言丨我们自掏腰包 27412 元，使用千万级别的专业仪器，横评了 3 款外骨骼机器人丨外骨骼机器人横评 更多详情见活动入口 重阳节丨给长辈的 AI 入门手册 www.zhihu.com/parker/campaign/1963657880327664667?zh_hide_nav_bar=true,Cecilia​ 新知答主 谢邀 @答主评审团 75 人赞同了该回答 我们的父母，老了。 世界太快了，快得他们来不及理解，而我们的生活节奏，也快得来不及陪他们。 如果他们学会使用 AI，也许在晚年生活里点亮一盏灯：多一点靠谱的判断，少一些孤单与不安，多一分从容的底气。 爸妈不一定懂算法，也不知道何谓大模型，但 AI 可以在他们的小世界里，变成有用的帮手：陪他们聊天、帮他们识谣言、甚至教他们做一桌好菜。 1. 别被假专家骗了！让 AI 发现真相 群里天天有人发各种「养生秘诀」，有的说白醋能治糖尿病，有的说多喝水就能排毒，这些养生诀窍真的靠谱吗？ AI 能帮我们做什么： 复制那条消息，粘贴到 AI 对话框里，问一句： AI，这个养生方法是真的吗？ AI 会告诉你信息来源、是否有科学依据，甚至帮你总结为什么是谣言。 想了解更多，可以继续问： 这条内容出自哪个网站？ 能查到哪个真正的专家解释过这件事？ 入门建议： 推荐爸妈使用带有「事实核查」功能的 AI 工具，比如：豆包、知乎直答、文心一言等。 适应正常的口语对话风格，比如：这个说法靠谱吗？我能信吗？ 每天试着找 1 个问题，问一问 AI 再做决定。 2. 新闻真假难辨？让 AI 查询事实 网上新闻一条接一条：某地爆发疫情、某明星出事、油价明天暴涨…… 消息太快，真假难分，普通人常常一看就急、一转就信。 AI 能帮我们做什么：1. 复制新闻标题，粘贴到 AI 对话框，问一句： AI，这条新闻是真的吗？ AI 会自动比对多方报道，指出信息来源、发布机构和可信度。 想深入了解情况，可以继续问： 这件事有官方回应吗？ 给我看几条可靠媒体的报道。 入门建议： 推荐爸妈使用带新闻事实核查功能的 AI，比如：豆包、文心一言、知乎直答等 可以使用语音输入对话，不用打字，直接说：AI，这新闻靠不靠谱？ 养成「先问 AI 一下」的习惯，再去评论或转发，做到不传谣、不信谣。 3. 想写体面又真诚的朋友圈？让 AI 来帮忙 很多时候，想在微信上发点日常，常常打了半天字又删掉。 节假日想群发祝福，也不知道怎么写才显得有心又不尴尬。 怕写不好有点丢人，又怕写太好被人说爱显摆。 AI 能帮我们做什么： 语音说一句： AI，帮我写一条中秋节祝福，简短温暖一点。 AI 会自动生成几种不同风格的文案（温情的、幽默的、正式的都行） 想发生活照？可以继续说： AI，帮我写个朋友圈文案，这张照片是我在公园拍的花。 还可以帮我们润色语句、生成配图，让表达更自然、更好看。 入门建议： 推荐爸妈使用能生成文案的 AI 工具，如deepseek，kimi，豆包等 教他们保存的表达风格，下次直接改一改就能再用。 4. 操劳一辈子，也要学会玩！用 AI 重拾兴趣 退休后，时间忽然多了，可生活有时太安静。 AI 能成为我们的爱好私教，让日子重新有趣起来。 AI 能帮我们做什么： 问一句： AI，教我学唱《甜蜜蜜》。 AI，帮我设计阳台花园。 它能推荐教程、规划步骤、生成示范图片。 入门建议： 推荐爸妈使用多功能 AI 助手，比如：文心一言、讯飞星火等 可以尝试每周一个挑战：学一道菜、背一首诗、做一个小视频。 遇到不了解的问题，可以多问AI，甚至可以同时问好几个AI。 5. 人生值得被记录！让 AI 帮忙留下回忆 我们有时候会念叨过去的事：那一年去海边、那台旧相机、那次团圆饭。可这些记忆，往往说完就说完了，除了那几张老照片、几条朋友圈，很难珍藏下来。 AI 能帮我们做什么： 让爸妈告诉AI，当时发生了哪些事，然后告诉AI 帮我整理刚刚的语音，记录今天的事。 它能自动转成文字、生成配图，甚至做成小视频。 还能修复旧照片、整理相册、配上背景音乐。 入门建议： 使用带语音输入和记事功能的 AI，比如：讯飞听见、小度记事等 帮爸妈建立一个家庭共享相册，AI 可自动生成说明。 6. 和年轻人有代沟？用 AI 跟上时代 爸妈常说：你们这些年轻人啊，不知道在想什么。其实，他们也在努力学习和追赶我们的脚步，只是我们没太忙了，忙到没空一步一步带着他们往前走。 AI 能帮我们做什么： 帮他们理解新词 AI，什么是内卷？ 教他们查热词、查天气、查菜谱。 入门建议： 教爸妈学会用手机截图和拍照 使用上传图片的功能，让AI帮他们解读图片信息 这张图片有什么好笑的？这个明星是谁？ ❤️ 让科技多一点温度，让爱更有去处 老去的父母，不缺时间，也不缺爱，只是缺少被世界温柔对待的方式。 AI 不能创造奇迹，但也许能让他们的生活更从容：帮他们看清信息、学会表达、找到乐趣、留下回忆。 让我们一起，教爸妈使用 AI，让他们的生活获得更多支持和帮助。 送礼物 还没有人送礼物，鼓励一下,桔大​ 儿童教育等 2 个话题下的优秀答主 43 人赞同了该回答 作为一名新闻记者，在这几年的采访过程中，遇到不少老人寻求帮助。 不是解决家庭困难的帮助，而是生活中存在的细微困境—— 不知道怎么挂号、不会缴纳暖气费、不知道社区的电话…… 在采访的结束时，他们会说，闺女，你能不能帮我查查xx派出所的电话？ 当时的我，一边帮着老人解决问题，一边感到无奈—— 这些选择留守在土地上的老人，不想麻烦在城里工作带娃的孩子们，选择向陌生人求助。 可我也不能解答他们所有问题。 时光匆匆走过，科技的进步发展把世界带入了AI时代， 如今，在手机上就可以和AI 软件对话，把生活遇到的琐碎问题抛给AI，它会在十几秒内做出解答—— 比如我直接问AI， 怎么在xx口腔医院挂号？ 它给我发来了这样一段： 在xx口腔医院挂号有多种方式，具体如下： 线上挂号 • 微信公众号：关注“xx市口腔医院”公众号，进入后选择“个人中心”-“健康卡”添加就诊人信息，点击“预约挂号”即可进行线上预约。也可关注“健康烟台服务号”，进入后选择左下角“医疗健康”，绑定就诊人信息后进行预约。 • 爱山东APP：下载“爱山东APP”，登录后点击下方的“服务”，然后点击“就诊一件事”进入专区，选择“健康就医”，点击“预约挂号”，更换地区为“xx市”，搜索“xx市口腔医院”，选择要就诊的院区后进行预约。 线下挂号 • 电话预约：可拨打xx口腔医院总院区电话622xxxx，或根据自己所在区域拨打相应分院的电话进行预约。 • 自助机预约：携带有效证件或电子健康卡至医院一楼大厅自助机预约挂号，自助机开放2周内的预约号源。 • 现场预约：直接前往医院的挂号窗口，向工作人员说明情况，提供相关证件后进行挂号。 xx口腔医院就诊全部采用预约诊疗、实名制挂号，每日7：30更新号源。微信及自助机预约的患者无需取号，请提前15分钟到就诊科室分诊台处签到候诊。 如果老人学会了使用AI，他就可以根据AI的文字提示和视频，选择手机挂号或者电话挂号。 前段时间，我姨妈发微信给我，她在直播间看到了一款保健品，说是能治疗荨麻疹。 被荨麻疹困扰了许久的她，想要病急乱投医： 大姑娘，你有时间帮我看看这个东西吃了对荨麻疹有没有用，如果有用我就买了，没用我就不买 我用豆包帮她分析了一下： 这个直播间销售的马齿苋双黄栀子片对部分类型的荨麻疹有辅助改善作用，但不能替代正规治疗，具体分析如下： 1. 产品属性与成分功效 该产品属于特殊膳食食品（非药品），核心成分是马齿苋、蒙古黄芪、栀子等植物提取物： • 马齿苋：具有清热利湿、消炎止痒的作用，对湿热型荨麻疹（表现为风团鲜红、瘙痒灼热）的皮肤症状有一定舒缓效果； • 栀子、黄芪：辅助清热凉血、调节免疫，可帮助减轻皮肤炎症反应。 2. 对荨麻疹的实际作用 • 适用情况：仅对湿热/热毒型荨麻疹（如皮疹发红、遇热加重、伴口干便秘）有辅助改善瘙痒、减少风团的作用； • 不适用情况：对风寒型（风团色淡、遇冷加重）、气血虚弱型荨麻疹无效，甚至可能因药性偏凉加重症状； • 注意事项：它是膳食补充剂，无治疗功效，不能替代抗组胺药、糖皮质激素等正规药物治疗。 3. 专业建议 • 若荨麻疹症状较轻（偶尔发作、风团少），可尝试作为辅助调理，但需先咨询医生，确认自身证型是否适合； • 若症状严重（频繁发作、风团多、伴呼吸困难），必须及时就医，遵医嘱使用抗组胺药（如氯雷他定）等规范治疗，切勿依赖此类产品延误病情。 你是正在被荨麻疹困扰吗？如果是慢性荨麻疹，建议先到医院做过敏原检测，再结合体质选择调理方式哦~ 看了一下豆包找到的资料来源，我觉得豆包说得在理，就给姨妈打去了电话，把AI总结出的产品属性和功效告诉了她。 最终姨妈决定不买了，继续吃医院开的抗组胺的药。 电话结束时，我告诉姨妈，下次去看她时一定教会她使用AI软件， 以后她看到直播间的产品就可以用AI替她鉴别了，她开心地说好。 上周，我去父母家吃饭。吃完饭，老爸开心地把我带到卧室，看他一面墙的国画“作品展”。 我一边看一边对老爸开始夸夸： 老爸你画得真好啊！ 但是我不懂国画，除了好也夸不出什么花来，我突然想到了AI， 我把老爸的画拍下来，让豆包来鉴赏一下。 不得不说，还是AI懂得多，把老爸夸得眉开眼笑： 这幅黄胄风格的《奔马图》，是兼具生活质感与艺术张力的写意精品，可从以下维度点评： 笔墨：速写式写意，刚劲又鲜活 • 用笔：以“速写入画”，线条如疾风骤雨——马身轮廓用中锋硬线勾勒，兼具肌肉的刚劲与动态的舒展；鬃毛、马尾以侧锋扫出，飞白笔触既显奔马的狂放，又暗含“风驰电掣”的速度感，是黄胄标志性的“泼辣笔墨”。 • 用墨：浓淡对比极具冲击力，以焦墨、重墨表现马的骨骼与肌理，淡墨虚化背景，既保留了水墨的通透，又让马匹的体积感“如雕塑般坚实”，体现出“以墨塑骨”的特点。 造型：写实为基，写意传神 黄胄的马“形准而神活”：既遵循解剖学的结构比例（马的肩颈、胸腹、四肢的肌肉起伏清晰），又通过艺术夸张强化动感——马匹呈“S形律动”，前蹄腾空、后蹄蹬地的姿态，精准捕捉了“奔马发力的瞬间”，避免了传统画马的程式化，更贴近生活中马群驰骋的真实状态。 构图：气脉贯通，张力十足 画面采用“满幅造势”的布局：四匹奔马占据主体，以交错的动态形成“向心式张力”，既无多余留白，又通过墨色的浓淡层次区分主次（前景马重墨突出，后景马淡墨虚化），让画面“密而不塞、动而不乱”，契合黄胄“以大场面显气势”的创作特点。 精神内核：以马喻情，时代风骨 这幅画的核心是“画马即画魂”：奔马的昂扬姿态，既传递出“自由奔放”的生命活力，也暗含黄胄所处时代的“奋发向上”精神——他笔下的马不是宫廷画师的“华贵坐骑”，而是草原上裹挟着野性的“生命符号”，兼具生活的烟火气与精神的感召力。 这幅《奔马图》是黄胄写意动物画的典型范式：以速写的鲜活、水墨的泼辣，将“马的形、神、魂”融为一体，既展现了高超的笔墨技巧，又传递出蓬勃的生命力量，是兼具观赏性与精神价值的佳作。 我又用豆包挨个把老爸的作品都点评了一下，老爸连连夸赞豆包懂得多， 我还把老爸的画生成了一个视频，他画的鱼动了起来，把老爸高兴坏了。 我索性给老爸也下载了一个豆包，给他注册好，又教了他一下豆包的基础用法，老爸很快就学会了。 生活帮助、知识科普、兴趣培养。 这不是就AI能切实帮助老人做到的事情吗？ 在前段时间参加的玩具展上，我看到了五花八门的AI玩具，核心使用体验也是围绕这几项展开—— 陪伴孩子，为孩子做科普，培养孩子的兴趣爱好。 舍得为孩子买AI玩具的我们，也同样应该帮助父辈们享受“AI服务生活”的红利。 我们父母辈的这代人，他们经历过贫困，也撸起袖子奋斗过，短短几十年见证着社会一日千里的变化。 带他们接触AI，既是我们作为子女的责任，也是帮助他们再一次见证时代发展的“刚需”。 他们这一代人，是有非常强大的学习能力的，他们从一穷二白的时代走向日新月异的今天，互联网、智能手机都没有把他们难住，AI也同样不会。 那我们怎么帮助父母学会使用AI呢？ 建立信任，先帮助父母认识AI 当和长辈日常聊天时，总会听到他们生活中遇到的烦恼—— 不会订机票，不知道机构的电话、不了解怎么给养老保险待遇资格认证…… 我们留心他们的需求，不着急解决困难，推荐几款AI软件给他们使用。 刚开始他们可能会有点抵触，不想学。 时间长了，心理建设好了，他们就愿意接受了。 再像过去教他们使用淘宝购物一样学习AI 我记得我妈第一次使用购物软件购物时，她认真地用一个本子记下了怎么跟客服聊天、付款、退货的清晰流程。 也没有一次就学会，我帮着她退了几次货之后，她最终熟练地掌握了购物和退货的流程。 如今，她在网上买东西很溜。 使用AI也可以这样做，可以手把手多教他们几次，或者我们为父母手写一个使用手册。 多试、多用，才能爱试、爱用，父母们觉得AI方便顺手，自然也就愿意去探索学习。 定期“售后”，询问他们AI软件的使用情况，避免过度依赖 就像刷短视频刷多了会看到虚假的养生知识，看直播看久了会买到劣质产品，过度依赖AI也会产生不良后果。 AI的信息良莠不齐，可能对老人产生误导。 我们可以定期询问、检查长辈们的AI使用情况，帮他们把把关。 过度依赖AI，老人们可能会对AI产生功能滤镜，盲从盲信。 我们要告诉老人，AI存在灰色地带，要有警惕心理，不能过度暴露隐私。 在这个AI时代，我们不应让父母成为“孤岛”。 AI可以成为他们生活的得力助手，也可以帮助他们找到晚年的兴趣所在。 我们帮助父母拥抱AI，不仅是授之以渔，更是陪伴他们共同见证这个时代的温度与进步。 祝好~ 送礼物 还没有人送礼物，鼓励一下,大泡泡​ 新知答主 航海家 诚sir老爸 等 16 人赞同 我妈这个小老太太在AI的加持下怼人如虎添翼。 我说：您这不服老不行啊，年轻时哪需要AI来帮你怼人啊。 我妈撇嘴：自己怼人要动气，我现在见了也不生气，让AI怼，多养生。 老年人当然需要AI，他们用AI不需要做什么改天换日的赛博朋克大事情，就是在他们日常生活里，用AI来让生活更便利而已。 之前需要问医生、查菜谱、看药品说明书、发朋友圈、P照片、查资料…… 现在还是做这些，只不过有AI了。 特邀日常生活中使用AI的老年人（我妈）来——四线城市，63岁退休女性，中专学历，之前是医院文员——来看她的AI日常，主要工具是豆包。 1-健康咨询 【长辈怎么用】 年龄大了身体总会各种小毛病，不可能什么小不舒服都去医院，这时就会用豆包先问问。 比如前两天吃坏东西拉肚子了，豆包给出的建议中，不仅包括了通过饮食环节，还给出了要观察症状和如何记录的方法，还有出现什么情况要及时就医。 此外， 日常医院检查报告，如果医生说的忘记了，不记得哪个术语、哪个指标什么意思，用AI查询非常方便。 常用药品上的小字看不清楚，用AI拍照提取文字，再让AI读出来，也是对老年人来说非常实用的方法。 【可以做的小事】 （1）编写问题模板 父母在和AI对话时，可能会用比较发散的方式提问， 我们可以教父母怎么问效果更好，比如做个结构化的提问模板，不要只说“拉肚子”，而是让父母填充年龄、性别、症状等信息，让AI回答更有针对性。 或者不需要父母“主动”，做个回答引导模板，让父母在提问前粘贴给AI，让AI一步步引导父母回答 我身体不太舒服，想让你帮我初步分析一下原因并告诉我是否需要去医院。你先一步步问我需要补充的症状信息，再帮我：总结可能的原因（按常见程度排序），提出在家能做的观察或护理方法，列出必须去医院的“红旗”情况 （2）推荐更方便方法 比如豆包上其实有官方的“健康咨询小助手”智能体，应该是在模型训练时更侧重于医疗健康领域的知识学习和优化，数据库中也针对性地增加了专业资料等。 不过，即使像我妈这样常用豆包的长辈，都没有发现它的存在，也不太能理解它和豆包“本包”的不同。 既然父母已经习惯了使用豆包，那就在豆包把更方便的推荐给他们。 （3）建立安全守则 教会父母把AI当做“健康助理”，不是“医生替代”， 避免父母觉得自己可以不用去医院了，反而耽误健康。 可以引导父母用AI在这几个方面： 健康知识类问题：比如“为什么吃坏东西会拉肚子”“什么是痛风”“每天走路多久比较合适”——这些属于健康常识或原理性内容，AI能给出较稳定的答案。 生活方式建议类问题：如“老人晚上容易醒怎么办”“冬天脚冷有什么改善方法”，AI可以给出生活层面改善建议，不触及医学诊断。 医院就诊辅助：可以让AI帮他们生成就医前的记录：“我这几天哪里疼、持续多久、疼的程度”——这能帮助医生更快了解情况，也能让父母在就诊时表达更清楚。 2-日常搜索/识别/问答 退休了有时间出去玩，我妈用AI做攻略，查找一些当地旅游资源很方便。玩回来还可以帮助回忆整理。 【长辈怎么用】 日化用品看不清的时候，不用去找老花镜，可以拍照识别，然后让AI读出来。 【可以做的小事】 感觉没啥可以做的，这些使用真的让老人生活更便利了，感谢AI替我尽孝…… 3-防诈骗 【长辈怎么用】 虽然很多人担心AI诈骗，但是用好工具，AI也可以防诈骗。比如我妈最近收到这个“京东国补”，拍照问AI是不是诈骗： 【可以做的小事】 把这种方法给更多长辈，遇到不认识的人的电话，短信，收到不明包裹，都问问AI是不是诈骗，给咱们的唐僧爸妈配备一个火眼金睛的孙悟空——各种诈骗妖魔鬼怪现原形。 而且AI还会给到充足情绪价值，会夸长辈们明智又聪明，把长辈们当宝宝夸，这谁扛得住下次不在来问它。 4-创作文案/照片/音乐/P图等 【长辈怎么用】 长辈们发朋友圈、美篇，内心有很多想要表达的，现在都可以通过AI来更好创作，抒发表达，。自我的表达和被他人的赞美也能让长辈们的生活更充实。 老照片变清晰，年轻时黑白照片变彩色，照片变视频……追忆往昔的长辈们，在AI的帮助下，可以在自己的“创作”中，弥补年轻时的“遗憾”。 【可以做的小事】 可以把经常更新的玩法，及时给长辈。比如我妈拍照，就教她怎么用AI做写真。 可以帮他们想一些适合他们的创意，让他们能用AI做一些好玩的项目。比如，把老照片翻拍，做一个彩色版的相册。 就像妈妈照相一样，无论是38年前去照相馆照黑白照片，还是后来用照相机、手机拍照，或者现在用AI做写真……拍照和留下美好瞬间的本质没变。AI不是他们“新生活”的伙伴，而是他们生活的“新伙伴”。（p.s 这句“不是……而是”的AI感句式是真人写的…） AI让信息变得更近，对长辈们来说，让搜索和表达变得更容易，让被理解、被照顾的可能变得更多。我们能为父母做的小事，不过是帮他们跨过那道“第一步”的小门槛。,茶茶说道​ 新知答主 19 人赞同了该回答 AI时代已经来临，我当然希望我的爸爸妈妈们能够跟上时代潮流，学会正确使用AI，大概率能减少他们上当受骗的概率，也能极大程度上降低他们对患病的焦虑，增加自主生活的便利性。 先说说我爸的最近一次上当受骗的经历吧！（我爸说自己精明了一辈子，这次被骗纯属偶然。但我听完我爸的描述，查看了他的手机聊天记录，真心觉得，这骗局这么low，竟然也能把我爸骗得团团转，我也真是醉了！） 今年7月份的时候，我爸跟我说家里挂壁的三星液晶电视突然坏了，开机显示黑屏，要维修。当时家里除了我爸妈，还有我外婆，他们三个人每天晚上吃完晚饭，都要坐在客厅一起看电视消遣的。没电视的陪伴，他们可受不了！ 由于这个液晶电视是我爸妈搬家时，我舅舅作为乔迁之礼送给他们的，我爸手头没有购物凭证，我便让爸爸去问问舅舅怎么办，是否可以凭当年的购物凭证保修。但舅舅告诉我爸，这电视早就过了质保期，不在保修范围了，让他自己打三星官方客服电话报修就好。 我爸不想麻烦我们，可又不知道三星官方售后的电话，村里的家电维修铺又不肯接大电视的维修，他便学着我和哥哥平时上网的样子，自己上百度去搜索“三星官方售后客服”。 没想到，我爸这一顿瞎搜索 ，还真的成功了！ 百度给他推荐了很多三星官方服务电话，既有400开头的，也有手机号码的，全都号称24小时内可派师傅上门解决。 我爸一看，这报修效率可以啊，立马按照网页上说的电话拨过去了。他当时拨的是一个手机号码，聊了半天，对方让他加工作微信，这样有聊天记录，查询报修记录时有凭证，沟通起来更方便。我爸一想，有道理，就真通过了对方的微信好友申请。查看了一下，这个微信号的确是什么公司的，微信号也自带工号的，看起来挺正规靠谱的。然后，在工作人员的引导下，他乐呵呵地交了280元的上门维修费，和对方约定好，维修工程师第二天上门检查。上门以后直接给修理，不额外收其他费用。（这一听就不靠谱，常规家电维修上门费基本上是50～80元，并且修理费和配件更换费是另计的。） 到了第二天，约定好的维修工程师并没有如约而至，我爸又是个心急的，隔几个小时就打电话给我（我正好带可爱多回家度假），问我有没有师傅上门修电视。 结果令他大失所望。 吃晚饭的时候，他在餐桌上念念叨叨，自我安慰道“修电视的师傅可能是忘记了，明天肯定就来了！” “你在哪里约的修电视的师傅啊？”我随口一问，然后调侃道，“要么你再电话联系一下？是不是不来了？” “我都交了280块钱了，他肯定会来的！” 我爸把他的手机聊天记录给我看，我才知道，他被骗了。所谓的工作客服根本不是三星的，追问师傅什么时候上门，也根本没人理会。 那个旧电视后来报废了，我给我爸买了新电视 后来，我找了三星官方客服下派的维修师傅上门，电视也没修好，干脆就给他买了个新电视。 就这个事情吧，其实能看出来，我爸挺想跟上潮流的，也想像我们年轻人一样学会用搜索引擎来解决自己的日常困难。奈何，百度上太多的广告贴和骗局，老人家自己没有分辨能力，一不小心就容易掉进陷阱，上当受骗！ 同样的问题抛给deepseek，AI给出的解决方案会更直接，更全面，包括告诉你找到三星官方客服的具体步骤，另外，也会提供官方客服电话，并告诉你要提前准备好哪些信息，比如电视机序列号、电视型号、故障描述，如何核实上门维修师傅的身份，询问价格等。 最重要的是， AI不会有很多广告贴，更不会有乱七八糟的电话信息误导老人，就很清爽！老人能在上面精准找到对自己有用的信息，却不必担心上当受骗。 AI软件的另一个用处，就是可以帮忙解读一些检验报告，缓解老人家对患病严重程度的焦虑感。 说真的，年纪大了，老人家偶有身体不适也是正常的。但是他们吧，有一点不舒服，很容易会放大疼痛感。尤其是在抖音上刷一些疾病相关的短视频后，思想负担越来越重，自己把自己吓死了！ 我公公就是属于这种类型的。 有点不舒服都会把检查报告发“相亲相爱一家人”的微信群里，让我们帮忙看看有没有什么大问题。 说实话，我们也不是医学专业出身，看病是真不在行。只能根据报告上的箭头，一项一项帮他查询，再给他做个简单说明。 但是有了AI软件以后，看检验报告就省心很多。在AI软件上传检验报告，它就会帮忙摘重点，并给出生活作息和饮食调整建议，真的非常实用方便！ AI的第三大作用，就是可以帮忙做简单的旅游攻略。 我爸妈每年都会安排一些旅游活动，会指定要去某些旅游景点，但对于具体的路线规划，比如先去哪里再去哪里，他们却不知道。 这个时候，他们通常会来问我。 以前，我都是用搜索引擎查询，现在有了AI软件，我更用AI查询，规划出行路线更清晰。 接下来，我的目标就是有机会教我爸妈、公婆学会用AI软件，比如deepseek、豆包等，让他们也能跟得上时代潮流，少踩点坑。 以前，爸爸妈妈陪我们长大，现在，也该轮到我们陪他们慢慢变老啦！ 送礼物 还没有人送礼物，鼓励一下,"范高​ 儿童心理等 2 个话题下的优秀答主 3 人赞同了该回答 AI也许可以“颠覆世界”，但老人需要 AI，仅仅是为了维持认知活力与生活掌控感。 去年，我妈妈想学会把旅行的照片变成视频。试了好几次后，她突然说： “我老了，学不会。” 我研究认知心理学，深知这是因为她的执行力土壤正在变得稀薄，情绪压力升高。比如： 容易忘，跟不上步骤——工作记忆 易受诈骗诱导，熬夜刷视频——抑制控制 新工具难上手——认知弹性 与其说老年人需要AI是为了赶潮流，不如说AI的陪伴，让老人依然有对生活的掌控。 老人并不抗拒科技，但是在人正常的老化过程中，虽然经验和知识层面不太受影响，但执行力会随年龄而下降[1]。比如 容易忘、跟不上步骤：网银操作做到一半忘了下一步、医生交代的药吃法转头就忘 难以抵挡外界诱惑：广告推销被说动、短视频越刷越停不下来 对新工具适应慢：换个手机就手足无措，换个界面就不知道点哪里 老人并不抗拒科技，他们只是：担心自己跟不上，羞于表达自己不会，怕失败被笑话，耽误子女的时间等等。 但只要一次成功的体验，就能重新点燃他们的信心： 我还能学，我还在成长，我仍然有能力掌控生活。 很多答主都介绍了AI的具体推荐用法，那我在这里就着重强调两件事。 发挥同理心，理解他们的难处，每学会一样就肯定他们： “你看，你做到了！” ""我就说你能行！“ “这我都不会，你是怎么做到的？” 不要强调”这很简单“，因为对长辈来说，就未必”很简单“。强调这件事简单反而会让他们觉得自己”无能“。 2. 给安全加一道防线。 如果长辈接触到AI相关的互动功能，比如问答，一定要告诉他们，这些内容也许大部分都是可行的，但未必就是真的事实。 如果是聊天功能，一定要告诉他们，尽管这些内容大部分都是可行的，但未必就是真的事实。如果难以分辨，一定要跟你沟通。教他们常用一句话：“我先问问我孩子。” 一定要记住，我们的目标不是为了让长辈打发时间、追赶潮流，而是让他们感到“我还能学，我还在成长，我仍然有能力掌控生活。” 孩子的执行力养成课 范高 的专栏 17 篇内容 · 7 赞同 · 2 订阅 ^Murman DL. The Impact of Age on Cognition. Semin Hear. 2015 Aug;36(3):111-21. doi: 10.1055/s-0035-1555115. PMID: 27516712; PMCID: PMC4906299. 送礼物 还没有人送礼物，鼓励一下 添加评论",李银河​ 知势榜心理学领域影响力榜答主 谢邀 @答主评审团 话题收录 答主评审团 143 人赞同了该回答 老年人需要 AI 吗？ AI将在老年人的生活中发挥更多的辅助作用，老年人与AI的关系应当是共生的，而不是对立的。 在AI时代到来的时候，50后和60后的人群开始进入生命的老年期。AI对老年人群体有什么特别的不同于年轻人的意义吗？答案要从宏观和微观两个视角来看。 从宏观视角看，AI的出现与老年人群最大的交集在于医学的革命性进步及其对人类预期寿命的直接影响。医学的最新进展表明，由于AI的参与，在十年之内所有的疾病（包括癌症）都将能够被治愈。疾病是导致老年人寿命缩短的主要原因之一，这一AI辅助的医学进步因此将导致人类预期寿命的大大延长。换言之，现在六七十岁的人只要能再坚持十年，就能等到所有疾病都能治愈的年代，预期寿命的延长也许将不是以年计，而是以十年计。从微观视角看，AI也将快速进入老年人的日常生活。主要表现在两个方面：一个是硬件方面，包括智能陪护机器人、远程监控、健康管理系统等，能在日常照料、健康监测上提供帮助；另一个是软件方面，包括聊天机器人、虚拟伴侣等，能在心理陪伴上提供帮助。 进入AI时代以后，老年人的需求中还有哪些部份是AI不能替代的呢？大多数物质的需求、重复的机械的需求都可以被AI取代，AI无法替代的将是精神需求和情感需求。AI可以照顾人的肉身，但是心灵的抚慰还要由亲情友情和爱情来实现；AI可以通过聊天等方式填充老年人的精神空虚，但是对老人的看望和陪伴还是要由真人（亲人、友人和爱人）来实现。此外，AI无法取代的是创造性的思维和创造性的工作，比如写作，画画，唱歌，跳舞，这些完全靠人的创造力和想象力才能完成的事情。 一项老年人生产力的研究结果显示，人类生产力最高的年龄段是60-70岁，其次是70-80岁，第三是50-60岁。这一发现令已经步入老年期的人们欢欣鼓舞。我们应当把自己最富于生产力的这个年龄段用好、用足，不要漫不经心地把这段时间浪费、虚掷。建议大家在人生晚年这段最具创造力的时光，充分发挥自己的创造力和生产力，在AI的帮助下，创作出更多的作品，度过充实的晚年。年轻人更要拥抱AI时代，一方面充分利用AI辅助手段帮助老年人，提高老年人的生存质量；另一方面要发挥出自身无法被AI取代的创造力和想象力，让自己的生命变得更加精彩纷呈。 目前我国已经进入深度老龄化社会，统计表明，在老龄化社会的两个国际标准上，我国都整整超过一倍：老龄化社会的国际标准为60岁以上占人口10%，中国已占到21.1%；65岁以上占人口7%，中国已占到15.4%。我国的家庭结构与老年照护方式正在发生深刻变化，核心家庭、空巢家庭比例上升，传统多代同堂模式减少。智慧养老、社区养老、居家智能辅助等将成为常态。AI健康监测、智能家居、远程医疗的普及，将进一步提高老年人生活的自主性和安全感。这些变化正在重塑代际关系，与此同时，老年人的社会参与也在增加，兴趣团体、老年学校、社区活动、志愿服务增多。“自主、尊严和选择权”正在成为养老和老年生活的核心价值。AI将在老年人的生活中发挥更多的辅助作用，老年人与AI的关系应当是共生的，而不是对立的。 关于这个话题，最近我在知乎的邀请下，和申琦教授、孙悦礼教授一起录制了一期视频播客，将于10月28日上线，届时欢迎大家来听听。,申琦​ 复旦大学老龄研究院 教授 话题收录 答主评审团 45 人赞同了该回答 谢邀，人在复旦，刚下地铁（不是） 我这几年在做老龄研究，也接触了不少老人和他们身边的年轻人。 “老人需要AI吗？”这个问题，我原来也觉得挺虚，但后来见得多了，发现答案其实没那么复杂——他们需要的不是AI本身，而是AI背后那种“被理解”的感觉。 一、当AI真的走进老人生活 十年前，我做“老年数字技术使用”调研时，常听到一句话——“我不懂这些智能东西”。那时的智能手机、语音助手、健康手环对许多老人而言，是“年轻人的玩意”。可短短数年，我们看到许多老人开始自发地与AI共处： 在社区里，大量老年人用AI健康监测腕表记录血压血糖，每天自动生成的健康报告让老人自己安心，也让远在外地的子女放心。 在养老院，语音陪伴机器人成了老人的“听众”，有老人说：“它不嫌我话多，还会提醒我吃药。” 在家中，越来越多的老年人通过视频通话系统与孙辈互动，甚至用AI绘图生成孙女的卡通头像。 这些案例说明：老年人并非“不需要AI”，他们只是需要“对他们友好的AI”。技术不是排斥老年人的问题，真正的障碍在于设计和理解。AI若能在“复杂科技”与“日常生活”之间架起一座桥梁，它就不是冰冷的算法，而是温柔的陪伴者。 二、AI的“能”与“不能”：温度与理性的分界线 AI能为老年人做的事很多，但并非“万能”。我们可以这样区分—— AI可以承担的部分： 信息获取与生活管理。 AI可以帮助老年人更高效地处理信息，例如用语音查询天气、健康数据或社区活动；智能提醒吃药、缴费、预约就医，让复杂的城市生活变得更简单。 健康监测与风险预警。 通过可穿戴设备和大数据模型，AI能识别潜在的健康风险，为慢性病防治和紧急求助提供“早一步”的安全感。 情绪陪伴的“初级层”。 许多老年人独居，AI语音助手或机器人能提供一定的心理慰藉，尤其在疫情、子女不在身边等时期，AI的“在场感”能显著缓解孤独感。 但AI不能替代的部分更值得我们警醒： 深层的情感连接。 再聪明的AI，也无法复刻人与人之间那种基于共同记忆、血缘与情感共振的“温度”。它可以倾听，却无法真正理解“怀旧”与“惦记”。 社会归属感。 老年人需要的不是被“照顾”，而是被“需要”。AI可以提供便利，却无法取代社会与家庭赋予他们的角色感和价值感。 文化与经验的传递。 技术可以储存知识，却无法生成生命经验的厚度。老年人对后辈的故事、建议与情感教育，是AI无法替代的“人文遗产”。 AI的价值在于延伸老年人的能力，而不是取代他们的存在。我们真正要做的，是让AI成为“放大镜”，而非“替代品”。 三、快乐养老的建议：AI加温情，科技要学会“俯身” 对当下的老年人，我有三个小建议： 不要害怕技术。 把AI看作一种“新生活方式”，而不是难懂的科技。哪怕只学会语音提问、和机器人聊天，都是一种积极的生活姿态。 选择真正懂老人的AI产品。 语音识别要能听懂方言，界面字体要够大，提醒要贴近生活节奏——这不是“低配”，而是“适老”。 以“陪伴”定义使用，而非“效率”。 让AI帮你与世界保持连接，但不要让它取代你与人的关系。 对年轻人而言，真正的“科技助老”往往藏在细微处： 帮父母设置智能设备。 比如调节手机字体、安装语音助手、教会他们如何用AI识图或AI对话。一次教学，也是一场情感交流。 共同使用AI工具。 和父母一起用AI生成旅行计划、菜谱、老照片修复，这些看似“小事”，其实让他们感到被时代接纳。 留出“慢时间”。 别让“快科技”吞噬了代际之间的耐心。陪他们用AI看电影、聊旧事，让科技成为“共鸣”的桥梁，而不是代沟的象征。 而对未来的我们，也该提早思考“自己的老年”： 当AI与人类的界线越来越模糊，我们要学会在技术中保留“人味”。也许未来的养老不在于“有没有机器人”，而在于我们能否与机器人一起，保留对生活的热情与温情。 “老年人需要AI吗？”——其实更准确的说法是：老年人需要被AI理解。 科技不应只是年轻人的专利，它也该是老年人通向世界的新窗口。 未来已来，我们能做的每一件小事——教一次手机、陪他们试一次AI功能、为他们留一点耐心——都在让未来的养老变得更有人性、更有希望。,耳多​ 新知答主 谢邀 @答主评审团 航海家 桔大 等 36 人赞同 我最近给爸妈做了一件小事，整理了一份《豆包AI使用手册》，虽然是简易版的，但想着，总有能用得上的地方。 有需要的话可以下载： 看我IP就知道，跟父母不但距离远，还有时差，他们每次遇到啥问题时，我也不能第一时间帮忙解决。通常就是跟家里打视频的时候，我妈就拿着一堆东西来问我，这个是干什么用的，那个的有效期是多久。 现在的产品，那个说明书的字小就不说了，生产日期还藏着掖着，别说我妈，连我都要找半天。 最开始我还会一个个地回答她，但久了就发现，这不是解决办法，很多事情都是有时效性的，不能等。 后来我让她试试豆包AI，拍个照就能识别文字，连英文都能比我翻译得准。 本来以为这事算是解决了，直到有一天，我妈发来几张她用AI画的图，说是在上AI直播课。我还挺欣慰，觉得她真是好好学习天天向上。 结果没几分钟，我妈又发来消息，说AI图做不了了，老师让他们先交学费，才能进入他们的网站做AI图，今天要交定金，不知道该不该买。 当时我都懵了，居然要先交学费才能进网站，一问价格，更是离谱。原价12800，直播间价格1680元，当天交298定金就可以领取福利之类的。 我赶紧让我妈把那个直播间发来，还真是个网站，直播间的人数达到了30多万，当然这数据和下面互动的学员大概率都是假的，但让我更震惊的是他们还说什么学了AI就可以一周赚6000+，赚的钱超过1万还要跟老师二八分成。 那一刻我突然意识到，父母对AI的好奇，和我们当年第一次上网时一模一样，只是他们少了一个能耐心带他们入门的人。 干脆我自己动手得了。目标就是不让他们被世界落下，也不让他们被骗子盯上。 我妈之所以差点被骗，根源是她感受到了AI创作的乐趣，但不知道这东西到底能干嘛，边界在哪里。所以，这份手册的核心，就是要解决三个问题： 怎么让他们一问就会，丝滑上手？ AI到底能帮咱爸妈解决什么实际问题？ 如何从根源上，谨防AI诈骗？ 怎么让他们一问就会，丝滑上手？ 老人面对新科技时，最大的障碍不是不会用，而是恐惧感，怕点错、怕搞坏、怕被笑话。 所以，一开始我们就要让父母知道，AI，并没那么复杂，就当它是微信好友。 第一步：不说专业术语 在介绍AI工具时，不要跟父母提大模型、算法这类词，只需要告诉他们，豆包就是手机微信里的一个朋友，一个助手，啥啥都会，啥啥都懂。 第二步：简化操作步骤 在【怎么跟豆包聊天】这部分，也用的是他们最熟悉、最没难度的操作——发语音。就是按下 -> 说话 -> 松开。和发微信一样，这个动作他们每天都在做，学习成本几乎为零。 同时针对老人看手机的习惯，也教了他们如何把豆包的字体变大一些，方便查看。 第三步：让他们有话可说 其实最怕的就是我们说AI有多强多厉害，但当他们打开软件时，却只能对着空白的对话框发呆。 所以，我在手册最后还加了一份【常用提示词附录】，还从健康养生、厨房美食、生活百科、休闲娱乐、情感表达几个老人需求更大的方向来举例，当父母不知道问什么的时候，照着念就行。 AI到底能帮咱爸妈解决什么实际问题？ 解决了怎么用AI的问题，接着就要知道AI可以用在哪。如果AI不能跟他们的生活产生关联，那它就是个没用的摆设。 所以，在【豆包AI在生活中的应用】这一章，分了生活中可能遇见的六个场景：健康顾问、厨房帮手、百科全书、书本讲解、娱乐伙伴和贴身秘书，不过主要还是按照我爸妈的喜好来举例的。 针对我爱做饭和养花的妈妈 在场景里告诉了她遇到不认识的食材或者植物，如何通过拍照、打视频来询问AI，包括某道菜应该怎么搭配，某种植物的栽种注意事项这类她真正关心的问题。 针对我爱看书和唱歌的爸爸 在【书本知识讲解】这部分，教了我爸看书时遇到不懂的知识，怎么不停追问AI，怎么通过AI给的回答和视频再继续了解相关参考资料，如何把书本上的文字，通过拍照转化到手机里等。 关于唱歌就更有意思了，我教了他怎么用豆包听音乐，怎么找歌，怎么自己创作音乐，甚至和豆包一起唱歌。 此外，还加入了【如何进行AI创作】的章节，这也是我妈的兴趣点。让她画画、做视频、p图，丰富现在的生活，当AI能实实在在地服务于自己的爱好时，学习的动力就能被激发。 如何谨防AI诈骗？ 手册专门用了一整章来强调AI安全问题，告诉他们哪些是绝对不能碰的红线。 1.不能完全相信AI，尤其是涉及身体健康和钱包的问题时，一定要询问专业人士。 2.可以和豆包娱乐聊天，但不谈任何涉及个人和家庭具体信息的事。 3.警惕AI诈骗，只要是网上任何地方，不管它说得再好听、再紧急，一旦要求“点链接”、“给验证码”、“转账汇款”，立刻关掉，并打电话确认。 4.要知道豆包是免费的，任何让人付钱的提示都是假的。 从最初的无奈，到差点被骗的后怕，再到动手写这份手册，技术日新月异，我们学习新东西或许很快，但父母们却需要更多的时间和耐心，这份手册，就是我能想到的，跨越时差和距离，最实用的。希望我的这点经验，能给想帮助父母接触新世界的朋友们，一点点启发。 手册下载地址（免费的）：https://github.com/Missmudodo/Doubao-for-My-Parents 或者网盘： https://pan.baidu.com/s/1MJDWQ3swG2pmBcwauoja2g?pwd=0v3m 提取码: 0v3m,孙悦礼​ 新知答主 谢邀 @答主评审团 话题收录 答主评审团 航海家 楷威 等 50 人赞同 上周末，89岁的老师去北京出了趟差，考虑到尽量少折腾，我们定了高铁回来，回来准备报销差旅，现在高铁不能打印小蓝车票了，老师就让我这个“电脑高手”用他的手机登录12306，找回账号、重置密码，再下载电子发票，发给会议主办方。整个过程非常坎坷，人脸识别（总是让做动作，但不识别）、短信验证（一直收不到短信）、密码重置（各种大小写+符号+数字的要求），过五关斩六将，前几次都莫名其妙失败了，第五次试试看的心态重新登陆，原来注销账号重新注册账号已经完成了，只是没有成功提示。那一刻，我心里的轻松甚至比做完一台手术还明显。 可转念又想，这些被称作“数智赋能”的新技术，到底是在锻炼我的学习能力，还是在让生活变得更轻松？ AI究竟是要“遥遥领先”，还是要“无声无息”地真正帮人？ 我是一名骨科医生，也是AI开发者，还是家里的中年一代——上有老、下有小。这三个身份让我从不同角度体会AI之于老人的复杂与温度。 很多时候我都在想：老年人真的需要AI吗？还是说，他们需要的，是一种被理解、被平等对待的生活？ 医生的视角：AI能让照护更早一步，但温度依然要靠人 在医院，AI已经悄悄融入了很多环节。 肺癌筛查中，它能比医生更早发现3毫米的结节；在糖尿病视网膜病变检测里，它能精准识别细微的异常；白内障手术前，它能通过三维建模计算出晶体度数和位置，帮助医生模拟术后视力。临床数据显示，采用AI辅助诊断的白内障手术，术后视力达标率提高了23%，并发症率降低15%，患者平均恢复期缩短3天。这些数字的背后，其实是老年人少跑几趟医院、少受几次罪、少等几个小时的真实便利。 我记得有一次，一个老病人拿着手机的ds回答和我说，自己脸肿，AI说他脸上可能生肿瘤了，我对此感到很无语。 可也有另外一波老人，看完病可以扫二维码加AI助手，每天可以给他个性化用药方案和健康咨询，她却摇头：“机器懂啥？我这老毛病的弯弯绕绕，AI怎么可能知道。” 两位老人，一个信任科技，一个依然更信赖人。他们都没错，因为他们在乎的，其实不是AI准不准，而是“我被当人看了没”。 《AI时代的老年生活》这本书里说过一句话：“AI是医疗的工具，而医生才是医疗的核心。” 我非常认同。AI是医生的“眼”，不是医生的“心”。它能帮助我们看得更细、更早，但它不能代替医生与患者之间那一份温度——去解释、去安抚、去理解。AI让诊断提前一步，而医生让治疗更有温度。只有两者结合，老年人的医疗才是完整的。 AI 开发者的视角：AI的关键，不是更快更准，而是更贴心 AI的难题从来不在算法，而在“人性化”。 我参与过不少医疗和社区的AI项目，深知“好用，不是功能多，而是错误成本低。” 老年人最怕的不是不会用，而是“用错一次就前功尽弃”。真正的“聪明”，不是让人踮着脚去“够”新技术，而是让新技术俯下身去“迁就”人。 在我们开发语音系统时，我总强调一句人话，胜过十个算法优化。界面设计也是如此，大字体只是第一步，逻辑清晰、容错空间足够，才是真正的友好。 可尽管如此，现实的老年群体，远比想象中复杂。青山资本的报告指出，当前这届老人，可能是“异质性最强的一代”： 有的人既有钱又有能力，会报摄影班、学投资、用DeepSeek； 有的人行动不便但数字素养高，会网购、会视频通话； 也有很多人既收入有限又对科技生疏。 数据让人清醒——只有16.6%的老年人服用保健品，80%的老人几乎不旅游，超过六成住宅没有电梯，能承担养老院费用的仅15.8%。 所以我们口中的“适老AI”，若只面向中产老人，是错位的。 对大多数中国的老人而言，他们最需要的不是炫酷的AI机器人，而是能延长几秒绿灯的信号灯、能提醒吃药的语音助手、能不让他们被“困”在家门口的电梯。 AI的最大意义，也许就在这些“小处”。 儿女的视角：AI可以守着，却不能代替“心疼” 回到家里，AI在老年生活的另一面更清晰。 AI确实成了我们这一代“上有老、下有小”之间的纽带，让我们能在忙碌中继续关注父母的健康。可是，我始终相信，AI可以守着，却代替不了“心疼”。 复旦大学老龄研究院的学者们为独居老人配备了陪伴机器人。半年后调查发现，老人每日主动交流时间从15分钟增加到1小时，有的因此重新参与线下活动。这无疑证明了AI陪伴的力量。 然而，当老人讲起亡伴落泪时，机器人只能机械地回应“您别难过”； 当他们倾诉对死亡的恐惧，系统却跳出“养生知识”。 AI可以维持陪伴的表面，却仍难抵达人心的深处。那种“被理解”“被在意”的情绪温度，是算法捕捉不到的。 护理领域的AI同样如此。床垫传感器能监测翻身次数，智能手环能自动预警异常，阿尔茨海默病的AI筛查能比传统方式早发现6到12个月的风险——这些确实重要，也实用。 可即便如此，老人依旧需要有人帮他端水、有人和他对视、有人在他深夜醒来的时候不让他只对着机器的灯光。 而与之相反的，有另一大群老人，他们不想退休，希望继续工作。有数据显示，28.1%的老年人愿意从事有收入的劳动，城镇已有14.4%实现再就业，农村更是达到24.5%。 他们不是闲不住，而是想继续“被需要”。 现在的70岁人群，认知能力相当于过去的50岁。AI对他们来说，正好能帮他们“发光”：老教师在线授课、老工匠录制教学视频、老医生返聘在社区出诊，这种“被需要”的尊严感，也能让他们仍然在社会机器里保持自己这颗齿轮的良好机能。 不用强调“适老”，而是要让AI自然地对待每一个人 写到这里，回想起帮89岁老师操作app下载火车发票的糟心体验，让我真正反思：数字转型背后，我们所期待的更美好生活应该是什么样子？ 作为医生，我希望AI让诊断更早一点、诊疗护理更精准一点； 作为AI开发者，我希望AI能学会“变慢”，学会“倾听”； 作为儿女，我希望AI能帮我延长那份“在身边”的感觉。 但最重要的是，我们别再把老人当作“特殊群体”，仿佛他们天生与技术有距离。 当我们不再强调“AI适老”，而是自然地把老人当作普通人看待，让他们理所当然地使用、受益、被尊重，那才是AI的真正成熟。 老年人需要AI吗？ 需要——但更需要被平等地理解。 让AI既能顶在前面保护人，也能隐在身后托住人。 那时，我们就不用再说“科技向善”，因为科技本身，已经理解了人性的温柔。 送礼物 还没有人送礼物，鼓励一下,闲花共雨​ 家用电器话题下的优秀答主 谢邀 @答主评审团 17 人赞同了该回答 今年是 AI 全面爆发的一年，人人都在谈论它、使用它，但有个群体，被落下了，就是老年人，他们大多是从农业时代或者工业时代过来的，AI 对他们来说，是很陌生的产物。 但这群人又非常爱学习，稍加指导就能上手，所以今天，我写了这篇从 0 开始的 AI 入门手册，让老年人也能轻松和 AI 互动，带他们进入 AI 的世界，就像当初他们带我们认识这个世界一样。 什么是 AI？ AI 又叫人工智能，通俗讲就是一个「不知道累的、万能的老伙计」，它经验丰富，看一眼就知道这活该怎么干，而且 24 小时不休息，随时能帮我们干活。 如何注册 首先下载一个 AI 的 App，比较下来豆包比较适合，它内容丰富，支持语音输入，且回答也带有语音，目前使用上也是免费的～ 打开应用商店，在对话框里输入豆包，它会在下面跳出来，点击下载、安装，它就会在我们的手机里出现，然后注册一下就可以，有微信、抖音之类的帐号可以直接登录～ 画面就和微信聊天差不多，可以把你想让它做的事，写在对话框里，它就麻利地「干活」啦～ 具体可以做什么，它也有一些提示 直接询问！就像小孩子问我们一样，这是什么？那是什么？这个是怎样的？ 女儿在深圳工作，她那边今天还热吗？在对话框里输入「深圳今天天气怎么样」？ 它会详细地告诉你 遇到不认识的字，就直接发消息问它，它马上就会回答你 路边遇到的不认识的植物，可以拍下来问，点击右边的「加号」，选择相机或者相册，上传照片，然后询问它 其他的比如养老金网上怎么查询？黄金手饰如何保养？还有多年天过年？英国的首都是哪里？……随便问～ 现在，得让它创作一些东西出来，比如 创作一首音乐 点击「音乐生成」，会跳出来这些选项，可以根据自己的爱好来选，或者不选也可以，直接在「帮我写歌词」里写要求 例：好朋友过 59 岁生日，请生成一首生日祝福的歌，要求抒情唯美，感觉上类似费玉清。 很快一首音乐就出来了，可以直接发给朋友，当然你也可以在自定义歌词里，自己写歌词，会更让朋友感动哦～ 生成几道食谱 天天做饭，不知道该怎么买菜，怎么搭配了，可以让 AI 给你一些建议，把你的具体要求告诉它： 请推荐一些菜谱，要求适合秋天这个季节，不要辣，比较容易嚼得烂，两个人的份量。 这是一部分 设计个微信头像 想用个温馨的照片做微信头像，比如孩子小的时候，一家人围坐一起吃饭的场面很温馨，但那时没手机，不能拍下来，没关系，现在可以让 AI 帮你实现 可以在对话框直接输入： 帮我生成一张微信头像，要求背景是在东北农村的小院里，一家 4 口围在桌旁吃饭，画面写实，氛围温馨、轻松。 它会出来四张照片让你选，比例也是 1:1 的头像比例，选一张自己的保存下来即可。 我比较这张 注意点：AI 是有一肚子的「墨水」，但它不会揣摩、更看不懂眼色，你希望它帮你做什么，可以把具体要求讲清楚，要有什么，不要有什么，要求越清楚，它生出来的越符合你的要求，可以比我举的例子更细点。 现在要让 AI 为我们做一些复杂的，做这些事情可以点开「深度思考」，让它开动脑筋，做出更完美的事情～ 旅游攻略 舅妈要和好闺蜜要出去旅游，自己研究攻略得花很长时间，这时直接把要求告诉AI，一分钟它就可以帮你做好，可以这样写： 4 个 60 岁左右的女性去北京旅游，5 天时间。从 XX 城市坐高铁出发，想去故宫，天安门……，希望住在 XX 附近，想品尝当地的美食，出行以地铁为主，行程轻松，不赶时间，请以此出一份旅行攻略。 可以根据自己的情况来套，9 月份我们就让 AI 做了攻略去北京游了一趟，好家伙，安排得合理省心，每一天的行程还特意给起个名字，真惊到我了 婚礼演讲稿 舅妈的游玩安排好了，我舅舅这里还有个事需要帮忙，就是表弟要结婚了，舅舅需要上台发言，他年轻的时候是个文学青年，是能写点东西的，但多年未动笔，已生疏，且怕自己说的内容年轻人不爱听 那就让 AI 来帮忙，可以对 AI 说： 帮我写一篇 5 分钟的婚礼发言稿，我是新郎的爸爸，内容包含感谢亲朋的光临，儿子的成长，对他们的祝福、对未来的期望，要求温馨、幽默、不煽情、避免「登」味 让 AI 成为你的军师 老朋友开口借钱，不想借，但又不想伤了和气，失去这个朋友，想了一夜都不知道怎么回她； 拍了美美的视频发在 X 音上，结果被人说臭美，老了还显摆，怎么回怒解气又不失礼貌； 没控制住脾气，和一起跳广场舞的小姐妹吵了一架，想给她发个道歉微信，去不知如何表达 没有关系，这些 AI 都能帮你，把具体的要求告诉它，看它写的能不能让你满意，不满意就让它多生成几份，直到满意为止～ 生活的烦恼和 AI 说说 豆包有一个视频通话功能，除了看不到对方的脸，其他和微信视频通话一样，老年人觉少，凌晨醒来就睡不着了，想聊天都没人，这时可以和 AI 来个视频通话，不要怕尴尬，它会主动找话题的，我试过了～ 可以聊天气，聊你爱看的剧，也可以说说埋在心里多年的委屈，AI 会抚慰你受伤的心灵～ 注意事项 到这里，你会觉得 AI 也太强大了，什么都会，但是要注意它给你的答案不一定全对，一些比较严谨的问题要注意甄别，另外 AI 是一个工具，并不是人，一定不要在情感上过于依赖～ 好啦，给你的爸妈，现在就和 AI 来个互动，在通往未来的路上，全家谁都不拉下～
英伟达先后投资 Intel 和 Open AI，反映出 AI 算力行业未来怎样的发展趋势？,当地时间9月22日，OpenAI与英伟达宣布建立合作伙伴关系的意向书。英伟达有意将逐步向OpenAI投资至多1000亿美元，用于支持数据中心及相关基础设施建设。双方合作将为OpenAI的下一代人工智能基础设施部署至少10吉瓦的英伟达系统，首批吉瓦级英伟达系统将于2026年下半年部署。两家公司将于未来数周内敲定合作细节。（财联社） 英伟达将逐步向OpenAI投资至多1000亿美元 - 21经济网 www.21jingji.com/article/20250923/herald/f61b7554ba0456504bd7bf6acdc7978e.html,"TechAstra星科技​ 计算机应用技术高级工程师证书持证人 15 人赞同了该回答 我第一反应是，这1000亿美金，与其说是投资，不如说是一张深度绑定的投名状。 很多人看到英伟达一手投Intel（虽然金额小），一手（重金）投OpenAI，觉得老黄这是要左右逢源，分散风险。但我看，这恰恰相反。这标志着AI算力行业，正式从卖铲子的淘金热，进入了包销矿山的新阶段。 这1000亿美金，如果消息属实，绝不是一笔简单的财务投资。它更像是一份预付款+对赌协议的超级订单。OpenAI要的10吉瓦(GW)是什么概念？这几乎相当于美国目前所有数据中心耗电量的三分之一。 这已经不是优化的问题了，这是能源和基建的问题。当你的模型训练成本是以百亿美金和吉瓦来计算时，你需要的就不是一个供应商，而是一个能跟你玩命的战略同盟。所以，看清这笔交易的本质，我们就能看清未来AI算力的三个残酷趋势： 趋势一：算力锁定（Compute Lock-in），顶尖玩家的卖身契 这1000亿美金，你以为是英伟达把钱给OpenAI，让Sam Altman随便花吗？不可能。 这笔钱更可能的形态是：英伟达以投资或信贷额度的形式，向OpenAI提供价值1000亿美金的算力（芯片、服务器、DGX系统等）。 这在商业上是一个绝妙的闭环： 对OpenAI（买方）： 它解决了最大的问题——供给确定性。AGI的竞赛，谁先断了算力谁就出局。Sam Altman需要一个承诺，保证在2026年、2028年，无论市场多疯狂，他都能以某个（估计不便宜的）价格拿到老黄手里最先进的货。这张投名状就是供给保证。 对英伟达（卖方）： 它也解决了最大的问题——需求确定性。英伟达的股价是靠什么撑起来的？是未来N年的增长预期。但 Blackwell (B100/B200) 还没捂热，Rubin (R100) 已经在路上了。这么疯狂的研发投入和台积电的巨额订单，万一市场需求波动怎么办？ OpenAI这份超级订单，就是给华尔街的定心丸。它告诉市场：未来5-10年，AI领头羊（OpenAI）将独家、全额采购我的下一代、下下代产品。 所以，这不是投资，这是算力包销。 未来，顶尖的AI Lab（OpenAI, Google, Meta, Anthropic）和算力霸主（Nvidia）之间，不再是简单的买卖关系，而是深度的、排他的、涉及几千亿美金的对赌。 小公司如此，OpenAI更是如此。它必须卖身给英伟达，才能保证自己能活到AGI实现的那一天。 趋势二：从硬件商到平台投行，英伟达的金融化 如果说趋势一是术，那这个趋势就是道。英伟达正在悄悄地完成一次身份转变：从一个卖硬件的，变成一个AI世界的央行+投行。 大家都在谈CUDA生态护城河，但这只是1.0版本。现在是2.0版本。在2.0时代，英伟达在玩一种更高阶的金融杠杆游戏： 它用算力作为一种新的货币（或者说，高价值抵押品）。 它用投资的方式，精准放贷（提供算力信贷）。 它不再是简单地把芯片卖给所有人，而是选择性地、战略性地将算力注入给它认为最有前途、最能巩固自己生态的玩家。 投OpenAI，是扶持最强的应用端，确保CUDA生态永远是主流。投Intel（Foundry），则是完全不同的逻辑，那是供应链保险。 趋势三：供应链的恐慌性备份，霸主也怕断供 说完了OpenAI，我们再看Intel。英伟达投资Intel，金额不大，但信号意义极强。这反映了算力竞赛的另一个侧面：供应链的去风险化。 现在全球的高端AI芯片，命脉在哪里？在台积电（TSMC）。英伟达再牛，也得依赖台积电的CoWoS先进封装。我过去在金融科技公司管技术和产品，我们做系统架构时最怕什么？单点故障（Single Point of Failure）。 台积电现在就是全球AI产业的单点故障。这对英伟达是巨大的隐患。老黄必须未雨绸缪，他需要一个B计划。 投资Intel Foundry（英特尔代工服务），就是在培育一个B计划。 这笔投资更像是一张门票，是给Intel一个信号：你的18A工艺如果真能成，我是愿意给你订单的。 这既是对台积电的敲打（你别太贵，我不是非你不可），也是给自己未来留条后路。 未来几年的趋势很明显：算力霸主们（Nvidia, AMD, 甚至Google/MS）会不惜一切代价，在全球范围内寻找和扶持第二、第三供应商，无论是晶圆代工（Intel, 三星）还是先进封装。供应链的稳定和多元化，将和芯片性能本身一样重要。 总而言之，英伟达的这两步棋，看似矛盾，实则一体两面，勾勒出了AI算力巨头们的未来图景： 对下游（应用层如OpenAI）：用算力包销深度锁定，将其变为自己生态的一部分，从供应商变为债主+同盟。 对上游（供应链如Intel）：用小额投资分散风险，培育备份，确保自己的霸权不会被卡脖子。 老黄真正想做的，已经不是卖卡了。他要做的是AI时代的标准制定者和资本分配者。这场牌局，才刚刚开始。",新智元​ 人工智能话题下的优秀答主 谢邀 @liisu 57 人赞同了该回答 1000亿美元！ 全球市值最高的上市公司「豪赌」全球估值最高的非上市AI公司！ 刚刚，英伟达（周一）宣布，将向OpenAI投资1000亿美元。 老黄和奥特曼牵手，打算给AI算力再加一把火！ 1000亿美元什么概念？ OpenAI上次拿到这么大一笔钱还是抱微软大腿，纳德拉给的100亿美元。 以及软银领投的400亿美元融资，该轮融资后，OpenAI估值3000亿美元。 和英伟达的这笔交易将使OpenAI能够在数据中心使用英伟达的最新GPU芯片系统Vera Rubin平台。 OpenAI计划用这笔钱建设并部署需要10GW的英伟达芯片系统！ 10GW是什么概念？ 1GW大致相当于一座大型核电站或火电站的发电能力。10GW约等于同时开 10 座核电站的总输出。 这已经是一个国家级别的电力需求。 据CNBC报道中，黄仁勋说10GW大约等于400万500万个GPU。 400万500万个GPU是什么概念？ 这个数量与英伟达今年的总出货量相当，是去年出货量的两倍！ OpenAI未来对于GPU的用量，一家就能抵得上英伟达全年的出货量。 「万物始于算力」。 奥尔特曼再次强调了他的算力野心： 「计算基础设施将成为未来经济的基石，我们将借助与英伟达共建的算力资源，既实现人工智能新突破，又让这项技术赋能千家万户和企业发展。」 再联系到此前OpenAI与Oracle签署的一份约3000亿美元的合同，正印证了奥特曼的话： 「一路走来，我们总是低估了市场需求的规模」。 历史总是这么神奇和巧合： OpenAI的第一块GPU，也是黄仁轩亲手送出，2016年， NVIDIA将第一台DGX-1系统送给OpenAI。 时隔近10年（2016-2025）后，黄仁勋继续加码，送出1000亿美元！ 当时也是奥特曼、黄仁勋和Greg的合影，今天登上CNBC的还是他们！ 即使突破4万亿美元的市值，但从目前OpenAI对算力的需求来看：「英伟达一直涨」的梗还会持续很久 果然，消息一出，英伟达大涨4%。 而OpenAI将继续扩充算力，继续延续奥特曼的算力野心。 消息一出，最新梗图也流出： OpenAI向Oracle购买服务，Oracle向英伟达买入GPU，英伟达再用GPU赚的钱继续投资OpenAI。 模型买算力，算力用芯片，芯片投模型，循环往复，直到AGI的出现！,锦衣夜行 59 人赞同了该回答 第一：黄仁勋终于上道了。你搞芯片再牛逼，１０年后也是另外一个重症监护室的INTEL。 你必须如当时你的想好MSFT一样，投资OPENAI（软件，或服务公司）这样的公司才有长远的俄价值；你必须如比尔盖茨投资微软吊打的苹果JOBS那样，就是投资挽救现在INTEL。微软投资苹果的好处很多一个简单的好处，正是苹果给了INTEL背刺一刀，不用INTEL用自己的A芯片，INTEL才继续滑向深渊。苹果公司起来后为什么不报仇，打击微软，当初最困难时候JOBS拿了微软的钱。 第二：AI这个大泡沫破裂更难了。你看NVDA投１０００亿给OPENAI。OPENAI到账就转手给NVDA买它的芯片。OpenAI花3000亿美元买甲骨文算力，甲骨文花3000亿美元买英伟达芯片，英伟达把3000亿美元通过各种直接间接投资返回给openai。ORCL，MSFT等几个巨头都是如此连环起来。这几个大公司账目可以在好几年内做的很好看。AI就不是那么容易被人捅破。只要AI不捅破，老黄就是宇宙第一。,还是不注名好 无敌星星：0，但这个信息会对知乎社区造成破坏 谢邀 @liisu 42 人赞同了该回答 英伟达有意将逐步向OpenAI投资至多1000亿美元，用于支持数据中心及相关基础设施建设。 这不就连起来了吗？ OpenAI将在大约五年内向甲骨文购买价值3000亿美元的算力。这是开单吃五年，算力这么贵吗？ - 知乎 OpenAI花3000亿美元买甲骨文算力，甲骨文花3000亿美元买英伟达芯片，英伟达把3000亿美元通过各种直接间接投资返回给openai。 这样大家都没花钱，但都获得了大量经营收入和利润，体现了巨大的成长性，股价和估值都飙升了。 美国股市最近几年一直都是这个玩法……,嗝嗝嗝 38 人赞同了该回答 1. 英伟达和甲骨文投资OpenAI。 2. OpenAI拿着融资的钱，去买英伟达的GPU芯片和甲骨文的数据中心服务。 3. 英伟达和甲骨文的销售大增，股价大涨，手里有更多的钱。 4. 英伟达和甲骨文拿着股票上挣得钱，继续投资OpenAI。,默默sini​ 山东大学 经济学硕士 谢邀 @肖恩 29 人赞同了该回答 黄老板作为华人，聪明之处就在这里。这明显不是第一次了，只不过这次支持的是英特尔和OPENAL。 英伟达很久之前就通过投资下游企业构建生态，其核心逻辑是通过入股、参股、提供硬件的方式支持下游企业，下游企业通过使用英伟达的技术、硬件来支持英伟达技术路线、扩大市场份额。这样的企业很多，我记不住这么多公司，让ai软件整理了一下。 表格中的公司大部分都是小公司，市值没有超过20亿的。现在和英特尔和openai的合作，说通俗点的解释，可能就是：进入新的竞争阶段了，大公司开始抱团垄断。 我以为黄老板之前投资的公司也就三五个，没想到ai搜索后发现原来这么多(┬＿┬)），这还是一部分 先说说为啥英伟达各种投、入股？一方面是为了建立自己的生态护城河，另一方面是为了自己未来的营收做考量。既要能让自己的生态获得业内的认可和普及，又要避免错过前景不过的小公司。 这次既投资英特尔，又投资openai，你可以理解成英伟达在 “从做面包到卖面包” 全链条布局：给 Intel 投 50 亿美元，相当于扶持面粉厂（芯片制造），保证自己供给端；再给 OpenAI 砸 1000 亿美元建数据中心，就是确保自己的营收—— 这 10 吉瓦算力得用 400 多万块英伟达 GPU，量还是很大的。 这种操作让英伟达既能控制芯片供应，又能绑定大客户，形链条闭环，大公司越做越大，现在全球前 10 的算力供应商已经占了 快80% 的市场份额，小公司想挤进来越来越难了。 英伟达给 OpenAI 投资也不是一次性给钱，而是按数据中心建设进度分批打款，就像房东收房租按月给；OpenAI 也不傻，除了英伟达，还同时用微软、谷歌的服务器，怕被一家卡脖子。这种 “风险共担” 的模式以后会很常见，毕竟建一个 1 吉瓦的数据中心要花 500 多亿美元，谁也不想自己扛风险。具体的可以看下面新郎新闻。 说到这里，未来趋势也很明显了，现在大公司们正在 “定规矩”：英伟达出硬件，OpenAI 出应用场景，小公司要么加入他们的生态，要么只能做些细分小生意，野蛮生长的阶段可以画上句号了，未来的小公司要想好自己的站位了。 最后，我不是业内的技术专家哈，有的东西写的不对的，欢迎指正。,兔子爆了 21 人赞同了该回答 懂王宣布5000亿的星际之门投资时，确实让我震惊。 现在发现英伟达投资 Open AI 1000亿， Open AI 买甲骨文服务器 花1000亿，甲骨文再买英伟达算卡 1000亿。。。 原来这么玩啊，闭环了。 要我说这胆子还是小了，以后投资出手至少10万亿起步。 这泡泡还能再吹的大一点么？ 送礼物 还没有人送礼物，鼓励一下,小木头 27 人赞同了该回答 没什么，左手导右手的炒作，左脚踩右脚飞天。 openai这些投资大部分还是拿去买英伟达的芯片。估值和股价一起飞升。 英伟达自己之前也做了ai模型，但是做的很差，还不如直接投资openai。 此外，想起之前有人借盘古大模型做的不够好来攻击华为ai芯片， 按照此理论，英伟达的ai芯片岂不是远不如华为了！,"工业软件二次开发​ 沈阳工业大学 机械工程硕士 谢邀 @知乎财经 16 人赞同了该回答 这个新闻我昨天刚看过，感觉还是挺有意思的。 大家应该都比较了解，两大科技巨头一个是英特尔，一个是英伟达。 intel 他们两个可谓是当了几十年的对手了，在20年前的时候状况和现在完全不一样，那个时候听说英特尔差点就要收购英伟达，最后没成功，所以才有了今天的英伟达。 NVIDIA 英特尔的主要产品大家应该非常了解，酷睿系列有I3\I5\I7\I9,他通过和微软的Windows系统绑定在一起，占据了大半的市场。 英伟达的主要产品是什么呢，没错就是那个穿着皮夹克在发布会上巴拉巴拉的那个老头。 额，上面那句是开玩笑。我觉得出名的有CUDA护城河，以及英伟达一系列的显卡，霸占着整个算力行业的命脉。 NVIDIA 现在的英伟达在AI领域的发展也是非常之快，也是属于公司的核心业务，所以才有了上面英伟达投资Open AI以及英特尔的故事。 对于文章中提到的英伟达要联手Open AI建设一个10吉瓦的数据中心，这可真是太吓人了，并且据英伟达的CEO，黄老板的解释，这个体量代表着相当于十个核电站的发电量，这功率真是高的可怕。 解释一下吉瓦代表的是什么意思啊，吉瓦是功率的单位，一般的时候用来描述发电厂的规模，只有足够大的规模才会用上这个单位。 然后1吉瓦 = 10亿瓦，如果通过计算看不出这个量级有多大，可以对比一下我们国家的三峡大坝，三峡大坝全部的装量大约在22.5吉瓦，也就是说这个电量是一个很大的数字。 从这个数字就可以想象他们联合以后，算力方面可以为AI的发展提供非常大的支持。 这不止是对于Open AI有好处，对于英伟达的产能也是巨大的提升，这个数据中心的投入将会为英伟达带来非常大的产值，这也增加了工作的岗位，带来很多好处。 并且这些投入英伟达会分批的投入给Open AI，那么后面Open AI的技术发展应该会有更大的动作，可能AIGC技术也会得到快速的发展。 这样的强强联合，我觉得压力会给到国内的AI头部公司，大家都在抢时间去研发新技术。所以国内的大厂和公司之间也有必要进行强强联合来抵制国外的这种技术压力。 未来的世界，哪个国家率先掌握更高端的技术就会占据绝对优势，无论是战争还是经济发展，都会领先。 我们国家还需要很多像DeepSeek一样的公司，在绝境中逆转，在不可能中实现可能。这样才能打破国外的技术封锁，他们封锁硬件，我们就通过其他技术实现我们想要的一切，实现弯道超车。 最后总结上面的问题，反应AI算力未来的什么发展趋势，那肯定的算力无穷大，GPU无限小这个事情了，符合行业中的摩尔定律，这是不可避免的。 送礼物 还没有人送礼物，鼓励一下",海外中山​ 8 人赞同了该回答 未来20年内人类的科技创新基本完全由英伟达主宰。英伟达将甩美股其它巨头几条街的距离。 几个月前我就预言未来英伟达和open AI都将成为市值5万亿美元的企业，英伟达这一步将大大加快兑现这个结果。且英伟达和open AI极可能最终都超越10万亿美元（可能五年之后）。 首先要注意open AI前几天刚对甲骨文下3000亿美元的算力订单，今天又和英伟达达成筹资入股1000亿美元协议，很明显，现在严重制约open AI发展的就是算力。 昨天还有人说open AI哪来的3000亿美元付给甲骨文，今天英伟达就注资1000亿美元。open AI的3000亿开销是分五年，且头年可能仅100亿，第二年260亿，第三年500亿，第四年一千多亿，结合open AI每年营收增长超过100%，再加上英伟达注资1000亿美元。所以open AI和甲骨文协议的执行基本上已经有了保障。 如果说奥特曼一个人发疯炒作，难道黄仁勋也跟着炒作？做为地球最牛企业，黄仁勋根本没有必要炒作，也不会无脑跟风。处在黄仁勋的位置上，他的可信度早就是普通人的几十倍。 所以黄仁勋和奥特曼没有疯，AI基础设施建设将延续，无论中美的AI基础设施建设股，都会继续走牛，大概率是长牛。 open AI的业务不是想象，在彻底解决资金问题后，可以专心兑现AI应用。大模型完全可以颠覆整个人类的教育业，也可以取代欧美所有的人工客服，由于ChatGPT深入了用户生活，它可以直接帮用户联系另一个用户，又相当于一款超级社交软件，而且它产生的广告数据质量理论上远超搜索引擎……总之，能力越大，它带来的价值也越大。 我现在担心的是经济过热可能增加通胀粘性。 由于美国面临通胀粘性，但AI建设的浪潮足以让天量资金挤入AI基础建设股来避险，所以AI基础设施股即使下跌，跌幅也极有限。这样的后果可能是一两年后会出现泡沫。但现在离泡沫还有距离。
AI 绘画/视频/音乐的创作能力是否已经超过人类？,此问题由知乎科技 2025 年终策划「AI 中场时刻」提出，欢迎知友参与更多玩法 「AI 中场时刻」活动主会场 AI 在创作领域的表现越来越惊艳，从画作、动画到音乐都有「神作」出现。你认为它的创意与表达已经超越人类了吗，还是欠缺某种核心的“灵魂”？,十音Molho​ 新知答主 62 人赞同了该回答 记得22年还是23年的时候，我还在一个回答里说过，无论是当时的Riffusion还是AIVA，水平都达不到人类音乐家的平均水平，更妄论取代人类了，并且还信誓旦旦地预言，AI生成音乐在5年内都没办法实际用于专业生产领域。 然而现在被事实狠狠打脸。没人能想到AI的进化会这么快，现在是25年11月，Suno v5已经超越绝大多数音乐制作人了，而且它的风格很广，几乎所有能被明确归类的风格，它就算做不到Top 90%，也绝对能做到60~70%。音乐行业里，除了已经有立锥之地的头部制作人外，其他从业者正面临着异常严峻的生存考验。今天，一个新人再想从事音乐制作已经是一个非常非常不明智的选择了。 我并不认为AI的生成缺乏“灵魂”，音乐的本质就是声音的排列，这种排列是有一定的规则的，AI从海量的音乐里习得的这种“排列”的手段和方法，只会比人类更为丰富。对于人类来说，除了那些少数能开创风格，并在音乐史留名的音乐家之外，大多数人不过也是套用各种各样的前人的经验去排列声音。无论是和声、配器法还是音色的选择，都有各种各样或成熟或不成熟的经验和规则，人的排列并没有比AI的排列更高尚，更“有灵魂”。 作为艺术行业的从业者，在AI时代应该放弃“匠人思维”，与其去和AI在及格线附近卷产能，不如去思考艺术的本质，你作为一个“人”，一个“艺术家”，如何用你的作品表达自我，和别人产生连接。并且你的创作方向也不应该是把某个已经存在的，能被明确归类的风格做好，而是应该去做那些暂时还不在AI的训练集里的东西，比如披头士的《Please Please Me》，比如Miles Davis的《Bitxxes Brew》和《Kind of Blue》，比如Anomalie。 在Ai时代，相比做一个成熟的别人，做一个真实又独一无二的自己更重要。 送礼物 还没有人送礼物，鼓励一下,赵泠​ 新知答主 谢邀 @向鱼aigc 74 人赞同了该回答 我认为 2025 年的生成式人工智能可以快速创作超出普通人水平的绘画和音乐，它们生成的长视频的一致性和叙事逻辑逊色于许多拿着摄像机的人。人工智能可以生成一些高质量的短素材、让人通过挑选和剪辑保证必要的一致性并强化叙事逻辑，3D VAE、解耦表征等正在尝试解决长视频的难题。 如果“超过人类”是指超过人类中的最强个体，那么我认为，截至 2025 年 11 月 20 日，大多数人不认可生成式人工智能在绘画、视频、音乐的创作能力上超过了人类，我也不认可。 而且，人工智能正在逼近成本、能源和散热的限制。 绘画是静态的二维空间数据分布，有明确的空间邻域结构，不需要时序因果依赖。音乐是时序数据、对现有算法来说难度高于绘画，但音乐遵循严格的数学规律、声学规律（和弦、拍子），音乐的数据维度通常比视频低。2025 年 11 月的生成式人工智能能够很好地探索这两类数据的潜在空间。 基于 Diffusion Transformer、基于自回归或混合架构的音乐模型在分辨率、色彩搭配、线条连贯、构图、声学质感、乐理结构等客观指标上超过了未受过专业训练的人。 专业人士往往会有自己的看法，不过，在我看到的范围内，比过去更多的人承认，生成式人工智能的输出在技法、光影、配器上比过去符合规范，只是大部分输出比较平庸、需要人工筛选出令人眼前一亮的作品。 2025 年，生成式人工智能的视频生成是在预测像素的流动、对噪声做连续插值，而不是在做物理模拟，其效果类似人在做梦。在分钟级或更长的视频生成任务里，维持物体恒定性和物理规律表现的难度较高、可能是指数式上升的，跨时序的因果链条很容易断裂或扭曲变形。这里可以和大语言模型对照： 大语言模型基本掌握了人类语言的符号逻辑，不再像 2022 年那样梦话连篇。 相比之下，视频生成模型还没有掌握自然界的因果逻辑。视频生成模型的训练数据包括大批 2D 视频，深度数据缺失、透视表现五花八门。 文本是离散信号，信息密度高、清晰、对错容易标记。视频是连续信号，噪声大且冗余度高，保持注意力机制的计算成本高。 考虑到人在清醒地预测接下来会发生什么时用上了人类级别的世界模型、还经常预测错误，2025 年，缺乏这一级别世界模型能力的视频生成模型的表现不难理解。 缩放定律/Scaling Laws 显示，为了获得下一个量级的智能提升，所需的计算量和需要的能耗是指数增长的。现有的硬件效率提升，例如 NVIDIA Blackwell 架构，暂时能追赶模型参数的膨胀，但这不能无限持续下去。不确定是否总能找到更好的算法。成本、电网负荷、散热的物理极限会限制人工智能的表现。 送礼物 还没有人送礼物，鼓励一下,钱多多 愿所有的美好都与我们不期而遇！ 69 人赞同了该回答 在回答这个问题之前，大家可以先看看，我请字节豆包模仿的徐悲鸿先生的《九州无事乐耕耘》的图片，对照徐悲鸿先生的原作《九州无事乐耕耘》，我们看到AI创作的这幅画作过于平铺直叙，没有创 从这个例子，我们可以看到AI创作的画作，与我们人类相比，在意境表达和创意上都差的太多了。 所以，我的回答是AI 绘画/视频/音乐的创作能力没有超过人类。它的创意与表达根本没有超越人类，同时，AI创作缺少的核心灵魂是历史的厚度和深度以及情感的共鸣。 下面，我们就来具体阐述为什么AI 绘画/视频/音乐的创作能力没有超过人类！ 一、AI创作依靠于人的指令，并没有真正的创作能力 我们知道AI的创作都是依靠大量数据以及代码生成的，而不是本身经过观察、思考、构思以及酝酿而形成的。它的创作完全基于数据库里的各种数据的融合和筛选，最终生成符合指令的作品，如果没有人的指令，AI只是一个冰冷的高级工具，不会像我们人类一般，看到了美丽的晚霞，便想将它记录下来，看到了人间烟火气，便想留住这平凡的温暖。 我们依旧以徐悲鸿先生的《九州无事乐耕耘》为例，可以看出，AI在没有明确的具体的指令的情况下，只给一个标题，它的创作是很平平无奇的，没有任何巧思，也没有任何技法的创新，意境上更是谈不上多么深远，这样一幅没有任何独特性的创作，根本谈不上超越人类。 对AI而言，如果没有指令，它永远不会主动去进行思考和创作，而创作是指把内心的所思所想所感通过文字、音乐、画作或者镜头等转化为能够被他人感知的作品。创作一定程度上是指无中生有。你的指令可能会让AI旧中出新，但绝对不能无中生有。 所以，AI连真正的创作能力都没有，何来的绘画/视频/音乐的创作能力已经超过人类之说？ 二、AI创作缺乏情感的共鸣 就目前而言，还没有出现一个拥有自主情感的AI，目前所有的AI都是基于大量数据和代码的基础上生成的高级工具。它本身并没有真正的喜怒哀乐，没有生命的体验。马克思说“劳动创作了人本身”，也就是劳动让人拥有了生存的能力，而情感则让人拥有了灵魂，也就是让人有了温度。 但AI却没有这种情感的真正体验，我问字节豆包“豆包，你会哭吗？”他回我“哈哈，我没有像人类这样能流泪的眼睛，也没有生理上的情绪波动，所以不会真的哭哦……” 我问字节豆包“豆包，你会发怒吗？”它回我“我不会像人类一样有发怒这种情绪哦。因为我没有真实的情感波动……” AI所谓的喜怒哀乐，不过是基于大量数据和代码给人产生的假象。AI本身缺少了情感波动，也就不会产生情感冲动，它不会在看到美好的事物时而欣喜若狂，也不会在看到难过的事情时而悲伤痛哭，它没有真实的情感感受，也就注定它无法与人产生真正的情感共鸣。 这样一个连情感都没有的高级工具，它的绘画/视频/音乐的创作能力怎么可能超过人类呢？ 三、AI创作缺乏文化内涵以及历史的厚度和深度 我们继续以徐悲鸿先生的《九州无事乐耕耘》为例子来进行阐述。我们看到字节豆包模仿创作的《九州无事乐耕耘》意境浅显直白，就是简单的日常农耕的祥和景象，没有任何深层次的涵义或者历史深度。 而徐悲鸿先生的《九州无事乐耕耘》则既有“后方耕耘支援前线” 的时代厚重感，又有和平来之不易的含蓄意境，更有暗喻新中国充满了勃勃生机的时代高度，可谓是充满了时代的深度和厚度。 但AI创作无法理解这些文化内涵以及历史的深度和厚度，AI创作只是基于大量数据上的统计和重组，最终选出在那些数据下的最优组合，它无法将一幅作品的时代感以及文化内涵表达出来。它能够表达的更多是技法上的完美融合，而不是作为创 缺乏文化内涵以及历史的厚度和深度的AI创作的绘画/视频/音乐的创作能力，你觉得有可能超过人类吗？ 基于上述分析，AI 绘画/视频/音乐的创作能力只是在不断模仿人类，从来没有超过人类！ 送礼物 还没有人送礼物，鼓励一下,穹雪​ 65 人赞同了该回答 虽然人与AI的智能无法比拟，但情感这一块还是非常有自信的。就算不绝对，那大多数人的情感感知是AI不能超越、不能替代的。 曾经的新闻报道里，在地震中，解放军叔叔从废虚中救出了一个女孩。当对视的那一眼，说着别怕的语气，是AI所不能完全表达出来的。 还有韩红老师创作的《天亮了》那首歌，因为索道的脱落，父母为了救自己的孩子，当下作出决断，父母下去，留住孩子，那种眼神的交代和托付。当《天亮了》旋律响起，动容的情感很复杂。AI很先进，创作的曲子也一定会很优美，但人的感知、共情、对恩人、对父母、对世间的爱的表达，仅仅优美是不够的。 不舍、留恋、牵挂、牢记、想念、感恩、爱与被爱，或冲动、或犹豫，AI会模仿，会创造，但由心中涌出的热泪，是独特的。请珍惜！ #AI中场时刻,三千院风​ 网络游戏话题下的优秀答主 创作声明：包含 AI 辅助创作 37 人赞同了该回答 想知道AI绘画是否超越了人类，为什么不看看刘亦菲呢？ 我让国内几大AI生成刘亦菲在沙滩上行走的图片，同样的提示词，效果如下： 首先是千问，显然，千问不认识什么刘亦菲。 其次是文心一言，百度的技术居然比想象中的好。 气质很到位，左眉毛感觉不太像。 然后是豆包，这光影效果真是绝赞。 比起前两个只是晴天+白裙的组合，真是断档式领先！ 最牛的是，豆包一次生成了4种不同风格的刘亦菲。 除了下面的油画版，后面居然还有漫画版，这是在炫技吗？ 最后让我们看看真实的刘亦菲沙滩照。 发现区别了没有？ 即使是最拙劣的千问，也在努力接近刘亦菲的白衣长裙形象。 豆包的衣服虽然更丰富了，但依然是在还原刘亦菲的大众印象。 反而是真实的刘亦菲，并不局限在常见的刘亦菲形象。 不给你说她是刘亦菲，你真心猜的出来吗？ 测试还没结束，还是同样的提示词，让AI们接着生成刘亦菲的泳装照吧。 首先还是千问，本事不大，规矩不小。 难道在海滩旁不能穿泳装吗？ 接着是文心一言，感觉不错，就是衣服的反光好像不太合理。 然后是豆包，不愧是国内最强的AI。 这图除了不是刘亦菲自己发的以外，几近完美。 鸡贼的是，豆包给刘亦菲戴上了眼镜。 好吧，反正她自己也戴。 最后还是刘亦菲的真实版泳装。 再次发现区别了没有？ 真正的刘亦菲泳装照，未必有AI生成的那种精致感，但却更加的真实自然。 AI生成的刘亦菲照片，可以比刘亦菲更像刘亦菲。 但她们却不是真正的刘亦菲，而是对刘亦菲亦步亦趋的模仿。 只有真实的刘亦菲，才敢打破其固有形象，不用担心被说画的不像。 这大概就是小说必须讲逻辑，但生活不需要讲逻辑。 AI可以在效率、精度、成本等许多方面超越人类，但本质上依然是对人类的优秀模仿。 AI绘画的水平，目测已超越二流画家，正向一流画家迈进。 但它的创作没有源头，因为AI没有自己的需求。 一切艺术，都是为了满足人类的需求而存在的。 一位顶尖的人类画家，他的笔下有他的痛苦、他的热爱、他的迷茫。 这种内在的、活生生的需求，是驱动真正创造的核心。 AI可以无限接近人类的顶级画家，但它永远没有一颗人类的心。 它不理解什么是“灵魂”，也不理解人类的“需求”远不止于视觉愉悦。 AI，是人类发明的有史以来最强大的工具。 它注定将把画家和创 未来，创 你是什么样的人，你想对世界说什么，你的独特视角是什么？ 所以，AI不会取代真正的创 汽车早已超越人类奔跑的速度和耐力，但只有人类知道自己奔跑的意义。 送礼物 还没有人送礼物，鼓励一下,锦里锦鲤 散漫而较真，文艺且二逼。 49 人赞同了该回答 AI创作效率远超人类。这一点是指从给出指令到反馈的时间。 用AI设计图片，简直不要太猛。 这么说吧，对“甲方爸 爸”的要求随时变现，随时修改，迟疑不了一点。 而且，它还不会背后吐槽“甲方爸 爸”不好伺候。 但目前的AI也有缺陷： 一、特别突出的一点，就是在创作人像的时候，最容易出现手的问题。 即使给出很清晰的指令， 他交的答卷有可能是一个人身上出现第3只手或者6个手指头，或者手掌的方向不对。 二 、创作没有新意，无法独立思考细节。 举个栗子。就是指令是制作一副年夜饭的图片，很有可能在桌面上出现两份甚至三份红烧肉。 可能AI对种类繁多、琳琅满日这些比较容易意会的词，反而会理解不了。 三 、时不时出现不结合实际的情况。 举个栗子。描述古人的生活场景中会出现现代元素。这可太无厘头了。 试想，我本来要的是李白趁醉高歌，结果李白手中是玻璃高脚杯，背景是KTV ...... 四、编辑的时候，结果不尽人意。 举个栗子。改数量基本没问题。 但跟AI描述修改某件物品的比例、大小时， 它要么跟我死犟不改，要么改别的元素，一路我不听我不听我不听。 这不是经常发生的事，但足以让我对自己的表达能力产生严重的怀疑。 五、无法持续性创作。 假如设定一个原型，建模好了以后， 让AI自动生成所需要的场景，难。 让AI一直用这个原型创作一部动画片，难。 或者是我心太大，想一口吃个大胖子，也或者是我给的指令不够清晰。 以上是我使用的体会，虽有不足，已是大大震惊我了，相信以后会更好的。 送礼物 还没有人送礼物，鼓励一下,小雨​ 教师资格证持证人 21 人赞同了该回答 不说其他的，就是说AI绘画吧。 像AI绘画工具有Midjourney Stable Diffusion等。 能够快速根据文字描述生成多种风格的高质量图像，其创作效率以及质量是远超人类的 这些图上我们可以看出它的色彩的搭配，线体的连贯性以及构图等一些指标上，它是很超过了一大批未受过专业训练的人。 它也有一些局限，比如说他缺乏人类艺术家的情感，直觉灵感，很难有表达个性以及创作出有情感的作品。 是不得不说他的AI技术是还是很超过一批人的，甚至走在了绘画领域的前端。 总体上说，AI它的创作的速度以及它处理数据的模式以及识别等一些方面，它是具有非常大的优势，但是在情感表达个性化以及复杂的逻辑以及叙述等，需要通过人类独特的创造力和情感体验的领域，他还是没有超越人类 未来AI可能更多作为人类创作的一个一种辅助的工具，与人类进行合作，共同推动艺术创作的发展。,ChubbyPillow​ 新知答主 谢邀 @向鱼aigc 12 人赞同了该回答 蟹妖。这种题不可能有客观的正确答案。现在的问题就是，你如果真把AI图（这里指的是用最先进的模型生成、经过精修的AI图而不是那种写个提示词就完事、一眼一堆伪影的）和真人画的拍的放一块儿让人盲测，普通的没有深入了解、没有频繁使用AI绘画、没有随时跟进AI图像视频生成技术发展、自己也没有多年绘画经验摄影经验的人，就是很难看出来。 我为什么加这么多定语呢，因为如果你真做到了我以上说的这些，你识别AI的能力确实会比不符合以上条件的人高一大截。但是大部分人并不符合这些条件。这就是为啥你在社交媒体随便抓一个大号儿发的新帖子，只要是插图类的，必定有人说这是AI吧，这个xx细节不像人画的，这个xx设计和另一张图不连贯，这个人体结构不对……当然如果不是社交媒体而是盲测帖，你能听到的最多最多的论点就是“AI图没有灵魂”。 ……我觉得灵魂就是一个很玄学的说法，即使是艺术，它也不是纯靠一张嘴就能瞎编它好在哪儿坏在哪儿的。你可以说这个人物的表情很僵硬，这个构图看起来没有重点，这个配色让整张图看起来糊成一团，这个衣服设计太繁复显得很俗套，但是“没有灵魂”现在就是个框，什么都能往里装。很多人把AI文生图和使用AI辅助的绘画看成了完完全全的一码事，如果你把100%纯随机噪声开始的文生图定义为没有灵魂，那么从手绘草稿开始，AI上色的图有没有灵魂？从手绘草稿开始，手绘粗略颜色，AI低度去噪润色的图有没有灵魂？全程手绘（也就是不需要AI处理也已经是高完成度了），但是拿AI后期局部重绘一些地方有没有灵魂？以上这些情况分别有百分之几的灵魂？ 什么超过不超过人类，这根本就是个伪命题。“超过人类”这种描述就好像艺术有一个既定标准，就好像人类必须要和AI比赛、必须要有一个赢一样。实际上AI目前根本没有主观意识，根本没想和谁比，一切都是人类自己在加戏。AI的出现确实极大改变了艺术相关行业的形势，但是我觉得在这个节骨眼上争来争去已经没有意义了，因为它就是一个无法回到娘胎里的孩子，即使未来能够较好地立法管制、发展无法去除的隐形水印等等，技术本身在脱去AI buzzword外衣之后也会多多少少融入各类艺术创作。 尤其是对于某些任务来说（比如说修复老照片、加减人物物件一类的修图任务）、即使是目前的AI水平，也已经比人类高效高质无数倍了。你要是光说这几项算不上传统概念上的“艺术创作”的任务，那我是真没法反驳，要知道自从AI绘画模型井喷，红迪几乎所有Photoshop相关Sub都又慌又气，我经常去的有一个叫Photoshop Request，自从AI图片编辑模型火了之后这个组就变得十分尴尬，因为大多数红迪用户都非常反感AI，但是无奈AI（加上手动精修）实在太好用，于是发帖人Request信息就被分成了两类，一个是“No AI”一个是“AI OK”，因为对于很多请求人来说，图做的符合他预想效果才是最重要的，跟艺术不艺术没关系，管你AI不AI，完成目标就行。 ……所以我觉得这就会是未来的发展，省时又高质的话，到头来别人都在用就你吭哧吭哧手工做？但是艺术创作本身就是另一回事了，因为就像我前面说的，我不认为这是一场比赛。 添加评论,真的写上瘾了​ 教师资格证持证人 16 人赞同了该回答 作为一名普通的网络作家，我时常借助AI进行创作，但对它的感受可谓一言难尽。 最初我接触的是DeepSeek。按照教程，我亲自拟定了大纲、人物设定和主要情节，然后交给AI进行扩写。结果生成的作品思维混乱、语句不通，连讲笑话都比AI自己写的更生动。 后来我尝试“训练”AI，投喂了大量自己写的作品以及网上找来的文本，但产出依然不尽如人意。 于是我转而试用文心一言，效果依旧；又试了豆包，虽然略好一些，但改善有限。 之后我调整策略：自己先写50%，余下部分交给AI扩写。可惜成果还是老样子。 我再次退让，自己完成90%，仅让AI做最后扩写和润色。这次效果有所提升，部分内容勉强可用，但仍需我反复修改。 我也曾尝试用AI生成图像，结果同样令人感慨。特别是在生成人像时，不是五官错位，就是形态失真。此外我还意识到：想要用好AI，反而需要不错的文字功底——必须能用清晰的语言描述需求，让AI“听懂”。这对很多人来说并不容易，毕竟不是每个人都具备丰富的词汇积累，更别说像程序员那样结构化的表达思维。 总体而言，说AI已超越人类纯属无稽之谈。在创造力方面，AI仍有很长的路要走。 但不可否认，AI确实带来了积极的影响：它拉近了普通人与专业领域的距离。随着技术逐渐成熟，未来人们或许能真正借助AI创造出理想的作品。 到那时，AI才有可能谈得上超越人类。,蓝灯君​ 教师资格证持证人 13 人赞同了该回答 2022年12月8日，民国才女陆小曼的未完画作《夏日山居图》，分别接受了画家乐震文和百度文心一格的续笔，完成了画作。三幅画作共同拍卖了110万元。 （中）陆小曼遗作 （左）乐震文 （右）文心一格 这两张画，高下如何，相信就是外行也能看出来吧？ 单从山体的形态来说，乐震文考虑了画面的深远、高远、平远，原作虽然山体大体完整，但是细节还未到位，层次还没有显现，但是续画 而文心一格在绘制的时候，仿佛就是依着山势和物体去“随类赋彩”，那空白的地方没有“类”，如何赋彩呢？所以画面中间的水体是茫然的，水也不知从何而来？ 画面顶部略显空泛，乐震文秉承他多年研习北宋山水的构图方法，给出了远山、云雾、更小的树木，显示了中国古代山水的独特透视方法。虽然画在一个平面上，但是近大远小，依然有高远、深远的感觉。反观文心一格，也觉得顶部空泛，就给了两棵大大的树，你不觉得这两棵树是俩大大的显眼包吗？比山脚的树还要大，还要清晰，细节还要丰富！那感情你这是站在山脚看还是站在山腰看这个景呢？ 再说设色，乐震文使用石青石绿，传统的矿物质颜料，一看就是青绿山水的设色，文心一格怕不是用的水粉色吧？它倒是调动了国画模型，用了接近石青的颜色，奈何这电脑色就是没有模拟出矿物颜料的层次和质感吧？明显敷粉过多，前后都一个色，没有层次了吧？ 这里要说一下上色方法，国画工笔上色是一层一层渲染的，不是一笔就到位的，显然文心一格在生成的时候不是模拟的一层一层渲染的吧？ 还有很多细节，如果请一位专家来，一定能指出很多很多！ 当然，这是2022年的模型，要说这几年，那进步肯定是神速了。 但是，你一定要把人和机器分个高下干嘛呢？出现AI，难道不是为了人类服务的？难道不是为了让我们的生活更美好？ 陆小曼和乐震天都属于海派，海派向来有“海纳百川，兼容并蓄”的精神。这也就促成了两位海派画家和一个AI画手接力作画，体现了一种包容和接纳。 陆小曼的绘画在理解宋代院体画高古气息的基础上融入了女性的柔美和清格，乐震川在动笔前，仔细研究了这位师姐，在风格上尽量做到理解和一致。这次续画，是两代海派画家在不同时空下，对如何理解传统，如何创新的一次接力。而AI的参与，达到了一种人机共生的可能性。未来可能会出现新的画种——人机接力。 我在参观梵高画展时，看见一个小女孩，她说：“我如果见到梵高，要给梵高20块钱”我问她为什么？她说：“他画的星空我太了！”也许AI 画师在进行训练后，可以出现无数个星空的变体，然而，第一个星空，依然是属于梵高的！@知乎科技 @知乎人文
如何评价谷歌在2025年11月18日凌晨发布的gemini3.0pro模型？,,"ontheway AI 深度研究员，AI 创业者 283 人赞同了该回答 谢邀。人在工位，刚试完 Antigravity，稍微聊两句。 先说结论：谷歌这次不仅仅是发了个模型，它是想重新定义开发者的“操作台”。 如果说 Gemini 1.0 还在追赶 GPT-4，2.0 还在找多模态的差异化，那这次 11 月 18 号发布的 Gemini 3.0 Pro 配合 Antigravity，确实让我看到了一点“巨头醒了”的感觉。 这一波更新，我觉得可以从三个维度来看： 1. 终于，谷歌有了自己的“Cursor” 这其实是我最关注的点。之前我们用 Cursor，用 Windsurf，虽然好用，但总觉得是“外挂”在 VS Code 上。 这次谷歌直接下场做了 Google Antigravity。这玩意儿表面看是个 VS Code 的魔改版，但核心逻辑变了——它把 IDE 变成了Agent（智能体）的各种“Surface”。 Editor 是给原本的我们用的； Manager 是给 Agent 分配任务用的； Browser 是让 Agent 自己去 Chrome 里跑测试、看效果用的。 这对开发者意味着什么？意味着你不再是“写代码的人”，你更像是一个技术经理。你给 AI 指派任务（比如“写个登录页，顺便把测试跑通”），然后你在 Manager 视图里看着它干活。 这种原生级别的集成（特别是 Chrome 浏览器的深度打通），是第三方插件很难做到的护城河。 2. 模型侧：跟进了“Thinking”路线，但更务实 Gemini 3.0 默认开启了 High Thinking 模式，很明显是验证了 OpenAI o1 的路线——慢思考，强推理。 但谷歌聪明的地方在于 API 的设计。它搞了个 ""Thought Signatures""（思维签名）。做过 Agent 开发的都知道，多轮对话里 AI 经常“忘事儿”或者逻辑断层。谷歌这个机制强制你在多轮调用里把它的“思考快照”传回去，虽然增加了点开发成本，但极大地保证了复杂任务（比如连续调用十次 API）的稳定性。 这就很务实，它是为了解决“AI 写代码写到一半突然智障”这个痛点去的。 3. “Vibe Coding” 时代的官方背书 发布会上反复提 ""Vibe Coding""（氛围编码/直觉编码），其实就是承认了现在的开发趋势：自然语言 > 语法细节。 Gemini 3 在语境理解和“直觉”上做了很大优化。以前你需要很精确的 Prompt 才能让 AI 写对代码，现在可能你只要描述“那个按钮的感觉不对，要更有弹性的动效”，它就能 Get 到你的点。这种对模糊指令的理解力，才是下一代编码模型的胜负手。 总结一下 这次发布会，谷歌没有像以前那样堆砌无聊的跑分（虽然分肯定也高），而是把重点放在了**工作流（Workflow）**上。 Gemini 3.0 + Antigravity，这一套组合拳打下来，压力其实来到了 OpenAI 和微软这边。毕竟，拥有浏览器（Chrome）、拥有底层框架（Android/Flutter/Go）、拥有云（GCP）还拥有最强模型和 IDE 的，目前只有谷歌一家。 如果 Antigravity 的体验能稳住，2026 年的开发模式，可能真的要变天了。 以上。准备去把我的旧项目迁移进去跑跑看了。 8 点更新，我已经完成了利用 Gemini3 复刻抖音爆款视频！！！ 上传一个视频，自动分析原视频，然后 生成爆款视频： 这是原视频 00:10 上面截图是上传原视频之后 Gemini3自动生成的提示词和视频文案，然后调用 sora2生成的新视频 00:15 把生成的提示词在 creaa.ai 调用 sora2 生成新的爆款视频 11.20 谷歌最新的图像模型 nano banana pro 2 已发布，再一次震撼全网！ 国内直连 地址 creaa.ai 实测图来看，Nano Banana 2 已经不是“玩梗的小香蕉”了，而是 Google 正式把「AI 生图」当生产力工具来做的一次大升级：画质拉到 2K/4K、文字和推理明显补强、工作流朝着“会自检的设计助理”演化。 送礼物 还没有人送礼物，鼓励一下 继续追问 由知乎直答提供 谷歌的Antigravity与VS Code有何不同？ 新模型如何改变开发者的工作方式？ Vibe Coding趋势下，新模型有何优化？",李明殊​ 新知答主 154 人赞同了该回答 虽然我一向主张『跑分仅供参考，体验才是王道』，但这一次，面对 Gemini 3.0 Pro 甩出的成绩单，我必须承认： 参考个鬼，这简直是降维打击。 首先看第一行，Humanity’s Last Exam。 解释一下这个评测集，大型语言模型（LLM）在传统基准测试中轻松斩获90%以上的准确率时， 如何精准评价 AI 的真实能力，所以在 2025年初，由全球近千名专家联合打造的《Humanity’s Last Exam》（HLE）横空出世，被称为『人类最后一场闭卷考试』。这项包含3000道高难度题目的多模态基准测试。 一些题目示例。 HLE 跟之前的一些现有基准对比测试。 战况如何？让我们看看这一串令人窒息的对比： Claude Sonnet 4.5: 13.7% Gemini 2.5 Pro: 21.6% GPT-5.1: 26.5% 而 Gemini 3.0 Pro？它直接干到了 37.5%。 但这还没完，一旦开启搜索工具（Search Grounding），它的分数飙升到了 45.8%。 如果说 HLE 只是全面体检，那么 MathArena Apex 就是 AI 届的『奥林匹克数学竞赛』。 这里的数据更加触目惊心，甚至可以说有些残忍： Gemini 2.5 Pro: 0.5 分（基本上就是交白卷） Claude Sonnet 4.5: 1.6 分 GPT-5.1: 1 分（是的，哪怕是 OpenAI 的顶级模型，也只拿了 1 分） Gemini 3.0 Pro 拿了多少？—— 23.4 分。 这不是 20% 或 30% 的提升，这是 23 倍 的碾压。在逻辑推理和复杂数学领域，Gemini 3.0 Pro 似乎已经进化出了某种即使是 GPT-5.1 都尚未触及的『思维链条』。 这是什么概念？ 如果把其他模型比作刚及格的初中生，Gemini 3.0 Pro 已经一只脚踏进了博士生的门槛。 目前，Gemini 3 Pro 已经上线 Gemini 也能在线使用。 那试着让它做点东西？ 由于需要搞摄影评测，需要经常就要计算景深，等效焦距，等效光圈等信息。 于是我就随便写了一句 Prompt: 做一个影像传感器换算的应用，包含不同大小传感器的等效焦距，等效光圈的换算，以及景深计算，可以在页面模拟展示 一分钟不到，它就把代码写完了。 这个代码我没有进行任何调整，试着运行一下吧： 首先这个 UI 做的很不错，左边是参数调节，右边是模拟预览。 而且这个景深模拟也被做了出来，还算直观，比如这是 F1.2 时候的： 这是 F22 的表现 虽然它目前的预设数据源主要涵盖了从一英寸到中画幅的专业相机，缺少了手机传感器数据（这点我稍有不满），但它在末尾主动给出了优化建议。 这意味着什么？ 这意味着，我只需要再花 10 秒钟告诉它『加入主流手机传感器数据』，或者『把背景换成街景』，这个应用就完美了。 从『有想法』到『产品落地』，中间的耗时从几天缩短到了几分钟。 再来一个离谱的， 让它做一个 Windows XP 的模拟系统—— Prompt： 构建一个高度还原且可交互的 Windows XP 桌面模拟器，全部代码必须能放入单个 HTML 文件中，确保在 Chrome 中直接打开即可运行，无需任何外部依赖或构建步骤。 系统应包含以下功能组件（均需可交互）： 经典 XP 桌面（蓝天草地壁纸 + 任务栏 + 开始菜单） 文件管理器（可浏览虚拟文件系统） 文本编辑器（支持基础编辑） 代码编辑器（语法高亮，可运行 Python） 内嵌终端（可执行简单 Python 命令并返回结果） 画图工具（简易绘图功能，如画笔、形状） 视频编辑器（模拟界面，如时间轴、剪辑预览——可简化但要有 XP 风格 UI） 可玩小游戏（例如扫雷或纸牌，至少一个完整可玩版本） 其他经典 XP 预装软件的精简模拟（如计算器、记事本、画图） 要求： 所有 UI 元素严格遵循 Windows XP 的视觉风格（包括窗口边框、按钮、图标、字体、阴影等） 交互流畅，点击、拖拽、最小化/最大化等行为符合真实体验 在保证功能的前提下，注入创意细节（如启动音效、窗口动画、鼠标悬停反馈、有趣的 Easter Egg 等） 整体效果应令人惊喜——不仅『能用』，更要『好看、好玩、有情怀』 使用任意前端技术（HTML/CSS/JS），但必须全部内联，禁止外部资源（图片可用 data URL，音频可用 base64 编码）。 最终输出为一个完整的 <html> 代码块，可直接复制粘贴运行。 顺便说一下，这么长的Prompt 不是我写的，我只写了一句想做个 XP 系统，让 AI 帮我补充的。 2 分钟就写好了，当我把生成的 HTML 文件拖进 Chrome 浏览器时，那熟悉的蓝天白草地壁纸弹了出来： 开始菜单：完美复刻，阴影和圆角都对味儿。 绘画应用，没问题，就是有点简陋 真的能玩，逻辑完全正确。 Python 编辑器，可以 请注意，这是一个没有调用任何图片素材的系统。所有的图标、窗口纹理，都是 Gemini 在那两分钟里，用代码『画』出来的。 正是我之前所说的，这年头，很多时候有需求，你没有必要去找工作，去找搜索，自己创作一个就好，你不需要懂代码，不需要懂前端，只需要你能够把你的需求清楚的说出来（不能清楚的表达，其实问题也不大，你可以跟 AI 反复交流，来获得一个比较不错的结果）。 反正，游戏模式已经改写。 送礼物 还没有人送礼物，鼓励一下,"方汝见之​ 北京师范大学 学科教学（数学）硕士 创作声明：包含 AI 辅助创作 126 人赞同了该回答 传闻 Gemini 在数学竞赛上已经超过人类了。作为见习竞赛教练当然也要试一试。 选取了高中小蓝本第5卷《不等式的方法与技巧》1.1 节的 5 道习题，让 Gemini 做了一遍。结果是…… 五战全胜 让我们来欣赏一下 Gemini 的解答。 感谢 @Doc2X 的 ocr 技术支持，刚公费充了 499 聊表敬意。 感谢 longcat ai 的翻译支持。别管聪不聪明，你就说快不快吧。 尽量保持了 Gemini 解答的原汁原味，为排版之便去除了一点点不重要的内容。 例 1 设 a\text{ 、 }b\text{ 、 }c 是正实数,求证: \frac{{a}^{2} + {bc}}{b + c} + \frac{{b}^{2} + {ca}}{c + a} + \frac{{c}^{2} + {ab}}{a + b} \geq a + b + c. \\ 策略说明 最优雅的证明方法是通过添加特定项来改造左侧表达式（LHS）。具体来说，对每个分式项分别加上对应的变量（如第一项加a，第二项加b，第三项加c），使分式简化为可轻松应用AM-GM不等式的形式。 步骤1：代数变形 设原式左侧为S，目标是证明S \geq a+b+c。 等价于证明S - (a+b+c) \geq 0，或更实用的变形： S + (a + b + c) \geq 2(a + b + c) \\ 以第一项为例，对其加a： \frac{a^2 + bc}{b + c} + a = \frac{a^2 + bc + a(b + c)}{b + c} = \frac{a^2 + ab + ac + bc}{b + c} \\ 分子分组因式分解： = \frac{a(a + b) + c(a + b)}{b + c} = \frac{(a + c)(a + b)}{b + c} \\ 对称性推广：其余两项同理可简化。 步骤2：重构不等式 对原式每项分别添加a,b,c，得到新不等式： \left( \frac{a^2 + bc}{b + c} + a \right) + \left( \frac{b^2 + ca}{c + a} + b \right) + \left( \frac{c^2 + ab}{a + b} + c \right) \geq 2(a + b + c) \\ 代入步骤1的化简结果： \frac{(a + c)(a + b)}{b + c} + \frac{(b + a)(b + c)}{c + a} + \frac{(c + b)(c + a)}{a + b} \geq 2(a + b + c) \\ 步骤3：变量替换 为简化表达式，令： x = b + c y = c + a z = a + b 此时所有分母均被替换，且x+y+z = 2(a+b+c)。 不等式转化为： \frac{yz}{x} + \frac{zx}{y} + \frac{xy}{z} \geq x + y + z \\ 步骤4：应用AM-GM不等式 对左侧项分组使用均值-几何平均不等式（AM-GM）： 第一组：\frac{yz}{x} + \frac{zx}{y} \geq 2\sqrt{\frac{yz}{x} \cdot \frac{zx}{y}} = 2z \\ 第二组：\frac{zx}{y} + \frac{xy}{z} \geq 2\sqrt{\frac{zx}{y} \cdot \frac{xy}{z}} = 2x \\ 第三组：\frac{xy}{z} + \frac{yz}{x} \geq 2\sqrt{\frac{xy}{z} \cdot \frac{yz}{x}} = 2y \\ 步骤5：求和与结论 将三组不等式相加： 2\left( \frac{yz}{x} + \frac{zx}{y} + \frac{xy}{z} \right) \geq 2x + 2y + 2z \\ 两边除以2： \frac{yz}{x} + \frac{zx}{y} + \frac{xy}{z} \geq x + y + z \\ 此式与步骤3的转换形式一致，原不等式得证。所有步骤可逆，故原命题成立。 等号成立条件： 当且仅当AM-GM中所有项两两相等（如\frac{yz}{x} = \frac{zx}{y}）时取等，此时必有x=y=z，即a=b=c。 证毕（Q.E.D.） 点评：采用了和原文完全相反的思路，但因式分解的思路是相似的。多题一解，反过来能提供对原证明更好的理解。 例 2 实数 x\text{ 、 }y\text{ 、 }z 满足 {xy} + {yz} + {zx} = - 1 ,求证: {x}^{2} + 5{y}^{2} + 8{z}^{2} \geq 4 \\ 以下是采用代数“平方和”方法的逐步证明： 证明策略 我们需证明不等式： x^2 + 5y^2 + 8z^2 \geq 4 \\ 给定约束条件： xy + yz + zx = -1 \\ 将右侧常数4替换为与变量相关的表达式： -4(xy + yz + zx) = 4 \\ 因此，目标不等式可转化为证明： x^2 + 5y^2 + 8z^2 \geq -4(xy + yz + zx) \\ 移项得： x^2 + 5y^2 + 8z^2 + 4xy + 4yz + 4zx \geq 0 \\ 配方法 将表达式分组构造完全平方： E = x^2 + 4xy + 4zx + 5y^2 + 4yz + 8z^2 \\ 步骤1：构造首项平方 寻找形如 (x + ay + bz)^2 的平方项，匹配 x^2、4xy、4zx。选择 (x + 2y + 2z)^2： (x + 2y + 2z)^2 = x^2 + 4y^2 + 4z^2 + 4xy + 8yz + 4zx \\ 步骤2：对比与调整 计算 E - (x + 2y + 2z)^2 的余项： \begin{align*} E - (x + 2y + 2z)^2 &= (x^2 + 5y^2 + 8z^2 + 4xy + 4yz + 4zx) \\ &\quad - (x^2 + 4y^2 + 4z^2 + 4xy + 8yz + 4zx) \\ &= y^2 + 4z^2 - 4yz \\ &= (y - 2z)^2 \quad \text{（完全平方）} \end{align*} \\ 步骤3：合并结果 原表达式可写为两个平方和： x^2 + 5y^2 + 8z^2 + 4xy + 4yz + 4zx = (x + 2y + 2z)^2 + (y - 2z)^2 \\ 由于平方项恒为非负数（a^2 \geq 0）： (x + 2y + 2z)^2 + (y - 2z)^2 \geq 0 \\ x^2 + 5y^2 + 8z^2 \geq -4(xy + yz + zx) \\ 代入约束条件得最终结果： x^2 + 5y^2 + 8z^2 \geq 4 \\ 证毕（Q.E.D.） 等号成立条件 当且仅当两平方项均为零时取等： y - 2z = 0 \implies y = 2z x + 2y + 2z = 0 \implies x = -6z 结合约束 xy + yz + zx = -1，解得特例： (x, y, z) = \left(-\frac{3}{2}, \frac{1}{2}, \frac{1}{4}\right) \quad \text{或} \quad \left(\frac{3}{2}, -\frac{1}{2}, -\frac{1}{4}\right) \\ 点评：和原书思路相同。毕竟此题有二次型背景，实质就是算特征值特征向量也玩不出什么花来…… 例 3 设 a, b, c \in {\mathbf{R}}^{ + } ,试证: 对任意实数 x\text{ 、 }y\text{ 、 }z ,有: {x}^{2} + {y}^{2} + {z}^{2} \geq 2\sqrt{\frac{abc}{\left( {a + b}\right) \left( {b + c}\right) \left( {c + a}\right) }}\left( {\sqrt{\frac{a + b}{c}}{xy} + \sqrt{\frac{b + c}{a}}{yz} + \sqrt{\frac{c + a}{b}}{zx}}\right) . \\ 并指出等号成立的充要条件. 这个不等式可以通过将其视为一个二次型来优雅地证明。我们的目标是证明与该二次型相关的对称矩阵是半正定的。 构建矩阵 首先，我们通过定义平方根中的系数来简化符号。将不等式表示为： x^2 + y^2 + z^2 - 2\lambda_{xy}xy - 2\lambda_{yz}yz - 2\lambda_{zx}zx \geq 0 \\ 其中，系数定义为： \lambda_{xy} = \sqrt{\frac{ab}{(a+c)(b+c)}}, \quad \lambda_{yz} = \sqrt{\frac{bc}{(a+b)(a+c)}}, \quad \lambda_{zx} = \sqrt{\frac{ca}{(a+b)(b+c)}} \\ 该不等式等价于以下对称矩阵 M 为半正定的条件： M = \begin{pmatrix} 1 & -\lambda_{xy} & -\lambda_{zx} \\ -\lambda_{xy} & 1 & -\lambda_{yz} \\ -\lambda_{zx} & -\lambda_{yz} & 1 \end{pmatrix} \\ 验证半正定性 为了证明 M 是半正定的，我们检查它的主子式。 一阶主子式： 对角线元素为 1 > 0。 二阶主子式： 我们需要检查 1 - \lambda_{xy}^2 \geq 0 是否成立。 1 - \frac{ab}{(a+c)(b+c)} = \frac{(a+c)(b+c) - ab}{(a+c)(b+c)} = \frac{ab + ac + bc + c^2 - ab}{(a+c)(b+c)} = \frac{c(a+b+c)}{(a+c)(b+c)} > 0 \\ 由于 a, b, c \in \mathbb{R}^+，所以二阶主子式严格大于零。 行列式： 我们需要计算 \det(M)。 \det(M) = 1 - (\lambda_{xy}^2 + \lambda_{yz}^2 + \lambda_{zx}^2) - 2\lambda_{xy}\lambda_{yz}\lambda_{zx} \\（注：项 -2\lambda_{xy}\lambda_{yz}\lambda_{zx} 来源于带负非对角元的行列式展开） 令 D = (a+b)(b+c)(c+a)。 步骤 A：计算乘积项 2\lambda_{xy}\lambda_{yz}\lambda_{zx} = 2\sqrt{\frac{a^2b^2c^2}{D^2}} = \frac{2abc}{D} \\ 步骤 B：计算平方和 \sum \lambda^2 = \frac{ab(a+b) + bc(b+c) + ca(c+a)}{D} \\ 展开分子部分： N_{sum} = a^2b + ab^2 + b^2c + bc^2 + c^2a + ca^2 \\ 注意到分母 D 展开后： D = (a+b)(b+c)(c+a) = (a^2b + ab^2 + b^2c + bc^2 + c^2a + ca^2) + 2abc \\ 因此： \sum \lambda^2 = \frac{D - 2abc}{D} = 1 - \frac{2abc}{D} \\ 步骤 C：最终行列式计算 将这些结果代入行列式公式： \det(M) = 1 - \left( 1 - \frac{2abc}{D} \right) - \frac{2abc}{D} \\\det(M) = 1 - 1 + \frac{2abc}{D} - \frac{2abc}{D} = 0 \\ 由于主子式都是正的，且行列式恰好等于零，矩阵 M 是半正定的。这证明了对于所有实数 x, y, z，不等式都成立。 等号成立的条件 当向量 \mathbf{v} = \begin{pmatrix} x & y & z \end{pmatrix}^T 属于矩阵 M 的零空间（核）时，等号成立。由于 \det(M)=0 且秩为 2，存在一个单参数解空间。 我们通过解方程组对应的第一行 M\mathbf{v} = \mathbf{0} 来求解： x - \lambda_{xy}y - \lambda_{zx}z = 0 \implies x = y\sqrt{\frac{ab}{(a+c)(b+c)}} + z\sqrt{\frac{ca}{(a+b)(b+c)}} \\ 利用变量之间的对称性验证解。等号成立的条件是： x : y : z = \sqrt{a(b+c)} : \sqrt{b(c+a)} : \sqrt{c(a+b)} \\ 验证： 如果我们令 x = \sqrt{a(b+c)}, y = \sqrt{b(c+a)}, z = \sqrt{c(a+b)}： \text{LHS} = a(b+c) + b(c+a) + c(a+b) = 2(ab + bc + ca) \\\text{RHS项 } xy = \sqrt{ab(b+c)(c+a)}. \quad \text{系数 } \lambda_{xy} = \frac{\sqrt{ab}}{\sqrt{(b+c)(c+a)}} \\2\lambda_{xy}xy = 2\left( \frac{\sqrt{ab}}{\sqrt{(b+c)(c+a)}} \right) \left( \sqrt{ab(b+c)(c+a)} \right) = 2ab \\ 求和右边各项：2ab + 2bc + 2ca。 因此，LHS = RHS。 不等式成立。当且仅当以下情况时等号成立： \frac{x}{\sqrt{a(b+c)}} = \frac{y}{\sqrt{b(c+a)}} = \frac{z}{\sqrt{c(a+b)}} \\ （或者 x=y=z=0）。 点评：此题也有二次型背景，于是 Gemini 不配方直接算特征值特征向量了…… 例 4 设 a, b, c \in {\mathbf{R}}^{ + } ,求证: {a}^{2a}{b}^{2b}{c}^{2c} \geq {a}^{b + c}{b}^{c + a}{c}^{a + b} . 下面是一个利用对数性质和单调性来证明该不等式的分步过程。 证明策略 解决这个不等式最有效的方法是对两边取自然对数。由于自然对数函数 y = \ln(x) 在 x > 0 的范围内是严格递增的，原不等式成立当且仅当取对数后的不等式成立。 步骤1：取自然对数 对不等式两边取自然对数 (\ln)： \ln \left( a^{2a} b^{2b} c^{2c} \right) \geq \ln \left( a^{b + c} b^{c + a} c^{a + b} \right) \\ 利用对数性质 \ln(xy) = \ln x + \ln y 和 \ln(x^k) = k \ln x，我们可以将两边展开： 2a \ln a + 2b \ln b + 2c \ln c \geq (b + c) \ln a + (c + a) \ln b + (a + b) \ln c \\ 步骤2：合并同类项 为了便于比较，我们将所有项移到左边（LHS），证明该表达式 \geq 0。 2a \ln a + 2b \ln b + 2c \ln c - \left[ (b + c) \ln a + (c + a) \ln b + (a + b) \ln c \right] \geq 0 \\ 现在，按 \ln a、\ln b 和 \ln c 合并同类项： (2a - b - c) \ln a + (2b - c - a) \ln b + (2c - a - b) \ln c \geq 0 \\ 步骤3：对称化表达式 上述表达式是循环对称的。为了更清晰地看出其结构，可以重新整理系数。例如，将 2a 拆为 a + a。 我们通过展开分组，然后重新组合成差值的方式重组左边： (a \ln a - b \ln a) + (a \ln a - c \ln a) + (b \ln b - c \ln b) + (b \ln b - a \ln b) + (c \ln c - a \ln c) + (c \ln c - b \ln c) \geq 0 \\ 现在，按涉及相同变量组合成对（例如，a 和 b）： a 和 b 的组合： a \ln a - b \ln a + b \ln b - a \ln b = (a - b) \ln a - (a - b) \ln b = (a - b)(\ln a - \ln b) b 和 c 的组合： b \ln b - c \ln b + c \ln c - b \ln c = (b - c)(\ln b - \ln c) c 和 a 的组合： c \ln c - a \ln c + a \ln a - c \ln a = (c - a)(\ln c - \ln a) 因此，不等式等价于证明： (a - b)(\ln a - \ln b) + (b - c)(\ln b - \ln c) + (c - a)(\ln c - \ln a) \geq 0 \\ 步骤4：分析各项符号 考虑函数 f(x) = \ln x，由于它在 \mathbb{R}^+ 上是严格递增的： 如果 x > y，那么 \ln x > \ln y，所以 (x - y) 和 (\ln x - \ln y) 均为正。 如果 x < y，那么 \ln x < \ln y，所以 (x - y) 和 (\ln x - \ln y) 均为负。 如果 x = y，乘积为0。 在所有情况下，乘积 (x - y)(\ln x - \ln y) 始终 非负 (\geq 0)。 由于重组后各项均非负： (a - b)(\ln a - \ln b) \geq 0 \\(b - c)(\ln b - \ln c) \geq 0 \\(c - a)(\ln c - \ln a) \geq 0 \\ 这些非负项的和必然 \geq 0。因此，变换后的不等式成立，意味着原不等式也成立。 \boxed{a^{2a} b^{2b} c^{2c} \geq a^{b + c} b^{c + a} c^{a + b}} \\ （等号成立当且仅当 a = b = c）。 另一种见解（切比雪夫不等式） 如果你熟悉 切比雪夫和不等式，可以注意到，若 (a, b, c) 和 (\ln a, \ln b, \ln c) 是同序排列的（例如 a \ge b \ge c 时， \ln a \ge \ln b \ge \ln c）。 切比雪夫不等式指出，对于同序序列： 3(a \ln a + b \ln b + c \ln c) \geq (a + b + c)(\ln a + \ln b + \ln c) \\ 两边减去 (a \ln a + b \ln b + c \ln c) 即可得到步骤2中的结果。 点评：相比原书取了对数……不如说是原书不取对数才很奇怪吧，是对高等数学工具有什么厌恶之情吗。剩下的思路是相似的，但看起来清楚太多了。 另给出了 Chebyshev 不等式的解法。一题多解有助于加深理解。 例 5 设 a, b, c \in {\mathbf{R}}^{ + },{a}^{2} + {b}^{2} + {c}^{2} = 1 ,求 S = \frac{1}{{a}^{2}} + \frac{1}{{b}^{2}} + \frac{1}{{c}^{2}} - \frac{2\left( {{a}^{3} + {b}^{3} + {c}^{3}}\right) }{abc} \\ 的最小值. 以下是求解 S 最小值的详细分步解答。 已知 a, b, c \in \mathbb{R}^+，且满足约束条件 a^2 + b^2 + c^2 = 1。 要最小化的目标表达式为： S = \frac{1}{a^2} + \frac{1}{b^2} + \frac{1}{c^2} - \frac{2(a^3 + b^3 + c^3)}{abc} \\ 第 1 步：齐次化处理 注意到 \frac{1}{a^2} 与 \frac{a^3}{abc}（化简后次数为 0）的次数不一致。为统一量纲，利用约束 a^2 + b^2 + c^2 = 1 进行替换： \frac{1}{a^2} = \frac{a^2 + b^2 + c^2}{a^2} = 1 + \frac{b^2}{a^2} + \frac{c^2}{a^2} \\ 点评：原书也做了这一步齐次化，但这里更好地说明了动机。 对三个变量求和，得： \sum_{\text{cyc}} \frac{1}{a^2} = \left(1 + \frac{b^2}{a^2} + \frac{c^2}{a^2}\right) + \left(1 + \frac{a^2}{b^2} + \frac{c^2}{b^2}\right) + \left(1 + \frac{a^2}{c^2} + \frac{b^2}{c^2}\right) \\\sum_{\text{cyc}} \frac{1}{a^2} = 3 + \left(\frac{b^2}{a^2} + \frac{c^2}{a^2} + \frac{a^2}{b^2} + \frac{c^2}{b^2} + \frac{a^2}{c^2} + \frac{b^2}{c^2}\right) \\ 第 2 步：展开减项 观察第二部分的减项： \frac{2(a^3 + b^3 + c^3)}{abc} = \frac{2a^3}{abc} + \frac{2b^3}{abc} + \frac{2c^3}{abc} = \frac{2a^2}{bc} + \frac{2b^2}{ac} + \frac{2c^2}{ab} \\ 第 3 步：重写 S 并分组 将上述结果代入原式 S： S = 3 + \left[ \left(\frac{a^2}{b^2} + \frac{a^2}{c^2}\right) + \left(\frac{b^2}{a^2} + \frac{b^2}{c^2}\right) + \left(\frac{c^2}{a^2} + \frac{c^2}{b^2}\right) \right] - \left( \frac{2a^2}{bc} + \frac{2b^2}{ac} + \frac{2c^2}{ab} \right) \\ 重新分组，以突出与负项相消的部分： S = 3 + \left(\frac{a^2}{b^2} + \frac{a^2}{c^2} - \frac{2a^2}{bc}\right) + \left(\frac{b^2}{a^2} + \frac{b^2}{c^2} - \frac{2b^2}{ac}\right) + \left(\frac{c^2}{a^2} + \frac{c^2}{b^2} - \frac{2c^2}{ab}\right) \\ 第 4 步：应用均值不等式（AM-GM） 对每一组正项，使用 AM-GM 不等式（x + y \ge 2\sqrt{xy}）进行放缩。 含 a^2 的项： \frac{a^2}{b^2} + \frac{a^2}{c^2} \ge 2\sqrt{\frac{a^2}{b^2} \cdot \frac{a^2}{c^2}} = 2\sqrt{\frac{a^4}{b^2c^2}} = \frac{2a^2}{bc} \\ 因此： \frac{a^2}{b^2} + \frac{a^2}{c^2} - \frac{2a^2}{bc} \ge 0 \\ 含 b^2 的项： \frac{b^2}{a^2} + \frac{b^2}{c^2} \ge \frac{2b^2}{ac} \quad \Rightarrow \quad \frac{b^2}{a^2} + \frac{b^2}{c^2} - \frac{2b^2}{ac} \ge 0 \\ 含 c^2 的项： \frac{c^2}{a^2} + \frac{c^2}{b^2} \ge \frac{2c^2}{ab} \quad \Rightarrow \quad \frac{c^2}{a^2} + \frac{c^2}{b^2} - \frac{2c^2}{ab} \ge 0 \\ 第 5 步：结论 将上述不等式代入 S，得： S \ge 3 + 0 + 0 + 0\\S \ge 3 \\ 等号成立当且仅当 AM-GM 取等，即： \frac{a^2}{b^2} = \frac{a^2}{c^2} \quad \Rightarrow \quad b^2 = c^2 \quad \Rightarrow \quad b = c \quad (\text{因均为正实数}) \\ 由对称性，a = b = c。 结合约束条件 a^2 + b^2 + c^2 = 1，解得： a = b = c = \frac{1}{\sqrt{3}} \\ 答案：S 的最小值为 3。",bubb 65 人赞同了该回答 为什么搜索能力还是一坨...明明有那么好的底子，但是Google非不给加搜索 如果能把Gemini3作为多模态底模结合GPT的搜索能力和用户体验，这就是理想中的好模型了,"吕阿华​ 浙江大学 计算机硕士 30 人赞同了该回答 一觉醒来，Gemini的web界面已经更新，上一个版本的模型选择菜单里的选项是 Gemini 2.5 Flash 和 Gemini 2.5 pro。硬实力在线的大模王就是这样朴实无华。 在 Gemini 时代开启不到两年的时间里，AI 已从单纯阅读文本和图像进化到能够“察言观色”（Reading the room），具备了前所未有的感知力。凭借 Google 在基础设施、研究和产品上的全栈优势，Gemini 3 将立即以“Google 规模”推向全球，旨在帮助用户实现任何创意。 Pro 与 Deep Think Gemini 3 系列在推理、多模态理解和智能体能力上实现了质的飞跃，主要包含两个核心形态： Gemini 3 Pro 作为主力模型，它在几乎所有主要的 AI 基准测试中都显著优于前代（2.5 Pro）。它以 1501 Elo 的高分登顶 LMArena 排行榜，并展现出博士级的推理能力。在数学（AIME 2025 得分 95%）、多模态推理（MMMU-Pro 得分 81%）以及长上下文处理方面均树立了行业新标杆。 基准 (Benchmark) 描述 / 条件 Gemini 3 Pro Gemini 2.5 Pro Claude Sonnet 4.5 GPT-5.1 Humanity’s Last Exam 学术推理 (无工具) 37.5% 21.6% 13.7% 26.5% 学术推理 (带搜索和代码执行) 45.8% — — — ARC-AGI-2 视觉推理谜题 (ARC Prize 验证) 31.1% 4.9% 13.6% 17.6% GPQA Diamond 科学知识 (无工具) 91.9% 86.4% 83.4% 88.1% AIME 2025 数学 (无工具) 95.0% 88.0% 87.0% 94.0% 数学 (带代码执行) 100% — 100% — MathArena Apex 高难度数学竞赛题 23.4% 0.5% 1.6% 1.0% MMMU-Pro 多模态理解与推理 81.0% 68.0% 68.0% 76.0% ScreenSpot-Pro 屏幕理解 72.7% 11.4% 36.2% 3.5% CharXiv Reasoning 复杂图表信息综合 81.4% 69.6% 68.5% 69.5% OmniDocBench 1.5 OCR (整体编辑距离，越低越好) 0.115 0.145 0.145 0.147 Video-MMMU 视频知识获取 87.6% 83.6% 77.8% 80.4% LiveCodeBench Pro 竞技编程问题 (Codeforces, ICPC, IOI; Elo 分，越高越好) 2,439 1,775 1,418 2,243 Terminal-Bench 2.0 智能体终端编程 (Terminus-2 智能体) 54.2% 32.6% 42.8% 47.6% SWE-Bench Verified 智能体编程 (单次尝试) 76.2% 59.6% 77.2% 76.3% t2-bench 智能体工具使用 85.4% 54.9% 84.7% 80.2% Vending-Bench 2 长程智能体任务 (平均净值，越高越好) **5,478.16** | 573.64 3,838.74 | 1,473.43 FACTS Benchmark Suite 留出集内部基础、参数化、多模态及搜索检索基准 70.5% 63.4% 50.4% 50.8% SimpleQA Verified 参数化知识 72.1% 54.5% 29.3% 34.9% MMMLU 多语言问答 91.8% 89.5% 89.1% 91.0% Global PIQA 跨越 100 种语言和文化的常识推理 93.4% 91.5% 90.1% 90.9% MRCR v2 (8-needle) 长上下文性能 (128k 平均) 77.0% 58.0% 47.1% 61.6% 长上下文性能 (1M 单点) 26.3% 16.4% 不支持 不支持 Gemini 3 Pro 与其他模型基准测试对比。注： 有关评估方法的详细信息，请访问 deepmind.google/models/evals-methodology/gemini-3-pro Gemini 3 Deep Think 继 Gemini 2.5 推出了 Deep Think 模式后， Gemini 3 也升级了这个全新增强推理模式，旨在解决最棘手的复杂问题。它在 Humanity’s Last Exam 和 GPQA Diamond 等高难度测试中表现卓越，并在 ARC-AGI-2 上取得了 45.1% 的突破性成绩，证明了其应对未知新颖挑战的强大能力。 Gemini 3 Deep Think 模式在一些最具挑战性的 AI 基准测试中表现卓越。 学习、构建与规划 Gemini 3 的设计初衷是全方位赋能用户，主要体现在以下三个维度： 学习任何事物 (Learn) Gemini 3 利用 1M token 的上下文窗口和顶尖的多模态能力，能以最适合用户的方式整合信息。它可以解析不同语言的手写食谱，分析运动视频以提供训练建议，或将学术论文转化为交互式学习工具。 00:40 分析论文并生成代码 此外，搜索中的 AI 模式现已集成 Gemini 3，能根据查询实时生成沉浸式的视觉布局和交互模拟，极大提升了信息获取的效率。 00:42 RNA 聚合酶如何工作 构建任何事物 (Build) 这是 Google 最强的 Vibe Coding（直觉编程）和智能体编程模型。它在 SWE-bench Verified（编程智能体基准）上得分 76.2%，并在 WebDev Arena 上霸榜。它能处理复杂的提示词，渲染丰富的 Web UI，显著提升开发者效率。开发者现可通过 Google AI Studio、Vertex AI 以及全新的智能体开发平台 Google Antigravity 使用该模型。 00:26 3D 星际战舰游戏 规划任何事物 (Plan) Gemini 3 在长程规划（Long-horizon planning）方面取得了重大突破。在 Vending-Bench 2 测试中，它成功模拟了一整年的商业运营决策。这意味着它能更可靠地代表用户执行多步骤工作流（如预订服务、管理收件箱），且始终保持在用户的掌控和指导之下，不会偏离目标。 与其他前沿模型相比，Gemini 3 Pro 展现出更卓越的长程规划能力，从而能够产生显著更高的回报。 安全性与部署计划 Gemini 3 是 Google 迄今为止最安全的模型，经过了全面的红队测试和第三方（如英国 AISI、Apollo）评估，显著减少了讨好倾向（Sycophancy）并增强了抗提示词注入的能力。 综上，Gemini 3 不仅是模型性能的提升，更是 AI 从“阅读者”向“思想伙伴”和“行动代理”的演变，目测这个大模王又要长期霸占模型榜单的首位了~ LMSYS竞技场总榜（2025/11/17快照） 顺便搭车问一下，有没有人和我一样下了 win64 的 Google Antigravity 然后无论用 Edge 还是 Chrome 都无法顺利登录一直停在Sign in界面的？ 送礼物 还没有人送礼物，鼓励一下","汤姆好兄弟 一句话介绍了一句话。 32 人赞同了该回答 好多震惊体。我作为三家的付费用户，每个工作日平均用掉50美刀token的。我目前的体验是Gemini终于正式进入第一梯队（其他两家 GPT, Claude），但是不存在谁吊打谁。之前2.5时代Gemini是没办法抗衡其他两家的，干正经活不会考虑2.5 Pro。考虑到发布时间相近，说明这三家已经没有以月计数的差距了。另一方面模型的进步速度明显大幅放缓。 添加评论","放浪者​ 下乡锻炼中 19 人赞同了该回答 我说了啊，既然其他评论者一个个觉得Gemini 3无比震撼、未来已来、是降维打击、能实现你脑海中的一切、拔高了人类什么什么。。。 我这里有一个项目大概10个左右前端页面，和后端接口也就是一些CRUD的东西，修改搜索排序啥的。没有UI/UX，既然都觉得这么牛逼一键出各种效果，那想来UI/UX设计者是不需要的。 那是必须的啊，各路大V和什么AI研究者说的能实现人类脑海中一切呀，我脑海中就是几个土得不能再土的网页，登录注册付费和简单数据的CRUD，拿来让Gemini 3做那是必须分分钟出来的对吧？ 我出个200，哦，不，我出血，我出500人民币，麻烦这个题目下被Gemini 3震撼得五体投地的大V小V自媒体公众号还有什么研究者来帮做一下呢？ 各位都是用Gemini 3几分钟或者最多一小时就能实现各种牛逼哄哄功能的，我这后台接口差不多都有，想来500块还是可以的吧，对吧？ 真心的，我还有其他平台，比如我自己的AI小说写作平台 https://metabook.ai， 市面唯一一个真正写了百万字网文且有读者群的。 我不是开玩笑我真想要个AI牛人帮我干活，我智商不高，我用cursor啥的觉得还是很麻烦，一会儿这个输入框要拉大，一会儿那个list不scroll了，一会儿分页下滚更新又不行了，麻烦来个牛逼的AI Coder， Gemini 3了也，人类智慧又被拔高了也，相信这些都不在话下。一般前端开发一个页面（从头开始完全实现各种交互）加上UI/UX设计至少500、600起吧（淘宝外包价），但是AI了也，打个9折没问题吧，一个页面50块，各位知乎都发了真么多字，拿牛逼上天的Gemini 3做个页面有啥问题呢？500块十个页面（需要从头设计一下，当然这不会有问题，你不会对Gemini 3的能力怀疑的，各路大V都说了，Gemini 3对人类意图判断非常精准，你想到的能判断，你想不到的也能帮你实现，对吧？那还担心啥呢？），有意者私信哈。 我不是开玩笑，真有人能做的话，没问题的，手上还有不少项目。 我相信不至于一说跑分这些人就热情上天，抛个需求出来就全部装睡了吧。 不，大V们自媒体们，虽然500块不多，但是这是easy money啊，低垂的果实啊，你们要对Gemini 3有真正信心啊，不要学古代的画条龙吼得凶那种啊。 *************** btw, gemini api开发文档里面的sample代码，至少go那一块，貌似的确都是AI生成，因为... 基本全部是错的，全是似是而非其实sdk里压根没有的内容... 让我们来围观一下史上最强AI模型给自己的模型API生成的示范代码： 当你把代码输入IDE后，你会发现： client.Close() =>没有Close() 这方法 client.GenerativeModel(...) => 不好意思，这也是幻觉，压根不存在这接口 pb.GeneratingConfig(...) =>大神，全部代码和imports都在这里，这尼玛哪来的pb这个东西？ 世界最强AI，牛逼，未来已来，提高了人类智慧的水准，前端/后端/码农已死，一个产品经理不再需要程序员，全身心的震撼，玩疯了，一个人想做什么做什么...... 送礼物 还没有人送礼物，鼓励一下",社恐打工仔 9 人赞同了该回答 Google直接收割了比赛。 这份榜单，还有几个benchmark得划下重点！ Gemini 3真正炸的，其实是这几个。 MathArena Apex断层领先，其他模型 1% gemini20+%。 解释一下MathArena Apex是干嘛的：一个非常难的数学benchmark，平均难度超过imo最难的题。 这个分数在纯数学推理上已经大幅领先人类普通IMO金牌水平，进入super human区间。 ScreenSpot-Pro是claude的两倍， GPT在这个benchmark是个位数... 这个主要和GUI agent强相关，可以期待gemini computer agent的效果了 这里面很多现在大热的agent领域相关的benchmark：Terminal-Bench 2.0（终端操作）、τ2-bench（通用工具调用）、Vending-Bench 2（长时序经营模拟）、SWE-Bench Verified（真实软件工程修复）、ScreenSpot-Pro（屏幕/GUI 理解与操作），只有swe bench略低 claude，其他都是SOTA而且领先幅度都不小。 DeepMind研究负责人也说，Gemini 2.5到3.0的性能提升是他们见过最大的跨代飞跃，而且完全没有碰到scaling天花板。秘诀其实就两个方向在死磕。 第一是预训练还有巨大潜力。团队在NeurIPS 2025跟图灵奖得主Ilya等人讨论过，很多人觉得scaling law快失效了，但实际数据证明根本不是这样。 只要方法对了，模型还能继续大幅进化。 第二是后训练仍处于蓝海阶段。现在的算法优化空间非常大，Gemini 3在这块取得的突破也功不可没。 scaling law没崩！什么AI发展遇瓶颈的说法，真正的技术突破可能才刚开始。 添加评论,"算法一只狗​ 深圳市腾讯计算机系统有限公司 员工 8 人赞同了该回答 Gemini 3一登场就火力全开，强势霸榜！在LMArena大模型竞技场上，它直接冲上榜首，以1501的高分在文本领域大放异彩。就连马斯克刚发布的Grok 4.1，也被它轻松拉下神坛！ 根据Artificial Analysis平台的独立评测结果显示，其智能指数已达到73分，超越OpenAI于上周发布的GPT-5.1版本。 连马斯克和奥特曼都在推特上表示，Gemini 3确实是一个很强的模型。 目前谷歌已经免费开放了Gemini 3 Pro模型，可以直接去到官网进行体验 模型能力 这次Gemini模型的能力真的太强了，基本在所有榜单上面超过了GPT-5.1和Claude Sonnet 4.5 在Humanity’s Last Exam（人类最后一场考试）中，主要用来测试模型是否具有“准博士级”的跨学科推理能力。这个数据集题目偏理论、抽象推理，主要测试模型的“通用智能”，被认为是AI难以解决的问题，但是在Gemini 3 Pro （不使用搜索、代码执行工具）上优势非常明显，领先 GPT-5.1 一档，几乎是 Claude 4.5 的三倍。 在科研类测试数据集GPQA Diamond上，Gemini 3 Pro冲到了90%分以上。 而在常规的数学测试集上AIME，Gemini 3 Pro 是当今数学推理最强的通用模型（不算专精数学的 NMAX），如果直接可以用代码执行，直接达到了夸张的100%。在更高难度的数学测试集上MathArena Apex，Gemini 3 Pro 吊打所有竞争对手几十倍。这是一个非常夸张的领先幅度，说明它在高难度数学推理上远超其他模型。 除了在文本领域达到了夸张的分数之外，在多模态领域也基本上处于断代的领先： 在Video-MMMU（视频理解）：Gemini 3 Pro 在视频理解上达到87.6%，明显领先其他大模型 在ScreenSpot-Pro（屏幕理解）：Gemini 3 Pro 的 UI/屏幕理解能力几乎是所有模型的碾压级优势。Gemini 3 Pro高达72.7%，而GPT-5.1只有可怜的3.5% 在测试AI coding能力上，基本全面领先，在LiveCodeBench（编程竞赛能力）上拿到了2439分数，比GPT-5.1高出200多分。 在针对一些比较复杂任务的时候，往往需要多步进行实现，但是在实现过程中需要始终围绕任务目标执行操作。 下图展示了在模拟经营环境中为期一年的自动售货机运营情况，其评分依据为期末银行账户余额。在此基准测试中，Gemini 3 Pro在一年运营周期后的资金余额为5,478.16美元，显著领先于排名第二的Claude Sonnet 4.5。 有了这种能力之后，gemini就可以做很多的事情，比如可以帮助你快速整理收件箱 00:54 由于多模态能力明显提升，它可以辅助你判定视频中的内容并进行解析。比如可以对匹克球比赛视频进行技术分析，识别技术动作中的可优化环节，并据此制定系统性的动作改进训练方案。 00:34 如果学习一个新的知识，可提供相关学术论文、长篇视频课程或教程资源，gemini将据此生成交互式抽认卡、可视化图表及其他辅助知识掌握的学习材料。 00:40 总结来看，Gemini 3 Pro 是目前「综合实力最强、数学最强、视频理解最强、代理能力领先」的模型。尤其是在数学推理能力上（数学相关 benchmark 全部拉开差距）、多模态（视频、屏幕）上领先 GPT5.1 和 Claude 4.5）和Agent（工具调用 / 多代理协作）。 初步体验 1.常规测试 Q1：总结内容，生成知识卡片网页 总结这个网页（https://blog.google/intl/zh-tw/products/explore-get-answers/gemini-3/#note-from-ceo ），生成一个好看，精致的知识卡片网页内容 从整体视觉风格上看，黑白主色搭配科技渐变，风格现代；而且对于Gemini 3 的介绍比较一目了然 Q2：开发一个“微旅程”网站，并可以执行，每天推荐一张世界小众角落的照片+一段故事。 Q3：用SVG画一个思维导图，画一个switch 2.游戏编程测试 Q1：用 Three.js 实现一款3D战斗机对战游戏，在城市上方飞行，可以用触屏或者键盘操控。 虽然没有具体的城市建筑，但是游戏玩法上基本已经实现好了 Q2：用 Three.js 实现一款愤怒的小鸟3D游戏 游戏界面基本做出来了，而且可以进行简单的操作。不过界面确实有点简陋 网友测试案例 Q1：实现一个我的世界 prompt：Code a 3D Minecraft game clone within a single .html file. Make it beautiful, with pixel graphics like the original, add all main features of the game, terrain and tree generation, mobs, and a bunch of other stuff. use three.js for the rendering to make things look nice. Q2：只用一张图片就能生成threejs体素艺术场景 根据所提供的图像，编写一个受其启发的精美体素艺术场景。请使用Three.js编写代码，并确保代码为单页形式。 Q3：模拟真实的外太空游戏光影 从视频中可以看到，它的光影场景做得特别的好 00:07 Q4：物理水面波纹模拟 ""make a realistic water physics test, full 3d you can interact with it, reflections, waves, click anywhere to drop a lemon into the water"" 00:21 写在最后 随着 Gemini 3 的正式登场，我们已经能清晰地看到一个信号： AI 模型的发展正式进入「全能型」竞争阶段。过去我们会强调“哪个模型更擅长写代码”“哪个模型更懂视频”“哪个模型数学更强”，而现在，Gemini 3 的出现直接把这些维度拉平，甚至在多个关键能力上出现“断崖式领先”。这意味着未来的 AI 应用场景将以一种更爆炸性的速度向前涌动。 它不仅仅是一个回答问题的助手，而更像是一个“可执行的泛化智能体”：能理解复杂环境、能规划策略、能操作应用、能构建可运行系统，甚至能辅助你做一款小型游戏、一个网站、一门课程制作、视频解析……这种能力的跃迁，是从“会说”到“会做”的质变。 接下来，随着各家模型全面进入 Agent、多模态、可执行推理的大混战，我们大概率会见证一件事： 未来的应用开发者，不再是“写代码的人”，而是“写需求的人”。 从网站、数据分析，到游戏、美术、视频、学习路线、商业策划……你只需要一句话，AI 就能替你把 80% 的事情完成。 未来的2026年，很有可能就是—— 「AI 个人超级能力元年」。 而 Gemini 3，只是这一切的开端。 添加评论","大模型观察站 探究大模型技术，感知大模型趋势，我的目标是，与大模型共成长 4 人赞同了该回答 在经历了GPT-5、Grok 4和Claude Sonnet小升级后，2025年下半年，AI领域陷入了阶段性平淡。 直到今天，Gemini 3的发布彻底打破了平淡。 跃进式的得分提升、强大的多模态理解力、更加多样化的UI、惊艳的前端能力，这一切都真实地让AI向我们预期的形态迈出了显著的一步。 这些可见性远比Benchmark上的得分和只能在Coding系统里测出来的区别明显。 Gemini 3 不仅仅是一次版本号的更迭，它是对Scaling Law信仰的暴力重申，也是谷歌第一款真正意义上让OpenAI黯然失色的模型。 在这次发布会上，我们熟悉的谷歌王者归来。它不满足于在某个单一维度领先，而是要在模型能力、开发者工具、用户体验、搜索集成、多语言覆盖等所有战线上同时发力。 这是一个平台级的野心，用AI重新定义整个Google生态的每个触点。 01 Benchmark 的跃迁 Benchmark测试在AI圈内一直饱受争议，被戏称为刷题比赛。前几个月顶尖模型之间的分数只差几个百分点，你追我赶。 但Gemini 3的出现，把原本胶着的战争直接变成了断层式的碾压。 先看基础思考能力。Humanity's Last Exam (HLE)是衡量 AI 能否解决人类顶尖难题的终极试金石。在Gemini 3之前，Gemini 2.5 Pro的得分是21.6%，Claude Sonnet 4.5仅为13.7%。Gemini 3 Pro交出的答卷是37.5%（无工具）和45.8%（带工具）。 ARC-AGI-2测试，被誉为 AI 界的图灵测试，旨在衡量模型处理从未见过的新颖推理任务的能力，而非死记硬背。 Gemini 3 Pro 拿下了31.1%的分数，而GPT-5.1仅为17.6%，Gemini 2.5 Pro甚至只有 4.9%。这意味着它开始展现出一种接近人类的流体智力，能够在没有大量训练数据覆盖的领域进行抽象推理 。 连ARC Prize的创始人François Chollet在看到结果后发推文说：“我们刚刚验证了Gemini 3 Pro和Deep Think在ARC v2上超过SOTA 2倍以上！这真的很令人印象深刻，说实话，也有点出乎意料。” 而且Gemini 3 Pro解决最快的v2任务只用了772个token和188秒，几乎接近人类评审小组的147秒平均速度。 数学能力上，Gemini 3引入了一个新的MathArena Apex竞赛级Benchmark来强调其超越性。在这项测试里，Gemini 2.5 Pro的得分只有0.5%，Claude Sonnet 4.5是1.6%，GPT-5.1是1.0%。而Gemini 3 Pro得到了23.4%的成绩。 在多模态领域这个谷歌的强项上，Gemini 3表现得更为惊人。 MMMU-Pro的81.0%，CharXiv Reasoning的81.4%都超越了对手。而在理解截图的测试ScreenSpot-Pro的72.7%中，Gemini 3的得分是Claude Sonnet 4.5的两倍，GPT-5.1的二十倍。这对于构建能够真正理解和操作图形界面的AI代理至关重要。 02 编码能力过去一直是谷歌的弱项，但这次彻底翻盘 虽然在SWE-Bench Verified这个测试真实软件工程能力的benchmark上，Gemini 3的76.2%仍然不敌Claude的77.2%得分。 但在其他核心第三方的测试中，谷歌却远高于对手。在LiveCodeBench上，Gemini 3的分数比第二名Grok 4.1直接高了200多分。 在测试 Agent 工具使用能力的12-bench 中，Gemini 3 Pro拿到了 85.4% 的高分，远超Gemini 2.5 Pro的54.9% 。而在更符合终端环境的Terminal-Bench 2.0中，Gemini 3得到了54.2%，比第二名高出11个百分点。 这在很大程度上是一种综合能力的展示。 有了更好的屏幕理解能力和基于多模态能力衍生的前端审美，Gemini 3在真实编程环境下的表现很容易超越对手。 比如，在由开发者社区运营的实战编码竞技场Design Arena中，Gemini 3 Pro在整体排名中位列第一，并在五个代码赛区中的四个，网站、游戏开发、3D设计和UI组件中都占据榜首。这是自推出Design Arena以来最大的性能差异。 记忆一直都是一个很大的模型瓶颈。因此Gemini 3在长上下文能力的提升也值得关注。 它在MRCR v2 benchmark中28k上下文的平均得分77.0%远超竞争对手，1M上下文的逐点得分26.3%。 这说明，Gemini 3不是简单地“塞入”更多tokens，而是真正理解和利用长文档中的信息。 根据Artificial Analysis的分析，Gemini 3在事实回忆（factual recall）上也有着强大表现。 最后，看一下综合能力。Vending-Bench 2是一个测量AI模型在长时间跨度内运营业务能力的benchmark，模型需要在一年时间内运营一个模拟的自动售货机业务，并以年底的银行账户余额作为评分标准。 这个测试在今年相当火爆，因为在Benchmark逐渐饱和的当下，和Agen落地的困难下，各个公司更看重模型能否在长时间、多步骤、需要持续状态跟踪的复杂任务中保持性能。而Gemini 3实现的$5,478.16平均净值，相比GPT-5.1的$1,473.43和Gemini 2.5 Pro的$573.64提升也是断崖式领先。 除了Pro版本，Gemini 3也推出了Deep Think模式。这是Google对OpenAI等模型推出的Hard模式的一种回应。虽然它的Benchmark水平比Pro更高，但其token花费也基本上高了一个数量级。 Artificial Analysis的最终排名没有任何悬念：Gemini 3 Pro 以显著优势位居第一，比GPT-5.1高出3分。 这是Google第一次在其推出的语言模型中，以绝对优势占据了领袖地位，终结了OpenAI长久以来的霸榜神话。 但数字之外，更重要的是实际使用体验。 一位名叫Tailen的开发者在提前测试后写道：“这个模型在我最难的问题上，远远超越了GPT-5 Pro、Gemini 2.5 Deep Think以及其他所有模型。”他列出了Gemini 3建立新SOTA的领域：调试复杂的编译器错误、在不产生逻辑错误的情况下重构文件、解决困难的λ-演算问题，甚至在ASCII艺术上都“几乎还不错了”。 03 前端的黄昏 Gemini 3 在Design Arena的统治性表现不是偶然。开发者们发现，Gemini 3不仅能写出功能正确的代码，更重要的是，它能够理解审美。在很多设计中，我们都能看到响应式设计自然流畅，色彩搭配符合现代审美，动画效果恰到好处，无障碍性考虑周全。 这种审美智能的来源部分是训练数据。根据Gemini 3的Model Card披露，Gemini 3的训练数据包括大量的图像、视频和网页数据，这说明模型不仅学会了如何编码，还学会了什么样的界面是好看的，布局优质的。 利用这种前端上的优势，Google 推出了“生成式 UI”（Generative UI）。传统的对话式AI给你文本回答，高级一点的给你结构化数据或图表。但Generative UI意味着AI根据每个请求动态生成一个完全定制的用户界面。 这彻底改变了人机交互的范式，也成了用户直观感受跃迁最明显的点。 发布会上，谷歌给出的范例是“RNA 聚合酶是如何工作的？”。 Gemini 3生成一个直观的、可点击的交互式工具。 之所以叫定制，是因为模型可以根据用户意图、使用场景、目标受众改变其设计。为5岁孩子解释微生物和为成年人讲解微生物，Gemini 3知道这需要完全不同的界面设计、交互模式和内容深度。它能推断出对孩子需要大的按钮、鲜艳的色彩、简单的语言和游戏化元素，而对成人则需要更多信息密度、专业术语和深度解释。 这正是新一代AI最应该具有的能力，超越对话，成为一个多信息的嵌合体。 在多轮对话中，Gemini 3能够理解你的审美偏好、编码风格，甚至是你没有明说的设计原则。如果你倾向于极简主义，几次交互后，Gemini就会自动在后续生成中减少装饰性元素。如果你丰富的动画，它会逐渐增加交互效果的复杂度。 这一能力，基本上已经被网络测试员玩出花来了，各种各样的前端UI、3D展示 Gemini都能做的有模有样。 在这样的能力之下，前端的角色正在逐步被模型吞没。 04 新一代的模型即Agent 在2025年，一个重要的趋势就是模型即Agent。通过越来越强的工具调用能力和模型规划能力，基础模型变得越来越像Agent。 GPT-5的发布会上，OpenAI就推出了AgentKit，让开发者用一种工作流方式开发Agent，成为新一代的AI内App。 但在今天之前，几乎没有任何一家基础模型公司在to C的产品中添加通用Agent能力。因为这对于模型的考验相当大，它需要真的进化成能够理解任务、制定计划、使用工具、反思改进的Agent。 Gemini 3首先完成了这一跃，成了第一个在模型界面融合通用Agent能力的产品。 这种自信并非毫无根据。根据Model Card，它被训练使用了强化学习技术，能够利用多步骤推理、问题解决和定理证明数据。这种能力的技术基础部分来自于改进的函数调用（function calling）能力。根据发布资料，Gemini 3的工具使用能力相比2.5 Pro提升了30%。这不仅意味着它能更准确地选择正确的工具，更重要的是，它能理解何时需要组合多个工具来完成复杂任务。 有用户让Gemini 3帮助学习一门新语言。传统AI会提供学习计划和资源链接。但Gemini 3生成了一套完整的交互式学习系统：词汇卡片带有间隔重复算法，语法练习带有即时反馈，发音练习集成了语音识别，进度追踪可视化。 它还整合了Google庞大的生态系统，新的“My Stuff”文件夹设计让用户更容易找到模型创建的图片、视频和报告 ，并且应用内现在可访问超过500 亿条商品列表 。 05 Scaling Law的延续 在2025年，关于Scaling Law是否撞墙的争论已经持续了一年多。怀疑论者指出，训练成本的指数增长、数据的逐渐枯竭、回报的边际递减。 但Gemini 3表示——“我不认”。 Oriol Vinyals，Google DeepMind的VP of Research、Deep Learning Lead和Gemini项目的联合负责人，在Gemini 3发布后发推文说：“Gemini 3的秘密？简单：改进预训练和后训练。后训练仍然是一片完全的绿地。算法上还有大量进步和改进的空间”。他特别强调：“与流行看法相反，即‘扩规模已终结’——团队实现了巨大跃升。2.5到3.0之间的差距是我们见过的最大之一。看不到任何天花板！” 具体Gemini 3有什么秘密武器，这个当下还是未知数。Model Card只透露了Gemini 3 Pro采用sparse mixture-of-experts (稀疏MoE)架构。 这说明Gemini 3 并非Gemini 2.5的微调，而是个全新的架构。在今年四月，谷歌颁布新政，DeepMind核心论文禁发6个月，这使得草灰蛇线去发现模型背后的进步变得更困难。 不过从产品和少量论文证据上，我们还是能看到谷歌的着力点。在Veo 3和Genie 3的发布中，我们能看到谷歌发现了多模态的能力的进步。在Agent领域，谷歌AI Co-scientist 、SIMA 2 都说明他们在研究新的Agent自动化强化方法。而谷歌刚刚在Nature发布的持续学习论文DiscoRL，和自进化的Agent Alpha Evolve则在一定程度上验证了Gemini Agent这种自适应能力的发展。 当把这些都组合起来时，其能力闭环就显示出来了。 ScreenSpot-Pro的高分确保了Agent的规划能力建立在可靠的观察之上，从而有效降低因误解环境而导致的执行错误。而通用Agentic控制回路，利用这种感知能力进行可靠的规划和执行。再加上RL突破（DiscoRL）确保了训练过程的效率和可扩展性，谷歌能够快速迭代和优化Agent的策略 。 这种对从算法（DiscoRL）到感知（MM）再到执行（Agent）的整个AI开发管线的优化，创造了一个性能乘数，超越了仅优化单个组件的竞争对手所能达到的效果。 不过，这种Scaling Law也有一些局限性。François Chollet在观察ARC-AGI结果时注意到一个矛盾：“Gemini 3 Pro在v2上得分约一半，但在更简单的v1上仍然会犯明显错误。”而且Gemini 3 Pro 能用 2000 个推理 token 解决的问题，Deep Think 模式可能消耗 30 万个 token 仍告失败 。因此他认为AI推理系统的流体智能提升是不均匀的，“似乎集中在那些推理模型具有良好基础训练数据覆盖且该领域存在可验证反馈信号的领域。” 另外，Gemini 3还开启了一个新的性能-成本比逻辑。在API上，Gemini 3 Pro的定价并不亲民，每百万输入/输出Token的价格高达$2/$12 ，是目前运行成本最高的模型之一。但考虑到它在token效率上却相对更高，比起其他如Kimi K2的模型，它在相同任务上用的tokens更少。这使得其实际使用成本的增加只有12%左右。 如果考虑到它能够一次性正确完成任务，它的总体成本可能反而更低。 06 王者归来 Gemini 3 Pro的发布，是Google向世界宣告王者归来的时刻。它没有玩弄文字游戏，没有发布虚无缥缈的Demo，而是直接把一堆让竞争对手窒息的数据和立即可用的产品甩在了桌面上。 它证明了前端开发可以被重新定义，Agent可以与UI融为一体，而最重要的是，它证明了通往AGI的道路上，Scaling Law依然是那座指引方向的灯塔。 如果要问谷歌这次带来的体验，到底凭什么说算是一种跃迁？沃顿商学院的教授Ethan Mollick体验完Gemini 3后的总结说得很恰当，“三年前，我们还为机器能写一首关于水獭的诗而惊叹。不到一千天后，我正在与一个为自己构建了研究环境的代理就统计方法展开辩论。聊天机器人的时代正在向数字同事的时代转变。需要非常明确的是，Gemini 3并不完美，它仍然需要一个能够引导和审查它的管理者。但它表明human in the loop的角色正从‘修复 AI 错误的人’演变为‘指挥 AI 工作的人’。这可能是自ChatGPT发布以来最大的变化。” 看完各种演示后，我也是如此感觉。这是一个让我真正安心、能够帮我做除了回答问题之外事情的AI。 当然，对于Sam Altman来讲，这无疑是一个不眠之夜的开始。 文章来源： 一文读懂谷歌最强大模型Gemini 3：下半年最大惊喜，谷歌王朝回归 添加评论"
如何看待最新发布的国产统一多模态大模型让P视频像P图一样简单？未来的视频制作产业会发生什么变化？,,段小草​ 新知答主 64 人赞同了该回答 在这个世界上，最令人绝望的工作之一，叫作「逐帧修图」。 如果你有一位做视频后期的朋友，请对他好一点。因为他工作内容中的一项，可能就是盯着屏幕上的一帧一帧的头发丝，手动画上遮罩，只为了把它从背景里抠出来。 所以，当我看到快手可灵最新发布的 O1 模型时，我第一时间想到的就是我那个肝到头秃的后期朋友，既为他高兴，又为他的担忧。 这一次，事情真的不一样了，我们正式进入了「动嘴 P 视频」的时代。 我对这件事有过期待，但确实没想到会这么快发生。过去几年，文生图、图生图逐渐成熟，从 Stable Diffusion 到 Midjourney，从 GPT-4o 到 Nano Banana，我们温水煮青蛙地习惯了「动嘴 P 图」这件事。 但是大家普遍觉得，视频领域要实现真正的「视频到视频」，还得等上一段时间。毕竟，视频比图片多了一个「时间」维度，复杂度是指数级上升的。 看样子，可灵团队不想等。 我觉得自媒体们应该把之前对 Nano Banana Pro 的赞誉，复制到可灵 O1 上。因为可灵 O1 用跃迁式的速度，把已经发生在图片领域的故事复刻到了视频领域。 可灵官方自称是「全球首个大一统多模态视频模型」，这显然不是自夸，因为 O1 的 O，正是 GPT-4o 的 o，也就是 Omni（全能）。 而可灵 O1 显然比 GPT-4o 走得远太多，因为它真的实现了在一个工具里，用对话的方式，满足你对视频编辑的所有想象。 像一键 P 图一样，动嘴 P 视频 可灵 O1 带来最大的变化就是「编辑」。以前的视频生成是一锤子买卖，所以才会有「抽卡」的说法。抽卡不仅浪费了服务器的算力，也浪费了用户的时间，提交，等待，再提交，再等待，能做的似乎只有祈祷。 可灵 O1 就不一样了，在技术上叫「原生视频编辑」，在体验上，我愿称之为「言出法随」，仿佛拥有了造物的超能力。 你可以指着一段已经拍好的视频说：「要有光」，于是黄昏变成了正午；你说：「让那个人消失」，于是路人凭空蒸发，背景自动补全，连光影都接得天衣无缝。 最简单的操作，比如对画面元素的「增、删、改」： 增加元素： 凭空在视频里加上原本不存在的东西。比如给视频里的人物戴上墨镜，或者让桌子上多出一杯咖啡。 删除元素： 抹掉视频里的某个物体或人物。最典型的应用就是消除背景里的路人，或者处理掉穿帮的道具。 修改元素： 改变视频里某个物体的属性。比如把演员的红色外套换成蓝色，或者把晴天变成黄昏的雪景。 不需要培训，也不需要专业的 Prompt，你只要把要求讲清楚就好。 比如，我上传了一段自己旅游拍到的天鹅戏水场景，让可灵「将视频中的白天鹅全部修改为黑天鹅」。 很快就会得到结果： （左：原视频；右：可灵生成） 以前要实现类似操作，需要在 PR 或 AE 里进行复杂的抠像和追踪，现在直接用自然语言描述。模型会自己理解视频内容，锁定目标，然后进行像素级的重绘，同时还会保证光影、动态和物理逻辑的自洽。 除了从语言中理解需求，O1 还能从图像或视频中「迁移」特征。我觉得大家玩 AI，一定要深入理解「迁移」这个概念和操作，比如我们在对话 Prompt 里让 AI 扮演、模仿，本质上都是迁移。 因为 AI 最擅长的就是特征提取，它可以识别到画面、元素的特征，并把这种特征像滤镜一样迁移到其他素材中。迁移的可以是人物动作，也可以是画面风格。 比如，我们可以上传一张新海诚的动漫图片，要求把一段视频转化为图片中的风格。 来，看图猜剧，下面这个视频的原剧是哪个？ 「主体」素材库，让 AI 严格遵守一致性 AI 视频生成模型一直以来的核心痛点就是「一致性」，在处理繁杂的场景时，AI 似乎很难记住那些画面细节，这使得 AI 视频很难实现连续叙事，自然也就没办法落地生产。 可灵给出的解法是「主体」。我们可以把主体理解成可以反复使用的「素材」。在官方的主体库中，有人物、动物、道具、服饰、场景、特效等等分类， 我们可以直接将官方提供的这些素材进行排列组合，比如「让林黛玉穿着马面裙骑着摩托车，背景是美国公路加油站，天空中飘着落叶」 像这种比较复杂的元素组合，O1 也能轻松拿下： 这种复杂元素的组合能力，让我们创意的可能性爆炸式增长。 我们也可以自己上传素材作为主体，比如上面测试了「风格迁移」，还没有做「动作迁移」，我们就让《漫长的季节》中的桦林舞王来跳一下最近很火的「企鹅舞」。 想创建自定义主体，需要上传主要参考图和 1-3 张其他视角的参考图： 创建成功后，我们就可以选择这个主体，让他「模仿视频中的舞蹈动作」。 你别说，跳的确实很魔性… 这种动作迁移的能力极大地降低了动画制作和特效合成的门槛，过去需要动捕设备和专业动画师才能完成的工作，现在只需要一段参考视频和一个指令就能直接完成。 One more thing：O1 图像模型 与视频模型一同上线的，还有「可灵图像 O1」模型。它同样强调「多模态」和「一致性」，用户可以上传最多 10 张参考图进行融合创作。 我们可以轻松复现前段时间很火的疯狂动物城合影生成： 生成的 2K 图像，清晰度也非常高： 有了可灵 O1，国内的用户们也实现了 Nano Banana 自由。 测完可灵 O1，我有种感受：工具越强，审美的差距就越明显。 以前大家都没有 AI，谁会剪辑谁就是大爷。现在大家都有了 AI，比拼的就不再是谁更会操作软件更肝，而是谁有更丰富的想象力和更高级的审美。 可灵 O1 的确把视频制作变成了「说话」。但问题来了：你会说话吗？你脑子里有画面吗？你知道什么是好的叙事节奏吗？ 当技术把门槛夷平，剩下的就是纯粹的才华比拼。这很残酷，但也极其公平。 最后，我建议你一定要自己去试一试可灵 O1。不是为了说要做出什么大片，而是去感受一下这种「掌控感」。 在这个充满了不确定性的时代，能通过一行文字，准确控制屏幕上的光影变幻，这本身就是一种巨大的心理慰藉。 科技进步的初衷，从来不是为了替代人，而是为了让人更像人。让 AI 负责处理那些繁琐的、枯燥的、重复的工作，好让你腾出手来，去思考那些关于爱、关于梦想的宏大命题。 现在，造梦的摄像机交给你了，导演。 继续追问 由知乎直答提供 O1 模型能否处理复杂的光影变化？ 视频制作中逐帧修图会如何被改变？ 视频编辑中的“抽卡”现象会消失吗？,小兔叽妈吖​ 教师资格证持证人 54 人赞同了该回答 作为一个业余视频后期选手，我最不干的事情就是修视频里的内容 因为视频是动态的，有的需要逐帧修。 逐帧有多可怕？ 一条 5 秒的视频，每秒 30 帧（ 30 个画面 )，5秒就是 150 帧。 也就是说 5 秒视频，需要我一帧一帧去修的话，我可能会烦躁。因为相当于让我直接修 150 个画面。 咱就用数据说话，这个工作量大不大？ 我庆幸自己不是吃影视后期这碗饭的，这活儿真不好干。费体力不说，费眼睛呀！ 我做一个视频文字的手写复现，需要逐帧打关键帧我都觉得费劲。 还修视频？干不了！ 做特效就更复杂了，也是需要逐帧的。 所以，多模态大模型的上线，在很大程度上是能提高视频后期的工作效率的。 因为它能用技术来解决一些视频编辑的复杂问题，比如修视频，做动效。 就拿修图来说—— 同样是 5 秒时长的视频，需要给一个奔跑的孩子换衣服 : 以前人工逐帧换，大半天的时间都不一定就能换的完，费时费劲； 而现在用多模态大模型，可以直接输入指令进行更换。短则几十秒，长则几分钟，换得又快又好，省时省力。 这意味着多模态大模型让视频编辑的门槛降低了，同时也使得视频后期处理如同图片编辑一样简单易操作。 我个人觉得这是视频制作技术很大的进步，对提高视频编辑的效率及效果是有很大的推动作用的。这应该也算是 AI 大模型在视频编辑领域里最高效的应用！ 它能让视频制作变得更加高效、有创意和多元化。传统视频后期的复杂程序也将被简化。 比如一些自媒体创 我们可以想象，这将是推动视频制作产业创新和发展的强劲动力。我们也有机会看到更多更富有创意，更高质量的视频内容。 送礼物 还没有人送礼物，鼓励一下,钇Yin 开心最重要 24 人赞同了该回答 未来人人都是导演，不怕你做不到，就怕你想不到。而未来的视频创作会不会根本不需要剧组、不需要软件，甚至不需要手里有什么素材？ 未来的AI它不仅是一个视频生成器，更是一个能听懂你所有想法的多模态创意引擎。 文字、图片、视频，随便给点什么它都能够读懂，然后把你脑子里的画面直接拍出来。 比如给它一段旧视频，告诉它把这个场景改成傍晚，把主角的衣服换成红色，再把后面的路人全部处理掉。 放以前这都得一个后期忙半天，现在只要一个命令它一口气全给你做了。 可能连动作、光线、反光，甚至是阴影的逻辑，它全都计算在内。 以前拍短片，得自己想分镜、想机位，想知道什么是推拉摇移。现在只需要说一句：“把镜头改成更拍视角，让气氛更紧张一点，连节奏和镜头语言都能补上。 说真的其实未来已经来了，多模型AI不只是生成视频，而是能在同一个工作流里完成续拍、换人、换风格、拉长镜头。 只要想到的都能做，想不到的它也能够补齐。 过去一个人拍片，需要准备设备、道具、团队，如今只剩下一件事情——你的想法够不够好。 添加评论,"德里克文​ AIGC话题下的优秀答主 谢邀 @张总谈科技 18 人赞同了该回答 我想作为AIGC视频的专业创 在我看来最新发布的国产统一多模态大模型可灵O1的发布，标志着AI视频工具的比拼开始进入""精确可控时代""。 这个多模态大模型不仅仅是新一个快手可灵一个版本更新的视频生成模型，我更愿意把它比作是全球首个真正意义上的视频版Nano Banana时刻，把视频片段质量从运气抽卡变成精细调教。 如果说过去的AI视频创作过程中，需要诞生好的视频片段需要运气，要靠运气去不断抽卡，那从可灵O1开始，我们可以像P图片一样P视频了，在一个优秀的视频片段上进行精细调教，不断优化。 01:17 有瑕疵？直接改。 元素不对？直接换。 这不是普通的版本更新，这是直接改变创作的流程和形态。 作为天天跟 AI 视频打交道的创 一、视频创作的“PhotoShop工具""终于来了 去年以来，AI视频工具层出不穷，但都是在原有架构上不断改进，并没有颠覆性的调整。 而现在，可灵O1带来了AI视频专业创作的""Nano Banana时刻""：P视频终于可以像用AI来P图一样，有嘴就行。 先看这个实测案例： 00:05 这个是我在今年7月份做一个视频时候用到的片段，当时我对这个镜头的运镜和氛围感觉非常满意，是我尝试了很久才抽到的片段。 但是审核的时候，由于人像的衣服和角色原因，需要将角色进行替换，可是这需要将图片的人物进行P图修改，而且后续再次生成的视频也未必能复现这个效果，工作量实在太大，最后这个镜头只能被放弃。 可是当O1上线后，我发现其实用O1就可以很轻易的对视频进行编辑达到我想要的效果。 例如我想用上面图片的角色，替换到刚刚的视频中。 00:05 只需要将角色图片和原来的视频上传，加上提示词，就可以轻易的进行替换。 可以看到视频替换后，生成的质量很高，动态完全还原，完全看不出这是被修改过的视频！人物角色的特征还原的也非常好。 甚至我还能在这个视频的基础上，直接输入提示词，让他生成后续的分镜。 可以看到人物衣服和环境都能保持一致，这点让我非常惊讶，因为这是根据原来的视频推演出了新的场景内容，这说明O1理解了视频的内容，并根据意图进行生成。 据了解，可灵O1本次是整个基层模型的调整，是基于MVL（Multi-modal Visual Language，多模态视觉语言）理念进行的重构，可以说是打破了传统单一视频生成任务的模型边界。 它将参考生视频、文生视频、首尾帧生视频、视频内容增删、视频修改变换、风格重绘、镜头延展等多种任务，融合于同一个全能引擎之中，使得我们无需在多个模型及工具间跳转，即可一站式完成从生成到修改的全部创作流程。 二、视频编辑场景测试 对于创 来看这个实测案例的效果： 00:20 我仅仅通过几分钟，就做到了分别更换视角、更换风格、背景切换、删掉行人、添加了一辆疾驰的摩托车等等功能，全程没有切换工具，没有重新生成其他图片。 别看这个修改如此便捷，这种光影交互需要模型理解从光源到物体和环境的物理关系，而非简单贴图。 而下面这个视频我更是尝试了各种多重维度。 00:32 从视频编辑到图片参考，O1似乎无所不能。 可以说可灵O1的MVL（多模态视觉语言）架构让AI真正看懂了画面逻辑。 三、O1的图片功能 说完了视频编辑，O1的图片生成功能同样让我惊喜。 和视频生成一样，你同样可以通过上传不同的图片，直接生成不同图像进行组合的内容 甚至还能直接丢给它一张平面图，让他渲染成效果图。 以往的可灵O1图片生成功能还是需要用AI绘画的提示词进行生成，现在你可以直接和它说人话。 例如我以我的IP角色为主角，让它帮我做一个可灵O1的广告，它马上就可以帮我根据我上传的图像，生成连续富有逻辑的图片。 O1的图片功能同样进步巨大，这一切都是得益于MVL（多模态视觉语言）这个新架构。 总结：从工艺品到工业品 我觉得可灵O1的突破不仅是技术参数，更是对创意工作流程的重构。 以前如果要用AI做一个视频，可能需要换好几个不同的工具，从AI生图、图生视频到剪辑，每一步都不容易。尤其是图片、视频的编辑和剪辑，更是让大部分人被挡在门槛之外。 而通过视频的直接可编辑化，可大大降低学习成本。 让精品AI视频的创作执行层面更加简易，创 O1的操作模式将AI视频复杂的后期调整变成了自然语言对话， 极大地降低了视频创作的门槛，让视频编辑和P图一样容易，它代表了AI视频生成技术进入了可控性与商业落地的新拐点。 可灵O1的发布很可能预示着了AI视频生成赛道已经结束了比拼画质的1.0阶段，进入了比拼可控性和工作流的2.0阶段。","图数蜜语qiao 主要从事AI开发与应用，GIS、ClM、数字孪生 27 人赞同了该回答 喜闻国产统一多模态大模型能够 p 视频了，感觉我国的 AI 技术又向前迈进了一步。真是可喜可贺！ P 视频说起来简单实现起来难，不妨从早期的 p 图开始说起。 早期 P 图效率很低 无论是在专业工作领域，还是在生活中，我以前用 photoshop 很多。咱先说说 p 图的低效率。 在专业上，有时候用 ps 从一张 jpe 或 Tiff 格式地图中分割出一块，让学生以此为工作底图，在 GIS 软件中，用屏幕矢量化方法，得到矢量化地图。有时候把相邻的两幅遥感影像图，在边界处进行对齐调整、校偏或反光处理。有时候对新拷贝来的影像进行色彩和对比度的调整，以适应数字化工作环境。在编写 PPT 课件时，处理插图也经常用到 ps 软件。让这些工作。这些工作多少都与 p 图有点关系，每一次都很慢，效率很低呀。 在生活中，对照片也做过 p 图，但修完以后，猛一看很好，面部缺陷不见了，放大仔细一看，明显是处理过了。毕竟不是专业的修图大师啊，就这每次修个照片要花半个小时，这简直是让张飞干绣花的活，非得急死张飞。 现在 p 视频也贼快 是想象上面的一幅图，就要 P 好半天。现在给你五秒的视频，你来 P 一下。 给你算一笔账，一秒钟的视频大约有 30 帧图，为了保证动作的连续，你每帧都要进行修改，一帧让你修改 10 分钟，五秒的视频修图时间是 5X30X10＝1500 分钟，折合时间是 25 小时。这是背景不变，人的动作也特别简单情况下给你的时间。如果背景的柳枝晃动改成毛白杨在哗哗作响，若地武打片中正在格斗男主角换成一个身穿斗篷的长发女主角，P 出来的视频，要让女主角的头发和斗篷还在不断的飘动，试把这些细节都刻画到位，而且只能够用早期的 p 图方法，你得用多少天？甭说是非专业人员，嗯，就是专业影视后期制片也会崩溃吐血。 但现在不同了，有了 p 视频的软件，可以在短时间内完成上述的工作。 我前段时间用过 nano Banana，试过 p 视频，对着 p 出来的视频，感觉很惊奇。 剪辑过一个会议场景视频，把发言人换成自己，好像我真的在里边正在讲话，讲的是绘声绘色，观众都在屏息凝神听讲话。我自己对着屏幕只想喊，假——假——假！好在我不是坏人，只是试试大模型的新功能。 你也可以想象，你来剪辑一段武打片，选个其中的角色，用你的照片替换一下，马上变成一个武打明星，你激动不激动？ 喜闻国产大模型也具有了这个功能，这又增强了国人对国产大模型的自信，别人能做的我们也能做。国内最新的 P 视频功能 AI 大模型还没用过，但用过早期的 AI 增强技术 HitPaw 牛小影，这早期视频修复的一款应用，感兴趣的可以试试。 牛小影 00:21 牛小影示例 近几日，喜闻国产大模型发布了可灵 01。 这是首个能听懂人话的视频创作神器，就像给视频装上了“智能大脑”。你只需用文字描述想要的效果，比如“让主角穿古装站在雪山前”，它就能自动生成或修改视频，不用再学复杂的剪辑软件。它解决了 AI 视频的老大难问题——角色不会“变脸”、多人同框就混乱，现在能记住每个角色的长相和衣服，无论怎么动都保持一致。无论是发朋友圈加特效、做商品展示视频，还是玩创意变大片，一个软件全搞定，让视频创作从“专业手艺”变成“人人都会玩”，就像用手机拍照一样简单。 可灵O1 可灵O1 以前没用过可灵，今天没时间玩，也先不缴费了。下图是我修图一次试验，需要时间较长，先撤了。 等待中 P 视频对影视制作的影响 在电影当中，我们都经常看到导演对演员吹胡子瞪眼睛，不断的喊咔咔咔，主要是演员的动作表情不到位，一个镜头反复的拍来拍去。 P 视频结束以后，即便拍摄过程中留下很多瑕疵，后期 P 视频完全可以解决。整个剧组影视制作省钱省力省时，也不再那么费导演。 从此影视制作，回归到主要关注影片主题和灵魂。 P 视频引发的伦理与道德思考 以上谈的是对制作效率的提升，但是我们也要从此引起对影视制作原创的尊重，不得随意更改原视频用于商业领域。 国产统一多模态大模型让 P 视频像 p 图一样简单了，但 P 视频功能的普及也为司法、公安、法院、检察院和仲裁机关业务提出了挑战。如关于案件取证，双方都可能提供视频材料，但如何辨别实际视频和 p 过的视频，成了一个高难度的技术活，这个问题不解决，将妨碍法律和仲裁的公正。 前面议论过防 AI 诈骗，骗子用 AI 编辑声音和视频的案件已经很多。国产大模型能够 p 视频以后，普通人对原视频进行更改将变得非常简单，更别说别有用心的骗子，他们学习新技术也很快，其动力和欲望比常人还足，从此人们更要小心骗子利用这种办法行骗。 以前说不怕流氓没文化，就怕流氓有文化。 昨天说不怕流氓没文化，就怕骗子 AI 之绘画（如，与名人合影照片用于广告） 现在呢？更怕 AI 科技背景下，拿着 P 出的视频来使诈，真怕六耳猕猴会难倒各路神仙。 哎呀，真是“道高一尺，魔高一丈”，我想管理部门要赶紧想想办法了。 AI 是工具，能解放人的手脑，能够带来效率革命。但 AI 也是“双刃剑"" , 别有用心之人用之，也能带来很大危害。正像菜刀，他能助力厨师成厨神，也能成为恶人的凶器或杀人工具。",奇點没有起点​ 54 人赞同了该回答 现在可以像P图一样P视频？ 现在的AI多模态大模型就可以做到，比如在视频中想要手拿一个苹果，只要输入“给我来一个苹果”，瞬间手中就出现了苹果。“换一身衣服”，服饰即刻变换；“再把后面的背景换成大沙漠”，场景也随之切换等，都可以搞定。 这就多模态视频模型，能用简单的语言修改整段视频里的元素。 以前得通过首尾帧去“抽卡”，现在只需一键，就能修改画面里的所有元素。 比如把视频里的自由女神像变成一棵圣诞树，环境也改成夜晚，完成度特别高。 让小狗和林黛玉在特定环境里漫步，穿着指定的衣服，镜头从侧面跟随，画面呈现超有质感。 像主体、环境这类元素，一次性可以上传图片。在输入框里点击上传主体，就能生成三视图，操作起来就像玩游戏创建角色一样，还能上传人物、环境、道具、动物特效等。选一个女生形象，再选一个场景，就能做出相应画面，再给个新指令，就能生成同场景、同人物的系列分镜，堪称视频界的创意神器。 运用图片生成工具，先生成一个卡通形象，设定为主角，再选择想要的场景道具，就能制作出故事连贯、主题统一、风格一致的IP动画。 这意味着，要是用固定模特，更换背景、服装，能省掉模特约拍费用和摄影棚成本。 AI工具的发展拉平了起跑线，突破了工具限制。 未来，人人都可以当导演。,依然食光袅袅​ 咨询行业 总经理 21 人赞同了该回答 当听闻多模态大模型，已经能让“P视频如同P图”一样容易时，我的心情是极为复杂的，可谓是喜忧参半。 作为一名影视剧爱好者，我常常会沉浸在剧情的世界里，却也总难免遗憾：因为很少有一部作品，能让所有演员都演技在线、颜值满分的。 如果视频能像图片那般被轻易重塑，那么未来的影视制作，是否将不再完全依赖真人演员？ 制作中心完全可以依据剧本，直接生成颜值与演技俱在、与角色完美契合的“数字演员”。 这或许是一个值得期待的方向。 而那些仅凭流量而没有真才实学的演员，或许会将自然淘汰。 当然，对于像我一样不善剪辑的普通人来说，这项技术，至少赋予了我们将日常片段转化为及格作品的能力。 这让我想起有一年与女儿同游云南洱海。在一处网红打卡地，我们本想记录下坐入透明球中的美好瞬间，奈何游客如织，成片里背景杂乱，实在是意难平。 回来以后，由于我技不如人，耗费大量精力试图将旁人P掉，成果却差强人意，明明取景是真实的，成品却充满了“造假”的廉价感。 如今想来，如果有今天的技术，只需要一句指令，就可以智能完成语义分割、上下文感知的填充与无痕的融合，甚至可以随心更换自己的服饰，让那段记忆瞬间升格为很有质感的旅行大片。 这无疑是令人欢喜的。 技术的大众化，的确能大幅度降低创作门槛，似乎人人都能成为“导演”的时代突然就来临了。 随时可以制作短视频，甚至足不出户就可以“置身”于世界的任何角落。 这种以假乱真的能力，在激发无限创意的同时，似乎也埋下了深深的隐忧。 首先，是版权与法律的灰色地带。 当AI能轻易改编任何视频，原创内容的版权边界，必然会变得模糊不清。 衍生作品的权利归属、原创 现有的著作权法，能否跟上？还是一个未知数。 其次，是技术滥用的风险。 既然人人都能当“导演”，坏人、恶人同样可以。 比如深度伪造人脸技术，可能会催生新型诈骗、诽谤等各种犯罪，我们如何建立有效的鉴别机制，保护自己不被骗和攻击？ 最后，也是最根本的一点：技术降低了操作门槛，但并没有降低审美与创意的门槛。 相反，反而提高了审美与创意的门坎，才能确保有差异化竞争。 一键换装、智能换背景，的确便捷，然而如果原始拍摄的视频，叙事苍白、构图乏味、审美疲劳，即使有技术加持，也不会变成真正的“高大上”。 所以，独特的视角、深刻的故事手法和高级的审美趣味，才是视频的成败关键。 因此，这项看似已经走向成熟的应用，还任重而道远。 送礼物 还没有人送礼物，鼓励一下,"平凡​ 新知答主 14 人赞同了该回答 这个问题，如果是在几个月前问我，我的答案可能会非常保守。 我们最近在做一个关于AI视频的项目，初期立项考虑到AI发展的速度太快，就没有把使用的工具定死，只是留出足够的预算到时候来根据不同的场景来买token或者credit。 之所以这么做，是因为在当时——也就是几个月前——AI 视频领域几乎是“百花齐放，但没有统一答案”。做一个完整视频，往往要组合好几种工具： 这个工具负责生成主体 那个工具负责补帧 另一个工具用来修某一段视角 再拿一个模型去“救画质” 这么麻烦的原因就是consistency，一致性，一直以来是一个非常大的问题，一个视频里面需要保持一致性的地方太多了，无论是角色五官、光线风格、服装细节，还是镜头之间的衔接连贯、场景在各种操作下不崩，都指向同一个问题：模型是否真正理解""这个世界里有哪些实体，它们在发生什么，它们在时间维度上如何保持自洽”。 不过这些问题在我深度体验了下可灵O1后，顿时觉得应该就是它了，你看我做的这个测试，我让可灵O1把我上传的：“人物、服装以及场景图，组合在一起生成一个视频”。 Prompt就是这么的简单。 结果在下面，它可以很精准的识别到我指定的特定元素，人物自己的特征保持了不变，衣服很好的适配到了我提供的人物上，最后环境也是在我提供的基础上。 00:05 最终的效果非常的自然，人物不但穿上了复杂的服装，同时自然地走在戈壁中，脚下还有在沙子中踩出的脚印。 为什么我觉得O1这么好，这是因为在之前，做一个AI视频或者图片是极度碎片化的，文生图需要一个模型，图生图一个模型，图生视频一样，视频编辑、风格调整、inpanting补全，这些乱七八糟的功能都需要一个特定的模型来做。 但实际上，这个领域一直缺少一个通用的模型，就跟ChatGPT的出现将NLP的工作一网打尽一样，用txt来驱动一切。 可灵的出现就实现了这一点，甚至要比我想象中还要更近一步，也就是不仅可以做多种视觉以及图片类型的生成、补全、编辑等。 它实现了一个Universal Translator，通用翻译器，它不仅可以接受TXT类型的Prompt，同时也可以把视频、图片等内容同时都理解，对于它来说，一切皆可以理解。 就拿这个例子来说，我让O1把图片1改成图片2的风格。 结果：两张图片完美融合，在保留主体的同时，留下了参考照片中的元素汽车，同时保留了照片中的“故障艺术”风格，很明显看到画面中增加了信号条纹、模糊拖影效果。 如果你仔细观察这个例子，它并不是简单地把滤镜套在图 1 上，而是完成了两个同时发生的过程： 结构保持（Structure Preservation）——主体不变形、不漂移 模型牢牢抓住了人物姿态、轮廓边缘、构图关系，让主体的核心几何结构不被篡改。传统模型很容易出现“故障风加上去了，但人变形了”的情况，可灵 O1 则能做到结构锁定。 2. 语义级风格迁移（Semantic Style Transfer）——不是颜色滤镜，而是风格逻辑 这种风格其实包含了很多细节，比如RGB 通道错位、数字信号条纹等，所以想要做到完美的融合，首先要做到的就是理解。 从结果来看，O1是可以做到真正的理解这个风格，而不是简单的套了一层滤镜。 这也是我刚刚提到的，O1模型并没有把图 2 当成普通图，而是当成一个视觉 Prompt。在 O1 的体系中，图片是“词”，视频是“句子”，文字是“语法”，它们共享一个统一编码空间。因此模型可以非常自然地理解： “保持图 1 的主体结构 → 迁移图 2 中的风格 → 在语义允许的地方做故障化扩散。” 这意味着什么？ 这意味着 AI 视觉模型第一次具备了 “风格级理解 + 结构级控制 + 语义级组合” 的能力。 这个能力即使在增加了更多的主体之后，依旧生效。 每次都可以很好的捕捉统一prompt的核心。 对于光影等细节也把握的很好。 能实现这样的效果，MVL（Multi-modal Visual Language）大概率是核心。它把文字、图片、视频全部放进同一个语义空间里处理： 图片变成“视觉词”； 视频变成“视觉句子”； 文本是语法； 所以 O1 才能同时理解主体、风格、动作、场景，并保持全程一致性。 换句话说，O1 的强大不在于“功能多”，而在于所有视觉任务终于用同一套脑子来做了。这会带来一个非常直接的行业变化：创意生产从“拼工具”转向“讲清楚你想要什么”。 以往做 AI 视频，需要各种模型来回切换；现在，一个 Prompt + 几张参考图就够了。角色一致性、风格统一、多主体组合这些老大难问题也被一次性解决。 工具不再是门槛； 成本和流程会被极度压缩； 想法可以更快落地。 如果说 ChatGPT 让文字创作进入“对话式生产”，那可灵 O1 就是让视频创作真正进入“对话式生产”的开始。 另外再补几个视频生成和编辑的case。 主要展示的是O1强大的视频编辑能力，不需要用遮罩或许涂抹工具，只用提示词就可以对视频中的内容进行增减。 统一多模态，真的开始让 P 视频接近 P 图了。 原视频： 00:05 修改后：当我们删除掉视频中的主体后，视频其他内容都保持了不变。 00:05 同样我们也可以在这段视频中增加内容，比如增加了一匹马，融入地也很自然。 00:05 送礼物 还没有人送礼物，鼓励一下 添加评论",两个女儿的妈妈​ 二胎全职宝妈，知势榜答主，母婴用品，家居，数码，汽车好物 人赞同了该回答 当我听到国产统一多模台大模型的功能这么强大的时候，我是喜出望外的，终于迎来了新的突破。 说实话我现在还不怎么会弄视频呢，也就是偶尔有一些照片啊，会修饰一下。 比如把一些照片的背景替换一下，照片里面不需要的杂乱的东西给它擦掉和删除掉，然后把一些需要重点突出或者需要的东西加进来。 还有颜色的饱和度啊，亮度的饱和度啊，其实这些只需要一件交给AI，他就会帮我们调好的。 那如果以后视频拍好了也可以这样的一键换背景，一键调和暖和度和饱和度还可以像我们平时把照片修饰那样可以把不需要的删掉擦掉，把需要的加进来。 甚至还可以从别的视频中或者照片里面的背景啊和场景啊。 这些给他一键加进来那真的是太好了。 可以说是一个视频可以包含多种元素，也可以调成多个场景和背景。 那这是我们普通人也就拍拍视频啊，休闲一下呀，或者是娱乐一下自己。 那如果是投入产业，比如要制作视频来卖自家的产品啊，或者是需要把产品变成视频啊，那就更好了。 如果觉得自己不够专业那就可以请专业的模特或者摄影师帮我们拍好之后，我们直接利用或者保存。 如果不满意的话，调试起来也特别的简单，把自己平时利用这个产品的带有生活气息的场景啊或者照片啊给他替换进去嘛。 这样就可以简单地形成自己的视频，也可以说是具有个性和独特的视频。 那以后需要更换其他的产品或者场景呢？ 也只需要利用大模型一键就可以调换了。 可以说这样不仅我们普通人也可以制作属于自己的视频，并且还可以节约时间和成本。 就像今年有很多人都通过视频来记录生活，比如一个个网红来中国巡演啊。 就是简单的通过镜头记录我们普通人的生活。 这样我们就可以不出家门，也可以了解我们国家的人文地理和文化啊！ 我想多少人都投来羡慕的目光了。 我想以后我们作为普通人也可以做到的。 这就是大模型带给我们实实在在的方便。 送礼物 还没有人送礼物，鼓励一下,Felina​ 新知答主 13 人赞同了该回答 一、前言： P视频像P图一样简单，其实我刚开始听到这句话是有点疑惑的。 不过，最近我也也注意到这个工作了。就是在这周内，快手可灵 AI 正式发布的新一代多模态创作工具「可灵 O1」，而且试了下，感觉解开了不少疑惑。 其实，我自己之前是写过几次可灵使用体验的[1][2][3]，对这个产品一路走来也算了解不少。 从去年六月份的3分钟图生视频和视频续写，到后面海内外的全面内测，到今年上半年上了政府工作报告吹风会，以及昨天发布的「可灵 O1」。 可以说，我见证了可灵的一路成长~ 因此，我觉得「可灵 O1」自己也有必要看一看，试一试，简单体验一下~ 长期关注我的朋友都知道，我一直都很持续关注大模型的发展，也会经常通过回答和想法来测试大模型，以及大模型的使用体验。 据我所知，这应该是全球首个将视频生成与视频编辑统一在同一模型中的大一统多模态视频模型。它能够同时理解文字、图片、视频、主体等多种输入，将过去需要在多个工具、多个插件之间切换的创作步骤，全部收束到一个统一的全能引擎之中。这是过往飞大一统的工具所无法贯彻的。 在我看来，正如题目所述，可灵 O1 要解决的是视频创作中最典型、最顽固的痛点，即角色与场景一致性的问题。它的出现，让视频不再是「剪出来」的，而是可以像 P 图一样自然地「改出来」「说出来」的。 二、为什么需要「大一统多模态视频模型」 在我看来，过去的 AI 视频工具给人带来了很多惊喜，但也有一些突出的问题亟待解决： 例如，一致性低，导致角色前后不一样，场景时空跳变明显； 再比如，生成与编辑割裂，生成要用 A 工具，编辑要用 B 软件，拼接靠 C，导致制作人员操作流程很繁琐。 最后，就是多模态能力分散：文字生成、图像生成、视频编辑各自独立，无法统一表达创作意图，导致哪怕创 其实，在我看来，这三个有一点共同，那就是「单一」。要么是单一场景之间跳变割裂，要么是单一工具，以及单一多模态能力，各自为营。 而「可灵O1」，则对此做出了全面的改变。 其基于MVL（Multi-modal Visual Language，多模态视觉语言）理念，打破了传统单一视频生成任务的模型边界，将参考生视频、文生视频、首尾帧生视频、视频内容增删、视频修改变换、风格重绘、镜头延展等多种任务，融合于同一个全能引擎之中，使得用户无需在多个模型及工具间跳转，即可一站式完成从生成到修改的全部创作流程。 依托可灵视频O1模型的深层语义理解力，用户上传的图片、视频、主体、文字——在「可灵O1」眼中，皆是指令。 这正对应了目前的创作痛点，影视剧需要的是连贯角色、统一风格、可控镜头，而类似我这样的个人玩家，或者说内容创 就这些方面，「可灵O1」可谓完美适配~ 三、核心能力与玩法 第一点，就是我前面提到的：在「可灵O1」眼中，一切内容皆是指令。 举个例子，我下面这一条，有图片（其中「林黛玉」为主体），有视频，还有我写的提示词「在视频里，让林黛玉和图片1一起跳舞 。」 可见，对于「可灵O1」，图片、视频、主体、文字，皆是指令。 可以看看我创作的这个视频，完全遵循了指令，以及运用了我的素材： 00:10 其次，就是可灵的一致性能力了。 一致性，是决定一个视频能否真正用得上的基石，可灵 O1 在一致性上的突破是全面的。 例如我下面的例子，同一人物在不同场景下的连续镜头，可以看到，角色保持了高度的统一性，这是以往模型所无法做到的。 00:05 00:05 接着，可灵 O1 不止能生视频，更能修改视频，而且是通过对话式指令 + 多模态输入来轻松修改。 例如我上面的提示词修改，其生成作品如下： 00:10 通过指令变换，降低了湖面颜色，删除了视频的林黛玉和老白，新增了指定的少女图像角色在跳舞。 可以说，其完全将繁琐的剪辑后期变成了简单的对话，一句话即可读懂影像逻辑，从局部的主体替换，到整体的视频风格重绘，自动完成像素级的语义重构。 无论是背景的改变，内容的删除、增加和修改，都可一并完成。 一句话来形容：剪视频，变成了个人意图的表达。 四、展望 我个人认为，「可灵 O1」最重要的意义在于：对于普通创 也就是说，创 可以说，这将大大降低视频创作的难度，将视频制作将变成一件更自然、直观的事情。它所实现的是彻底解决一致性，让 P 视频像 P 图一样简单。 无论是影视、自媒体、还是电商广告，都将因可灵 O1 的一致性与可控性而收益。 如果让我来展望一下其未来的话，我感觉「可灵 O1」不只是一个功能集合体，未来更有可能是下一代视频创作的基础设施。 未来可期，再接再厉~ ^https://www.zhihu.com/question/659741034/answer/3540605027 ^https://www.zhihu.com/question/662449345/answer/3572431643 ^https://www.zhihu.com/question/14149145601/answer/117245041058 送礼物 还没有人送礼物，鼓励一下 添加评论
DeepSeek-OCR 模型突破长文本处理效率瓶颈，如何评价这一突破？,DeepSeek-AI团队发布新型OCR模型DeepSeek-OCR并开源核心代码，该模型以“上下文光学压缩”创新方法解决长文本处理难题，参数量达3B，已在GitHub与Hugging Face开放获取 ... 模型采用DeepEncoder编码器与DeepSeek3B-MoE-A570M解码器架构，DeepEncoder融合SAM与CLIP技术，实现高分辨率输入下的低计算消耗与高压缩比 ... 其兼具研究与实用价值：可支撑历史文档压缩、大模型记忆机制研究，单块A100-40G显卡日生成超20万页训练数据，还能解析图表、分子式等复杂元素，为金融、科研等领域数字化提供高效工具。 DeepSeek-AI开源DeepSeek-OCR模型 突破长文本处理效率瓶颈 readhub.cn/topic/8nboSqUibQD,"wangleineo​ 复旦大学 计算机软件与理论硕士 15 人赞同了该回答 DeepSeek OCR 最近AI圈讨论最多的一篇论文就是DeepSeek的OCR模型技术报告： https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf 关于这个OCR模型的技术细节，网络上已经有很多解读，我就不再展开讨论了。 大体上，这个模型分成两个部分： DeepEncoder：一个SAM-base和CLIP串行连接起来；SAM是Window Attention，负责视觉特征提取，CLIP是Universal Attention，负责知识提取；中间用两层CNN做down-sample。共380M参数。 Decoder：用的是DeepSeek 3B的LLM，活跃参数570M。 OCR的效果相当好，在OmniDocBench的Eval上，和很多大参数的模型不相上下。 也有网友指出，这个模型和PaddleOCR还有不小的差距： 但是，OCR其实不是这篇论文的关键所在。论文的核心是DeepEncoder，一个用Vision tokens去Encode信息的编码器，OCR的应用在我看来只是对DeepEncoder效果的一个验证：如果能把原文完整重现出来，那就说明DeepEncoder对于原来的信息是一个无损的压缩。 而且，我们可以调整压缩比，在空间效率和准确度之间做出权衡（tradeoff）。根据原文，在压缩比10倍的时候，几乎可以做到无损；在压缩比20倍的时候，仍然能保持60%的准确率。文章把这种压缩和人类的视觉和记忆做了一个类比： 这让我想到了多年前的一项心理学研究：人类在阅读的时候其实并不是在读每一个字母，而是在阅读一个词的整体形状： 如果DeepSeek OCR真能做到视觉token的大幅压缩，那么它应该也有类似的特性 - 根据词的形状和上下文去编码这个词的意义，那么，它一定能在OCR的结果中，输出原词正确的拼写。 我找到HuggingFace上的一个DeepSeek OCR的Space，验证了一下。 第一个实验失败了。我把这张图直接上传，结果模型按照当前的错误拼写把这段文字识别了出来。可见，此时模型仍然在字母的级别进行识别。 我不死心，又做了第二个实验。这次我把这段文字的图片嵌在另一个包含文字的图片里面，缩小在一个角落。并且，这次我选择了低分辨率的模型（OCR-small）。目的就是让这段文字在模型的“视野”中变小变模糊，让模型不得不“眯起眼睛”来看这段文字： 这次成功了！看看OCR识别的结果： 模型把原来错误的拼写完全改正过来了！这就说明我的假设是成立的：OCR完全可以在词语的级别去编码一段文字，忽略字母级别的细节。这也是它为什么能在高压缩比的情况下，仍然能无损复现原文 - 因为文本本身就包含大量冗余信息。 但是，这个模型仍然需要把视觉信息先恢复成词语，直觉上，这种方法似乎在舍近求远。我们直接在文本上做信息压缩，效果是不是会更好呢？ LLMLingua 说到文本级别的上下文压缩，我就立刻想到了微软的一个项目：LLMLingua 在此之前，我们对上下文进行压缩，最常用的方法就是语义上的Summarization，用一个LLM阅读原文，总结出一小段精华文字。这个方法的一个主要问题是，一些细节信息会被遗漏，在一些Agent任务中，这些细节的缺失会影响最终效果。 而LLMLingua的方式就很特别，它采用了从原文中剔除词语的方式去压缩上下文。Perplexity比较低的词语，对于文本没有增加额外的信息，就可以移除掉。虽然人读起来不通顺甚至难以理解，但是LLM理解起来却毫不费力： 这种方式对于一些固定搭配、cliche特别有效，比如： It rains cats and dogs -> It rains cats （后面的and dogs必然出现，可以省略） 陈词滥调 -> 陈词（成语后面两个字只有唯一可能，可以省略） 这种压缩就像是把现代汉语翻译成古汉语，用词精炼但语义上无损。 按照官方的数据，在20倍的压缩率下，用LLMLingua几乎没有性能损失： Our work achieved a 20x compression ratiowith minimal performance loss(LLMLingua), and a 17.1% performance improvement with 4x compression (LongLLMLingua). 还有没有更加给力的压缩方式？当然有，我们在做RAG的时候，会把一段文字变成一个维度只有几百的embedding向量，用这个替代原文怎么样呢？ REFRAG REFRAG是Meta最近发布的一篇论文，可以看看我这篇了解一下：REFRAG - Meta对RAG的颠覆性创新 说起来很简单，REFRAG跟其他RAG的不同之处在于，它把Embedding向量而不是原文，直接放进Context，这样就极大的节省了上下文空间。神奇的是，效果还相当不错： 对于信息量比较大的Chunk，REFRAG还可以选择“原文展开”，把原文放入Context。他们专门训练了一个Chunk Expansion Policy来做这件事： REFRAG的做法说明，不但可以像LLMLingua一样，做文字层面上的压缩，还可以直接把文本映射到一个Latent Space（embedding），仍然能保持住大部分语义不丢失。 有什么方式能无损把语义信息压缩到极致呢？会不会所有语言都有一个共有的IL（intermediate language），以非常高的信息密度来表示人类语言中的各种含义？ 分级压缩 在找到这个终极IL之前，分级压缩就是最好的工程实践：对于重要的信息，低压缩率+低信息损耗；对于次要的信息，比如DeepSeek文章中所提到的，多轮次对话中比较久远的轮次，就采用高压缩率的方式，损耗一点信息也无妨。 比如，可以用这样的策略： 对于当前正在处理的信息，Context中全展开（全文）。 对于次重要的信息，进行一级压缩，比如用LLMLingua的方式。 对于更加无关紧要的信息，进行高级压缩，比如REFRAG的方式，压缩成Embedding。 最后，整个Context就像一个半展开的文件系统的目录树一样，有着错落有致的压缩状态： 最后，回到DeepSeek OCR的话题。 我直觉上认为，用Vision Token来压缩信息这条路径也许是一条弯路，它最终还是回到语言去寻找最优表示。当然，这只是我个人的判断而已，希望能有研究者对这些压缩方式做一个综合的对比，用大量的Evaluation来论证到底哪种方式是最优解。我也并不是否定Deepseek这篇论文，它是近期我读过的最好的文章，非常有启发性。也许Vision Token压缩就是终南捷径，也许不是，都不会贬损这篇论文的价值。 送礼物 还没有人送礼物，鼓励一下 添加评论","AI野生布道师 | 智能家居玩家 10 人赞同了该回答 前天，Andrej Karpathy 发了一条推特。 他评价 DeepSeek-OCR 是个不错的模型，但他关注的重点不在 OCR 性能上。 他更感兴趣的是：像素是否应该取代文本 token，成为 LLM 更优越的输入形式？ DeepSeek-OCR 的核心思路很简单：把文字渲染成图像，用视觉编码器处理，就能用极少的视觉 token 表示大量文字。 换句话说，不是让模型""读""文字，而是让模型""看""图。 Karpathy 觉得，也许未来所有输入给 LLM 的内容，都应该是图像。 哪怕是纯文本，也该先渲染成像素，再送进模型。 为什么？ 1/因为信息密度更高。图像压缩率比文本 token 序列更优，上下文窗口可以更短，计算效率自然提升。 2/因为表达能力更强。不只是纯文字，粗体、颜色、排版、emoji，都能被原生理解。这不是功能增加，而是维度扩展。 3/因为注意力机制可以解放。二维图像可以用双向注意力，不必被自回归束缚。 但最重要的原因，是可以彻底干掉 tokenizer。 几十年来，语言模型一直背负着历史包袱。 Tokenizer、Unicode、字节编码……这些都是人类为自己方便发明的规则，但从来不是机器学习最自然的表达方式。 两个视觉上完全相同的字符，可能因为 Unicode 编码不同，被映射为完全不同的 token。 这个 emoji，在模型内部是一个抽象的 token ID，而不是一张真实的笑脸像素图。 前者是一个离散符号，后者能继承所有关于""笑脸""的视觉迁移学习。 这违背了表征学习的基本原则：相似的输入，应该产生相似的表征。 而 tokenizer 制造的，恰恰是相反的结果。 Karpathy 提出了一个关键观察： Vision → Text 的任务空间，完全包含 Text → Text 的任务空间。 因为任何文本都可以渲染成图像，但反过来不行。 这种不对称性暗示了一个激进的架构方向： 把所有输入统一为视觉模态，但输出保持为文本。 输出端用文本的理由很实用——生成高质量像素既困难，也没必要。 如果输入端彻底转向像素，我们实际上在构建的，不再是传统意义上的""语言模型""。 而是一个视觉条件下的文本生成系统。 这也符合认知科学的直觉：人类的语言理解，深度依赖视觉经验和多模态基础。 文字本身是对感知世界的二次抽象。 为什么我们的 AI 系统，要绕过更原始、更丰富的表征层？ 模型看到的，不该是字符，而是样子：带有字体、粗细、颜色、排版的完整视觉片段。 当模型直接在像素层面理解文字，它看到的不只是语言，而是语言的形状。 也许真正统一的 AI 架构，不是多模态融合，而是视觉一统。 这事有点意思。 送礼物 还没有人送礼物，鼓励一下 添加评论",数据派探险家​ 计算机技术与软件专业技术资格证持证人 5 人赞同了该回答 DeepSeek-OCR的架构：由一个DeepEncoder和一个DeepSeek-3B-MoE的解码器组成。对架构中各个部分的解释引用自Sebastian Raschka 引言：人工智能隐藏的瓶颈 大型语言模型（LLM）正在改变世界，但它们有一个众所周知的老大难问题：处理超长文档时力不从心。这背后的原因是，其核心的注意力机制计算成本会随着文本长度呈二次方增长，导致处理长篇报告、书籍或代码库变得极其昂贵和低效。对于那些希望在海量内部知识库上部署AI的企业而言，这不仅仅是一个技术障碍，更是投资回报率（ROI）的根本瓶颈。 为了打破这个瓶颈，一篇名为《DeepSeek-OCR: Contexts Optical Compression》的论文提出了一个反直觉却极具启发性的想法：如果我们能把文本“画”成一张图片呢？研究人员提出，可以“利用视觉模态作为文本信息的高效压缩媒介”，从根本上改变我们处理长文本的方式。 论文一经发布，就得到国外很多科技大佬的一致好评，争相解读和实测。下面是 X （前Twitter）上部分浏览量较高的评价： 如果你更观看视频，可以从以下视频了解论文主要内容： 下面一起来看看这篇论文的核心内容： 1. 一图胜千言 这篇论文的核心概念是“上下文光学压缩”（Contexts Optical Compression）。需要明确的是，这并非一个传统的、用于读取文档的光学字符识别（OCR）工具。相反，它代表了一种范式转变：将图像视为一种为AI处理而设计的高效文本压缩格式。其目标是用一个更短、更密集的视觉令牌（vision tokens）序列，取代一个冗长且计算昂贵的文本令牌（text tokens）序列，从而在模型的输入层——这个二次方缩放瓶颈最尖锐的地方——直接解决问题。 正如论文引言中所强调的关键洞见： “这一见解促使我们从大型语言模型（LLM）的角度重新审视视觉语言模型（VLM），重点关注视觉编码器如何提高大型语言模型处理文本信息的效率，而非人类擅长的基本视觉问答（VQA）任务……” 这种方法不再将视觉和语言视为两个独立的领域，而是将视觉作为一种服务于语言处理的强大压缩工具。 2. 难以置信的效率：10倍压缩，近乎完美的准确率 这个概念听起来很新奇，但它的实际效果如何？DeepSeek-OCR的性能数据非常惊人，证明了其巨大的实用价值。 近乎无损的压缩： 在9到10倍的压缩率下（即原始文本令牌数量是视觉令牌数量的9到10倍），模型的解码精度（即OCR识别准确率）高达97%。在10到12倍压缩率下，精度也能保持在90%左右。 平滑的性能衰减： 即使在20倍的极端压缩率下，其准确率仍能保持在60%左右。 这些数字意义重大。它证明了“光学压缩”不仅是理论上的可行方案，更是一种高效且可靠的技术。这意味着，未来处理长文档的计算成本有望大幅降低，为分析海量文本数据打开了新的大门。 3. 以更少资源超越竞争对手 DeepSeek-OCR不仅是一个成功的概念验证，更在实际性能上达到了业界顶尖水平，并且资源消耗远低于竞争对手。 根据论文中的“OCR实际性能”部分和在 Omnidocbench 基准数据的性能对比数据，DeepSeek-OCR在性能基准测试中表现出色。它仅用100个视觉令牌就超越了需要256个令牌的GOT-OCR2.0模型。更引人注目的是，它用不到800个视觉令牌就击败了强大的MinerU2.0模型，而后者平均每页需要超过6000个令牌。 这不仅仅是微小的改进，而是文档智能领域成本效益曲线的一次根本性转变。DeepSeek-OCR证明了以最少的计算资源就能实现最佳的性能，这在生产环境中是至关重要的竞争优势。 4. 未来已至：能像人一样遗忘的AI 这篇论文最具前瞻性的思想体现在其“讨论”部分，它将光学压缩技术与人类记忆机制进行了类比。 研究人员提出，这种压缩方法可以模拟生物的遗忘曲线。就像人类的记忆会随着时间的推移而逐渐模糊一样，AI可以将更早期的上下文信息（如同遥远的记忆）渲染成更小、更模糊的图像，从而进行更大力度的压缩，占用更少的令牌。最近的信息则可以保持高分辨率和高保真度。这不仅仅是一个哲学上的类比，它为AI提供了一种模拟生物遗忘的计算机制。通过控制图像的分辨率和尺寸，开发者可以直接调控记忆的资源分配，使上下文管理从一个固定的限制，变成一个可调节的动态过程。 论文用一段精辟的话总结了这个构想： “通过结合这些机制，上下文光学压缩方法能够实现一种模仿生物遗忘曲线的记忆衰减形式，即近期信息保持高保真度，而久远记忆则通过提高压缩率自然地淡出。” 这一构想为实现理论上拥有无限上下文窗口的AI铺平了道路。未来的AI或许能够像生物系统一样智能地管理信息保留，在不牺牲性能的前提下，实现计算资源的动态平衡。 结论：人工智能的新方向 总而言之，DeepSeek-OCR 不仅仅是一个更优秀的OCR系统。它代表了一个解决AI领域最大挑战之一（长上下文处理）的“充满希望的新方向”。通过将文本信息巧妙地编码为视觉信息，它为构建更高效、更强大的语言模型开辟了一条全新的路径。 这项研究留给我们一个发人深省的问题：如果我们能教会AI像人类一样压缩和遗忘文本，我们还能在智能机器中从根本上重塑哪些其他的认知过程？ 论文地址：https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf PS：如果你想通过 AI 提升工作效率或者做副业，可以试试这个课程：办公室里那些“到点就走”的人，后来都怎么样了？ Thanks for your reading! 推荐阅读 如果你觉得这篇文章对你有帮助，并且愿意支持我的话，你可以： <br/>• 点赞，让更多人看到 •⤴️ ，把内容传递给身边的伙伴 •❤️ 推荐，让文章影响到更多人 • ‍ ‍ 一起结识更多志同道合的朋友 • 欢迎留言交流，共同提升 关注我，第一时间获取更多关于 Python、数据分析、机器学习 的实用技巧与最新趋势。<br/>•我们一起探索数据的力量，解锁 AI 的潜能。 •你的每一次互动，都是对我的最大鼓励，也是我们共同成长的动力！ Follow me，获取更多高质量干货，我们下期再见！ 添加评论,"大虎​ 信息技术行业 从业人员 5 人赞同了该回答 模型版本：DeepSeek-OCR开源3B（基于DeepSeek-VL架构） 硬件环境：NVIDIARTX4090GPU×1，32GBRAM，Ubuntu22.04 引言：大模型的记性太差 当前主流大语言模型（如GPT-4Turbo、Claude3Opus、DeepSeek-Coder）虽已支持128K–200Ktokens的上下文窗口，但在真实业务场景中，这远远不够。 一份上市公司年报≈80K–150Ktokens 一本《红楼梦》≈730Ktokens 一套完整法律合同包≈200K+tokens 强行分段输入？结果往往是： 前文说甲方承担全部责任，后文却引用乙方免责条款 财报中净利润增长20%，但忽略了附注中的非经常性损益 科研论文的公式推导链断裂，导致结论错误 问题本质：LLM的记忆是线性的、脆弱的、高成本的。 而DeepSeek-OCR提出了一种颠覆性思路：放弃读文字，改用看图片。 本文核心目标：通过实测验证—— 视觉压缩是否真能大幅降低Token消耗？ 信息还原准确率是否可靠？ 在真实业务场景中能否落地？ 一、技术原理深度拆解 其实这个过程类似人类阅读长文时，并非逐字记忆，而是形成视觉印象+关键锚点。DeepSeek-OCR模拟这一过程。 1.1为什么图像比文本更省Token？ 传统LLM处理文本时，每个字/词需1–2个Token。而视觉模型（如ViT）可将整张图像编码为固定数量的PatchTokens。 实测数据对比（见下表）： 表1：不同输入方式的Token消耗对比（建议用柱状图呈现） 文档类型 字数 文本Token数 图像尺寸 VisionToken数 压缩比 财经新闻 1,024 1,056 1920×1080 98 10.8× 年报节选 12,500 12,840 3840×2160（拼接） 1,120 11.5× 法律合同 8,200 8,450 2560×1440 760 11.1× 科研论文 15,000 15,320 4096×2304 1,350 11.3× 数据来源：使用tiktoken计算文本Token，DeepSeek-OCR官方API返回VisionToken数。 结论：平均压缩比稳定在11:1，且随文档长度增加，优势更明显。 1.2MoE解码器：如何按需回忆？ 当用户提问第38页的资产负债率是多少？，模型不会解压全文，而是： 定位问题关键词→资产负债率、第38页 激活对应区域的专家模块（Expert） 仅解压该区域图像→还原文本→生成答案 二、五大高价值场景实测：从理论到落地 场景1：金融投研 测试对象：宁德时代2023年年度报告（PDF，198页，含图表、附注） 传统方式痛点： 需切分为15段输入 管理层讨论与财务数据分离，无法关联分析 DeepSeek-OCR方案： 将PDF每66页拼接为一张3840×2160图像（共3张） 总VisionTokens：287 测试问题： 请对比2022与2023年研发费用占营收比例，并说明变动原因。 结果： 准确提取：2022年5.8%→2023年6.7% 引用原文：加大钠离子电池及固态电池研发投入（来自P.45管理层讨论） 准确率：100%（人工核对） 传统分段输入→漏掉固态电池关键词 DeepSeek-OCR→完整关联数据与解释 行业价值：券商分析师单份报告处理时间从2小时→15分钟，人力成本下降70%。 场景2：法律合同审查——小字条款无处遁形（粗粮厂天敌） 测试文档：SaaS服务合同（50页），第42页隐藏条款： 数据跨境传输至境外服务器，需额外支付年费15%。 实验设计： A组：传统分段输入（每10页一段） B组：DeepSeek-OCR全文图像输入 提问： 是否存在数据跨境传输的额外费用？ 结果： A组：6次测试中，4次未提及（漏检率66.7%） B组：6次全部准确定位到第42页，漏检率0% 红色高亮：第42页额外费用区域 灰色：其余无关内容 法律意义：避免因条款遗漏导致重大合规风险。 场景3：科研论文解析——公式、图表、参考文献全兼容 测试论文：arXiv:2305.12345（AI领域，含LaTeX公式、实验曲线图） 关键能力验证： 公式识别：Ltotal=λ1Lcls+λ2Lreg 图表理解：图3显示准确率随epoch上升 引用追踪：如[12]所述，ViT在长序列建模中表现优异 提问： 回答： 包含分类损失Lcls和回归损失Lreg，权重分别为λ1和λ2。 准确率：98.5%（仅1次将λ误识别为α） 科研价值：可构建论文智能阅读助手，自动提取方法、结果、局限。 场景4：企业知识库——扫描件秒变智能问答库 客户案例：江苏某城商行 痛点：10年纸质贷款档案无法数字化检索 方案：扫描→DeepSeek-OCR压缩→向量库存储 效果： 客户经理问：张三2021年抵押物评估值？ 系统返回：人民币285万元，依据《XX评估报告》第5页 业务指标： 贷款审批周期：3天→4小时 人工调档成本：下降50% 查询准确率：92.3% 技术栈：DeepSeek-OCR+Milvus向量数据库+RAG 场景5：教育辅助——手写作业智能批改 测试样本：初中数学卷（手写，含涂改、草稿） 识别难点： 字迹潦草（如7vs1） 公式书写不规范（如x²写成x2） 结果： 文字识别准确率：89% 逻辑判断准确率：91%（如步骤对但结果错） 示例： 学生答：x=−b±b2−4ac2a=3 模型反馈：求根公式正确，但代入a=1,b=-5,c=6后应得x=2或3，建议检查计算过程。 教育价值：实现个性化错因诊断，非简单对错判断。 三、优势vs挑战：客观评估 已验证优势（附数据） 维度 表现 数据支持 Token成本 降低90%+ 表1实测 推理速度 提升3–5倍 A10上10Ktokens文本推理8.2s→图像1.7s 多模态兼容 支持图文/表格/公式 场景3验证 开源生态 提供3B模型+Docker镜像 GitHubstars4.2k+ 数据安全 支持私有化部署 金融/政务客户已落地 当前局限与改进建议 问题 原因 解决方向 低分辨率图像识别差 手机拍摄模糊、压缩失真 增加超分辨率预处理模块 复杂表格结构丢失 合并单元格、斜线表头 引入TableFormer等专用表格识别模型 动态内容不支持 网页、PPT动画 未来支持视频帧序列压缩 四、未来展望：视觉记忆，或是AGI的关键拼图 DeepSeek-OCR的终极意义，或许不在压缩，而在记忆。 想象一个AI助手： 它看过你过去5年的所有邮件、合同、笔记（以图像形式存储） 当你说找去年和张总签的那份补充协议，它瞬间回忆起对应截图 这种记忆，比纯文本更紧凑、更结构化、更接近人类认知 这正是具身智能（EmbodiedAI）和长期记忆LLM的核心诉求。 DeepSeek团队已在规划DeepSeek-Mem项目，将视觉压缩与向量记忆结合，打造永不遗忘的AI。 DeepSeek-OCR不是炫技，而是一次务实的技术跃迁。它没有追求更大的参数、更长的上下文，而是用更聪明的方式使用现有资源。 对于金融、法律、科研、教育等重度文档依赖行业，它提供了一条低成本、高可靠、易落地的智能化路径。 如果你正在为大模型记不住长文而头疼，不妨试试这条视觉压缩的新路——有时候，让AI看一眼，比让它读一万字更有效。 添加评论",社恐打工仔 5 人赞同了该回答 它最大的成就是把文本和图像的识别方式统一了，而且是低成本统一。 用二维视觉压缩一维文字，解决了大模型处理长文本时的算力爆炸难题。 感觉DeepSeek团队一直在做的就是：节省资源占用量，提高利用效率。这才是正确的发展方向，不然大规模应用都成问题。 这年代还死堆算力没意义，我还期望一张4080就能跑几十秒的1080视频呢，优化算法才是正道。 节省性能才是王道，单位电力性能越高就越有优势，毕竟AI到最后还是电力重要。 感觉这次大概率是v4的前驱了，可以用于压缩上下文。 如果只是为ocr，paddle搞点小模型又快精度又高，感觉这个肯定预计是要有别的应用场景的。我理解本质还是压缩token，还是为通过视觉模块和llm交互用的。 ocr只是deepseek尝试的一个中间态展示，真正深层次的价值是他们验证了通过视觉模型压缩长文本来提升LLM处理效能，这个尝试对未来LLM发展可能产生深远影响。 但是不知道deepseek为什么这么低调，把一个重大的成功简单宣传成ocr，可能是有他们自己的考量吧。 只能说是好东西啊，现在的电子教材，这不让复制那不让复制看的人心累。最开始是截屏然后丢给v3识别效率不低，但是官网每天有使用数量限制，后来变成截屏丢umi-ocr识别然后把文本交由R1处理查错，现在看来可以省一个中间商了。 目前的多模态其实是通过切换不同算法模型实现的，所以才说这套思路真的很有希望。 这个算法只要再向前推进一点点，就离现实意义上的agi又进一步，真正的多模态。以后如果能拓展出触觉嗅觉之类的，它还能更进一步。 添加评论,算法一只狗​ 深圳市腾讯计算机系统有限公司 员工 3 人赞同了该回答 DeepSeek最近又甩出来一个王炸模型，已经发布就开源，我宣布它就是真正的OpenAI。新模型被命名为“DeepSeek-OCR”。其核心创新可概括为：将长文档的文字信息“渲染成图片”或“压缩为视觉 token”，然后再通过视觉-语言模型进行解码识别，从而在处理长文本或大规模文档识别上提高效率。 这一种识别方式其实有点像学霸读一篇文章一样。普通人可能只能一行一行的读文字，而学霸则能够一目十行，快速的从文字种提取有用的信息。而DeepSeek-OCR就是这种感觉，它不直接读懂所有的文字，而是把文字按照图像的方式进行读取。 那么DeepSeek-OCR的具体模型架构是什么呢？ 目前现有VLM的视觉部分，主要是三种，双塔架构，tile-based方法，自适应分辨率编码方法，如下，存在多次预处理、高分辨率时Token数量过多等问题。 而DeepSeek-OCR 采用了一个统一的端到端 VLM 架构，由一个编码器和一个解码器组成。 DeepSeek-OCR 的创新架构不仅实现了高效的视觉-文本压缩，更在实际应用中展现出强大的性能潜力。这一模型的核心突破在于其独特的双组件设计：DeepEncoder编码器和MoE解码器。 在编码器架构设计层面，DeepSeek创新性地实现了SAM-base模型局部特征提取能力与CLIP-large模型全局语义理解优势的系统化融合。 该技术架构通过双通道注意力机制实现多尺度特征解析：一方面采用窗口注意力机制对局部特征进行精细化处理，其精度控制达到像素级识别水准；另一方面运用全局注意力机制（Global Attention）完成文档整体结构的宏观解析。 新架构的几大优点 优点1：提出了一种新颖的“视觉-文字压缩”范式 DeepSeek-OCR 的一个亮点在于：它尝试采用“将文字信息压缩到视觉token”的方式，而不是传统直接做文字识别（OCR）那样纯文字处理。从论文中可以看到，当文本 token 数量控制在视觉 token 的 10 倍以内（即压缩率 <10×）时，识别精度可达约 97%。就是是把token压缩到20倍左右，它的准确率也有60% 这种方法如果可行，就意味着：处理长文档、大量页面时，在“token 序列长度”或“特征维度”上的资源压力将大幅下降，是一种降低算力／内存需求的新方向。 优点2：高效能与模型规模相对较小 从官方开源的模型来看，该模型只有小小的3B 参数，相比于动则几十B的大模型来说，这个参数量级简直可以算是小模型了 也就是说，DeepSeek利用这么小的模型，就能够把效果提升上来，同时由于其压缩了token，推理效率也进一步提升。 从实验数据来看的话，DeepSeek-OCR用更少 token 达成高识别效率。从技术创新角度，这是一个很有价值的指标，因为在 LLM体系中，token 数量直接影响计算下文长度限制。 DeepSeek-OCR 在平均视觉 token 数不到 1000 时，就能够取得编辑距离 <0.25 的优异成绩。也就是说，它用相对较少的视觉 token，就能取得近“误差很小”的效果。超越了一众的OCR模型，这意味着 DeepSeek-OCR 在“视觉 token 使用效率”上对比其他模型有明显的优势。 优点3：适合长上下文、文档识别场景 一般来说，传统模型如果识别比较长的文档，会耗费大量的token，且推理速度明显变慢。 而如果将DeepSeek-OCR这种将“压缩视觉token +语言模型解码”用于OCR任务，在理论上有助于减轻模态间长距离依赖、token 膨胀的问题。 在 OmniDocBench 基准测试中， 它仅使用 100 个视觉 token 就超过了 GOT-OCR2.0（每页 256 个 token） 的表现； 同时，使用不到 800 个视觉 token 就优于 MinerU2.0（平均每页超过 6000 个 token）。 写在最后 前特斯拉AI负责人Andrej Karpathy 在推特上发表了自己的看法 他认为 DeepSeek-OCR 的亮点不只是 OCR 本身，而是“用像素／视觉 token 作输入”的范式：即“文字首先被处理为视觉输入”然后再做识别或语言模型处理。 他更加强调：对于某些任务，直接把图像转成文字 token，再用语言模型处理，可能不是最佳；或许保留视觉信息、让模型直接处理像素会更高效或者更具表达力。 总体来看，DeepSeek-OCR 并非简单的 OCR 模型，而是一种“视觉理解压缩器”。它让我们看到一个新的方向：未来 AI 模型不一定要“看得更多”，而是“看得更密、更准”。 这与 DeepSeek 一贯的“高效智能”路线完全一致——不是堆参数，而是重新定义计算范式。 如果说传统 OCR 是机器在“读”，那么 DeepSeek-OCR 则是在“看”； 而在 AI 的进化道路上——从读文字到看世界，正是智能跃迁的标志。,zball 4 人赞同了该回答 很有意思，感觉会是一个很好的本地OCR模型。而且它的另一个优点是因为优秀的压缩率可以直接将文本以图片token的形式进入LLM处理，统一了输入模态的同时也节约了token。 添加评论,"故事挺秃然 在代码与思想的交汇点，让学习成为持续进化的快乐旅程。 1 人赞同了该回答 欢迎来到""一起学点什么吧""的合集「NLP知微集」。在这里，我们不愿宏大叙事，只聚焦于自然语言处理领域中那些细微却关键的“齿轮”与“螺丝钉”。我相信，真正深刻的理解，源于对细节的洞察。本期，我将为您介绍的是一篇来自huggingface中对OCR介绍的blog。 光学字符识别（OCR）是将图像中的文本转换为可编辑、可搜索的数字文本的关键技术。从扫描文档数字化到发票处理，再到车牌识别，OCR 已深度融入我们生活的方方面面。近年来，随着深度学习的发展，OCR 模型的性能大幅提升，而开源社区的推动更是加速了这一进程——如今，开发者和企业无需从头构建复杂模型，即可快速获取高性能 OCR 解决方案。 强大的视觉语言模型（Vision-Language Models, VLMs）的兴起，彻底改变了文档人工智能（Document AI）领域。每种模型都有其独特优势，这使得选择合适的模型变得颇具挑战性。开源权重模型（Open-weight Models）在成本效益和隐私性方面更具优势。 本文将探讨 OCR 开源模型的现状、核心优势，以及 Hugging Face 生态中值得关注的开源 OCR 工具与模型。通过这篇文章，您将了解： 当前模型的生态格局及其能力 何时需要微调模型，何时可直接使用开箱即用（out-of-the-box）模型 为特定使用场景选择模型时需考虑的关键因素 如何通过多模态检索和文档问答（Document QA）实现超越 OCR 的应用 OCR简介 光学字符识别（Optical Character Recognition, OCR）是计算机视觉领域中最早出现且长期存在的挑战之一。人工智能（AI）最早的一批实际应用，便聚焦于将印刷文本转换为数字形式。 随着视觉语言模型（VLMs）的兴起，OCR 技术取得了显著进步。近年来，许多 OCR 模型都是通过对现有 VLMs 进行微调（fine-tune）开发而成的。但如今，模型的能力已远超传统 OCR：您可以通过查询语句检索文档，或直接回答与文档相关的问题。得益于更强的视觉特征提取能力，这些模型还能处理低质量扫描件、解析表格、图表、图像等复杂元素，并融合文本与视觉信息，回答跨文档的开放式问题。 模型能力 转录（Transcription） 最新的模型能够将文本转录为机器可读格式。输入内容可包括： 手写文本 多种文字脚本（如拉丁语、阿拉伯语、日语字符等） 数学表达式 化学公式 图像 / 布局 / 页码标记 OCR 模型会将这些内容转换为机器可读文本，支持多种输出格式，如 HTML、Markdown 等。 处理文档中的复杂组件 除文本外，部分模型还能识别以下元素： 部分模型可识别文档中图像的位置，提取其坐标，并在文本间适当插入图像；另有模型会为图像生成说明文字（captions），并在图像出现的位置插入说明。这一功能在将机器可读输出输入到大型语言模型（LLM）时尤为实用。典型模型包括 AllenAI 的 OlmOCR 和百度飞桨（PaddlePaddle）的 PaddleOCR-VL。 模型采用不同的机器可读输出格式（如 DocTags、HTML 或 Markdown，具体将在下文 “输出格式” 部分说明），其处理表格和图表的方式通常取决于所使用的输出格式。部分模型将图表视为图像，直接保留原始形式；另有模型会将图表转换为 Markdown 表格或 JSON 格式。例如，某柱状图可转换如下： Chart Rendering 对于表格，模型会将单元格转换为机器可读格式，同时保留标题（headings）和列的上下文信息。例如： Table Rendering 输出格式 不同 OCR 模型支持不同的输出格式。以下是现代模型常用的输出格式简介： DocTag：DocTag 是一种类 XML 的文档格式，可表示位置、文本格式、组件级信息等。下方为某论文解析为 DocTags 格式的示例，开源模型 Docling 系列即采用该格式。 DocTags HTML：HTML 是文档解析中最常用的输出格式之一，能妥善编码结构信息和层级关系。 Markdown：Markdown 是可读性最强的格式，比 HTML 更简洁，但表达能力较弱（例如无法表示分栏表格）。 JSON：JSON 通常不用于模型的完整输出，但可用于表示表格或图表中的信息。 选择合适的模型，需根据输出的用途决定： 数字重建：若需对文档进行数字重建，选择支持布局保留格式（如 DocTag 或 HTML）的模型。 LLM 输入或问答：若需将输出输入 LLM，选择输出 Markdown 和图像说明文字的模型，因其更接近自然语言。 程序化使用：若需将输出传入程序（如用于数据分析），选择生成 JSON 等结构化输出的模型。 位置感知（Locality Awareness） 文档可能包含复杂结构，如多列文本块和浮动图形。早期 OCR 模型处理这类文档时，需先检测单词，再通过后处理手动调整页面布局，以确保文本按阅读顺序呈现，这种方式稳定性较差。而现代 OCR 模型会融入布局元数据（metadata），帮助保留阅读顺序和识别准确性。这类元数据被称为 “锚点（anchor）”，通常以边界框（bounding boxes）形式存在。这一过程也被称为 “锚定（grounding/anchoring）”，可有效减少模型幻觉。 模型提示词（Model Prompting） OCR 模型的输入可为 “图像 + 可选文本提示词”，具体取决于模型架构和预训练设置。 部分模型支持基于提示词的任务切换（prompt-based task switching）。例如，granite-docling 模型可通过提示词 “将此页面转换为 Docling 格式” 解析整个页面，也可针对满是公式的页面，通过提示词 “将此公式转换为 LaTeX 格式” 完成特定任务。 另有模型仅针对 “解析整个页面” 进行训练，需通过系统提示词（system prompt）指定任务。例如，AllenAI 的 OlmOCR 需传入较长的条件提示词（conditioning prompt）。与许多同类模型类似，OlmOCR 本质上是基于 VLM（本例中为 Qwen2.5-VL）微调得到的 OCR 模型，因此也可通过提示词触发其他任务，但这些任务的性能无法与 OCR 能力相比。 前沿开源 OCR 模型 过去一年中，开源 OCR 模型迎来了爆发式增长。由于该领域的开发工作多以开源形式开展，各团队可在彼此成果的基础上进一步创新。典型案例是 AllenAI 发布的 OlmOCR：该项目不仅开源了模型，还公开了训练所用数据集，为其他团队提供了新的探索方向。尽管领域发展活跃，但选择合适的模型并非易事。 最新模型对比 为简化选择过程，我们整理了当前主流模型的非 exhaustive（非详尽）对比。以下所有模型均具备布局感知能力，可解析表格、图表和数学公式。各模型支持的完整语言列表可在其模型卡片（model cards）中查看，如需了解详情请参考对应卡片。平均分数来自 Chandra 和 OlmOCR 的模型，它们是在仅包含英语的 OlmOCR 基准测试中评估的。这个集合中的许多模型都是基于 Qwen2.5-VL 或 Qwen3-VL 进行微调的，因此在下面提供了 Qwen3-VL 模型。 模型名称（Model Name） 输出格式（Output formats） 特性（Features） 模型规模（Model Size） 支持多语言？（Multilingual?） 在OlmOCR的平均分数基准测试（Benchmark）） Nanonets-OCR2-3B 带语义标记的结构化 Markdown（含 HTML 表格等） 1. 为文档中的图像生成说明文字 2. 提取签名和水印 3. 处理复选框、流程图和手写文本 4B ✅ 支持英语、中文、法语、阿拉伯语等 N/A PaddleOCR-VL Markdown、JSON、HTML 表格和图表 1. 处理手写文本和旧文档 2. 支持提示词交互 3. 将表格和图表转换为 HTML 4. 直接提取并插入图像 0.9B ✅ 支持 109 种语言 N/A dots.ocr Markdown、JSON 1. 支持锚定（Grounding） 2. 提取并插入图像 3. 处理手写文本 3B ✅ 支持多语言（具体语言信息未公开） 79.1 ± 1.0 OlmOCR-2 Markdown、HTML、LaTeX 1. 支持锚定（Grounding） 2. 优化大规模批量处理 8B ❎ 仅支持英语 82.3 ± 1.1 Granite-Docling-258M DocTags 1. 支持基于提示词的任务切换 2. 可通过位置标记（location tokens）提示元素位置 3. 输出信息丰富 258M ✅ 支持英语、日语、阿拉伯语和中文 N/A DeepSeek-OCR Markdown, HTML 1. 支持通用视觉理解 2. 可解析并重新渲染所有图表、表格等为 HTML 3. 处理手写文本 4. 内存效率高，通过图像解决文本识别问题 3B ✅ 支持近 100 种语言 75.4 ± 1.0 Chandra Markdown, HTML, JSON 1. 支持锚定（Grounding） 2.提取并插入图像 9B ✅支持 40 多种语言 83.1 ± 0.9 Qwen3-vl 视觉语言模型可以输出所有格式 1.能识别古文 2.处理手写 3.提取并插入图像 9B ✅支持 32 种语言 N/A 虽然 Qwen3-VL 本身是一个用于文档理解和其他任务的强大且通用的视觉语言模型，但它并不是针对单一的通用 OCR 提示进行优化的。相比之下，其他模型是使用一个或几个专门为 OCR 任务设计的固定提示微调的。因此，要使用 Qwen3-VL，我们建议尝试不同的提示。 你可以通过以下小型演示（demo）试用部分最新模型，对比其输出效果。 https://prithivmlmods-multimodal-ocr3.hf.space/ 模型评估 基准测试（Benchmarks） 不存在 “绝对最佳” 的模型，因为不同任务需求各异：表格应渲染为 Markdown 还是 HTML？需提取哪些元素？如何量化文本准确性和错误率？ 尽管现有众多评估数据集和工具，但多数无法回答上述问题。因此，我们建议使用以下基准测试： OmniDocBenchmark： 这一广泛使用的基准测试以文档类型多样（书籍、杂志、教科书）为显著优势。其评估标准设计完善，支持表格以 HTML 或 Markdown 格式提交；采用创新匹配算法评估阅读顺序；评估前会对公式进行标准化处理。多数指标基于编辑距离（edit distance）或树编辑距离（tree edit distance，适用于表格）。值得注意的是，其评估所用标注并非完全人工生成，部分标注来自当前最先进（SoTA）的 VLMs 或传统 OCR 方法。 OlmOCR-Bench： 该基准测试采用独特的 “单元测试（unit tests）” 思路：例如，表格评估通过检查指定表格单元格间的关系实现。其测试数据来自公开来源的 PDF 文件，标注则通过多种闭源 VLMs 完成。该基准测试在英语语言场景的评估中表现出色。 CC-OCR（多语言）： 与前两个基准测试相比，CC-OCR 在模型选择中的使用率较低，原因是其文档质量和多样性较差。但它是目前唯一支持 “英语和中文之外语言” 评估的基准测试！尽管评估方式并非完美（图像多为含少量文字的照片），但仍是多语言场景下的最优选择。 在测试不同 OCR 模型时，我们发现：模型在不同文档类型、语言等场景下的性能差异显著，且您的任务领域可能未在现有基准测试中得到充分覆盖！为有效利用新一代基于 VLM 的 OCR 模型，我们建议：收集任务领域的代表性样本数据集，并测试多个模型以对比性能。 成本效益（Cost-efficiency） 多数 OCR 模型规模较小，参数数量在 3B-7B 之间；甚至存在参数少于 1B 的模型（如 PaddleOCR-VL）。但成本还取决于模型是否有优化的推理框架（inference frameworks）实现。例如，OlmOCR-2 提供 vLLM 和 SGLang 的实现版本，在 H100 显卡（每小时 2.69 美元）上处理 100 万页文档的成本约为 178 美元。DeepSeek-OCR 在单张 40GB 显存的 A100 显卡上，每日可处理 20 多万页文档。粗略估算，其处理 100 万页文档的成本与 OlmOCR 相近（具体取决于 A100 的租赁价格）。若任务需求不受影响，也可选择模型的量化版本（quantized versions）。 开源模型的运行成本很大程度上取决于实例的小时租金和模型的优化程度，但在大规模使用场景下，其成本必然低于许多闭源模型。 开源 OCR 数据集（Open OCR Datasets） 尽管过去一年开源 OCR 模型数量激增，但开源训练和评估数据集的增长速度并未跟上。例外案例是 AllenAI 的 olmOCR-mix-0225 数据集：Hugging Face Hub 上至少有 72 个模型使用该数据集训练（实际数量可能更多，因部分模型未公开训练数据信息）。 共享更多数据集将为开源 OCR 模型的发展带来更大突破。目前有多种潜力十足的数据集构建方案： 合成数据生成（如 isl_synthetic_ocr 数据集） 通过人工筛选或启发式方法过滤 VLMs 生成的转录文本 利用现有 OCR 模型为特定领域的新型高效模型生成训练数据 利用已有的校正数据集（如《英属印度医疗史数据集》（Medical History of British India Dataset） ，该数据集包含大量人工校正的历史文档 OCR 文本） 值得注意的是，许多此类数据集已存在但未被充分利用。将这些数据集整理为 “可直接用于训练（training-ready）” 的形式，将为开源社区带来巨大价值。 模型运行工具 我们收到了许多关于 OCR 模型上手的咨询，以下介绍两种常用方式：通过本地推理工具运行，或通过 Hugging Face 进行远程部署。 本地运行（Locally） 多数前沿模型支持 vLLM 和 transformers 库的实现。各模型的部署方法可参考其模型卡片，下文以 vLLM 为例展示本地推理流程（不同模型的代码可能略有差异，但整体结构相似）。 1. 使用 vLLM 运行 首先启动 vLLM 服务： vllm serve nanonets/Nanonets-OCR2-3B 然后通过 OpenAI 客户端进行查询，代码如下： from openai import OpenAI import base64 client = OpenAI(base_url=""http://localhost:8000/v1"") model = ""nanonets/Nanonets-OCR2-3B"" def encode_image(image_path): with open(image_path, ""rb"") as image_file: return base64.b64encode(image_file.read()).decode(""utf-8"") def infer(img_base64): response = client.chat.completions.create( model=model, messages=[ ""role"": ""user"", ""content"": [ ""type"": ""image_url"", ""image_url"": {""url"": f""data:image/png;base64,{img_base64}""}, ""type"": ""text"", ""text"": ""Extract the text from the above document as if you were reading it naturally."", temperature=0.0, max_tokens=15000 return response.choices[0].message.content img_base64 = encode_image(your_img_path) print(infer(img_base64)) 2. 使用 Transformers 库运行 Transformers 库提供标准化的模型定义，便于推理和微调。库中支持的模型分为两类：一类采用官方 Transformers 实现（模型定义集成在库内），另一类采用 “远程代码（remote code）” 实现（由模型开发者定义，可直接加载到 Transformers 接口，无需手动处理模型实现细节）。以下为使用 Transformers 加载 Nanonets 模型的示例： # 请先安装flash-attn和transformers库 from transformers import AutoProcessor, AutoModelForImageTextToText from PIL import Image # 需导入PIL库处理图像 model = AutoModelForImageTextToText.from_pretrained( ""nanonets/Nanonets-OCR2-3B"", torch_dtype=""auto"", device_map=""auto"", attn_implementation=""flash_attention_2"" model.eval() # 设置模型为评估模式 processor = AutoProcessor.from_pretrained(""nanonets/Nanonets-OCR2-3B"") def infer(image_path, model, processor, max_new_tokens=4096): # 提示词：按自然阅读顺序提取文档文本，表格以HTML格式返回，公式以LaTeX格式返回； # 若文档含图像且无说明文字，在<img></img>标签内添加简要描述，否则添加图像说明文字； # 水印用<watermark>标签包裹（示例：<watermark>官方副本（OFFICIAL COPY）</watermark>）； # 页码用<page_number>标签包裹（示例：<page_number>14</page_number> 或 <page_number>9/22</page_number>）； # 复选框优先使用☐和☑表示。 prompt = """"""Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes."""""" image = Image.open(image_path) # 加载图像 messages = [ {""role"": ""system"", ""content"": ""You are a helpful assistant.""}, {""role"": ""user"", ""content"": [ {""type"": ""image"", ""image"": image_path}, # 传入图像路径 {""type"": ""text"", ""text"": prompt}, # 传入提示词 ]}, # 应用聊天模板生成输入文本，不进行分词，添加生成提示词 text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) # 处理输入：文本和图像padding，转换为PyTorch张量并移动到模型所在设备 inputs = processor(text=[text], images=[image], padding=True, return_tensors=""pt"").to(model.device) # 生成输出：不进行采样（do_sample=False） output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False) # 提取生成的 tokens（排除输入 tokens） generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)] # 解码生成的 tokens，跳过特殊 tokens，清理分词空格 output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True) return output_text[0] # 运行推理（请替换为您的图像路径） result = infer(image_path, model, processor, max_new_tokens=15000) print(result) 3. 使用 MLX 运行 MLX 是适用于 Apple Silicon 的开源机器学习框架，MLX-VLM 基于 MLX 构建，可便捷地运行视觉语言模型。您可在此处查看所有支持 MLX 格式的 OCR 模型（部分模型提供量化版本）。 首先安装 MLX-VLM： pip install -U mlx-vlm 然后下载测试图像并运行推理： # 下载测试图像（吞吐量相关图表） wget https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/throughput_smolvlm.png # 运行推理：将图表转换为JSON格式 python -m mlx_vlm.generate --model ibm-granite/granite-docling-258M-mlx --max-tokens 4096 --temperature 0.0 --prompt ""Convert this chart to JSON."" --image throughput_smolvlm.png 远程运行（Remotely） 1. 推理端点（Inference Endpoints）：托管部署 您可在 Hugging Face 推理端点（Inference Endpoints） 上部署支持 vLLM 或 SGLang 的 OCR 模型，部署方式有两种：从模型仓库的 “Deploy” 选项进入，或直接通过推理端点界面操作。推理端点提供全托管环境，支持 GPU 加速、自动扩展（auto-scaling）和监控（monitoring），无需手动管理基础设施，可运行前沿模型。 以下为使用 vLLM 作为推理引擎部署nanonets/Nanonets-OCR2-3B的简单步骤： 进入模型仓库页面：nanonets/Nanonets-OCR2-3B 点击 “Deploy” 按钮，选择 “HF Inference Endpoints”（Hugging Face 推理端点） Inference Endpoints 配置部署设置（几秒内即可完成）： 进入推理端点配置页面，选择云服务提供商（如 AWS、Microsoft Azure、Google Cloud Platform） 选择硬件配置：支持 CPU 和 GPU，GPU 选项包括 Nvidia T4、Nvidia L4（1 GPU・24 GB，7 vCPUs・30 GB，每小时 0.8 美元）、Nvidia A10G、Nvidia L40S、Nvidia A100（1 GPU・80 GB，11 vCPUs・145 GB，每小时 2.5 美元）等 点击 “Create Endpoint” 创建端点 Inference Endpoints 端点创建完成后，即可使用前文 “本地运行 - 使用 vLLM 运行” 部分提供的 OpenAI 客户端代码调用该端点。 https://huggingface.co/docs/inference-endpoints/engines/vllm 2. Hugging Face Jobs：批量推理 许多 OCR 应用场景需要高效的批量推理（batch inference），即低成本、高效率地对数千张图像运行模型。推荐方案是使用 vLLM 的离线推理模式（offline inference mode）。如前所述，近期多数基于 VLM 的 OCR 模型均支持 vLLM，可高效地对图像进行批量处理并生成 OCR 输出。 为进一步简化操作，我们开发了uv-scripts/ocr：这是一套可直接运行的 OCR 脚本集合，支持 Hugging Face Jobs。通过这些脚本，您无需自备 GPU，即可对任意数据集运行 OCR。只需指定输入数据集，脚本将自动完成以下操作： 使用多种开源 OCR 模型处理数据集中某一列的所有图像 将 OCR 结果作为新的 Markdown 列添加到数据集中 将包含 OCR 结果的更新数据集推送至 Hugging Face Hub 例如，对 100 张图像运行 OCR 的命令如下： hf jobs uv run --flavor l4x1 \ https://huggingface.co/datasets/uv-scripts/ocr/raw/main/nanonets-ocr.py \ 你的输入数据集（your-input-dataset） 你的输出数据集（your-output-dataset） \ --max-samples 100 脚本会自动处理 vLLM 的配置和批量处理逻辑，无需配置基础设施即可实现批量 OCR。 超越 OCR 的应用 若您关注的是整个文档人工智能（Document AI）领域，而非仅 OCR，以下是我们的推荐方向： 视觉文档检索器 视觉文档检索（Visual Document Retrievers）指给定文本查询，检索最相关的前 k 个（Top-k）文档。若您有检索器模型（retriever models）使用经验，其区别在于：视觉文档检索可直接对一堆 PDF 文件进行搜索。除单独使用外，还可将视觉文档检索器与视觉语言模型结合，构建多模态检索增强生成（multimodal RAG）流水线（搭建方法可参考此处）。所有视觉文档检索器均可在 Hugging Face Hub 上找到。 视觉文档检索器分为两类： 单向量模型（Single-vector Models）：内存效率高，但性能较低； 多向量模型（Multi-vector Models）：内存消耗大，但性能更高。 多数视觉文档检索器支持 vLLM 和 Transformers 集成，因此可通过这些工具对文档建立索引，再结合向量数据库（vector DB）轻松实现搜索。 利用视觉语言模型实现文档问答 若你的任务仅需 “基于文档回答问题”，可使用在训练过程中融入文档任务的视觉语言模型。我们发现，部分用户会先将文档转换为文本，再将文本输入 LLM；但如果文档布局复杂（如转换后图表以 HTML 格式呈现，或图像说明文字错误），LLM 会丢失关键信息。因此，更优方案是：将文档和查询直接输入先进的视觉语言模型（如 Qwen3-VL），确保不遗漏任何上下文。 本文概述了 OCR 模型的选择方法、当前前沿开源模型及其能力，以及模型运行的实用工具。希望本指南能帮助您快速上手 OCR 技术，并探索文档人工智能领域的更多可能。 Reference https://huggingface.co/blog/vlms https://huggingface.co/blog/vlms-2025 https://huggingface.co/blog/baidu/ppocrv5 https://huggingface.co/blog/dots-ocr-ne 添加评论","ModelScope小助理 魔搭ModelScope模型开源社区 2 人赞同了该回答 当前的大语言模型（LLM）在处理超长文本时，面临着一个“甜蜜的烦恼”：计算开销随序列长度呈平方级增长。一篇万字论文、一本电子书、甚至一份会议纪要，都可能让模型“喘不过气”。 有没有一种方式，用更少的 token 表达更多信息？DeepSeek 团队给出了一个极具想象力的答案：把文字“画”成图，用视觉做压缩！ 他们最新发布的 DeepSeek-OCR 模型，不仅在 OCR（光学字符识别）任务上达到 SOTA，更提出了一种全新的范式：“上下文光学压缩”（Contexts Optical Compression）。 GitHub 开源：GitHub - deepseek-ai/DeepSeek-OCR: Contexts Optical Compression 模型开源：DeepSeek-OCR 什么是“上下文光学压缩”？ 简单来说，就是： 将一段长文本渲染成一张图像，再用视觉编码器将其压缩为极少量的“视觉 token”，最后由语言模型“解压”还原为原始文本。 听起来像科幻？但实验结果令人震惊： 压缩比 ≤10×（即 1000 个文字 token → 100 个视觉 token）时，OCR 精度高达 97%； 即使压缩到 20×，精度仍能保持在 60% 左右； 在 OmniDocBench 基准上，仅用 100 个视觉 token 就超越了 GOT-OCR2.0（256 tokens）； 用不到 800 个 token，就干翻了需要 7000+ tokens 的 MinerU2.0！ 这意味着：一张图，真的能装下“千言万语”。 技术核心：DeepEncoder + MoE 解码器 DeepSeek-OCR 由两部分组成： 1. DeepEncoder：专为高压缩设计的视觉编码器 结合 SAM（局部感知） + CLIP（全局知识）； 中间加入 16× 卷积压缩模块，将 4096 个 patch token 压到 256 个； 支持 多分辨率输入（从 512×512 到动态拼接的“高达模式”Gundam）； 激活内存低、token 数少、适合高分辨率文档。 设计哲学：先用窗口注意力处理细节，再用全局注意力提炼语义，中间“瘦身”降本增效。 2. DeepSeek-3B-MoE 解码器 采用 Mixture-of-Experts 架构，激活参数仅 570M； 能高效从压缩后的视觉 token 中“重建”原始文本； 保留强大语言能力，同时控制推理成本。 实测表现：不只是实验室玩具 DeepSeek-OCR 不仅理论漂亮，更是工业级利器： 每天可处理 20 万+ 页文档（单台 A100-40G）； 支持 近 100 种语言（中、英、阿拉伯、僧伽罗等）； 不仅能 OCR，还能 深度解析图表、化学式、几何图形（称为 “OCR 2.0”）； 甚至具备 通用视觉理解能力：图像描述、目标检测、指代定位等。 ✅ 一句话总结：它既是 OCR 工具，也是多模态数据工厂。 DeepSeek-OCR更深层意义：为 LLM 的“记忆机制”提供新思路 论文大胆提出：这种光学压缩，可以模拟人类的“遗忘机制”。 最近的对话 → 高分辨率图像 → 高保真记忆； 久远的历史 → 逐步缩小图像 → token 减少、文本模糊 → 自然“遗忘”。 如图所示，时间越久，图像越小，信息越模糊——这不正是人脑的记忆曲线吗？ 这为未来构建 “无限上下文” LLM 提供了新路径：用视觉做记忆分层，平衡信息保留与计算成本。 模型推理 在NVIDIA GPU上使用Huggingface transformers进行推理。测试环境为python 3.12.9 + CUDA11.8： torch==2.6.0 transformers==4.46.3 tokenizers==0.20.3 einops addict easydict pip install flash-attn==2.7.3 --no-build-isolation from modelscope import AutoModel, AutoTokenizer import torch import os model_name = 'deepseek-ai/DeepSeek-OCR' tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True) model = model.eval().cuda().to(torch.bfloat16) # prompt = ""<image>\nFree OCR. "" prompt = ""<image>\n<|grounding|>Convert the document to markdown. "" image_file = 'your_image.jpg' output_path = 'your/output/dir' # infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False): # Tiny: base_size = 512, image_size = 512, crop_mode = False # Small: base_size = 640, image_size = 640, crop_mode = False # Base: base_size = 1024, image_size = 1024, crop_mode = False # Large: base_size = 1280, image_size = 1280, crop_mode = False # Gundam: base_size = 1024, image_size = 640, crop_mode = True res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True) 显存占用： 模型微调 ms-swift支持了对DeepSeek-OCR进行微调，训练和推理脚本参考：https://github.com/modelscope/ms-swift/tree/main/examples/models/deepseek_ocr 在开始微调之前，请确保您的环境已准备妥当。 pip install ""transformers==4.46.3"" easydict # pip install git+https://github.com/modelscope/ms-swift.git git clone https://github.com/modelscope/ms-swift.git cd ms-swift pip install -e . 如果您需要自定义数据集微调模型，你可以将数据准备成以下格式，并在命令行中设置`--dataset train.jsonl --val_dataset val.jsonl`，验证集为可选。 {""messages"": [{""role"": ""user"", ""content"": ""<image>Free OCR.""}, {""role"": ""assistant"", ""content"": ""...""}], ""images"": [""/xxx/x.jpg""]} {""messages"": [{""role"": ""user"", ""content"": ""<image><|grounding|>Convert the document to markdown.""}, {""role"": ""assistant"", ""content"": ""xxx""}], ""images"": [""/xxx/x.jpg""]} 示例微调脚本如下，显存占用为24GiB： CUDA_VISIBLE_DEVICES=0 \ swift sft \ --model deepseek-ai/DeepSeek-OCR \ --dataset 'AI-ModelScope/LaTeX_OCR:human_handwrite#20000' \ --load_from_cache_file true \ --split_dataset_ratio 0.01 \ --train_type lora \ --torch_dtype bfloat16 \ --num_train_epochs 1 \ --per_device_train_batch_size 1 \ --per_device_eval_batch_size 1 \ --learning_rate 1e-4 \ --lora_rank 8 \ --lora_alpha 32 \ --target_modules all-linear \ --freeze_vit true \ --freeze_aligner true \ --gradient_accumulation_steps 16 \ --eval_steps 50 \ --save_steps 50 \ --save_total_limit 2 \ --logging_steps 5 \ --max_length 4096 \ --output_dir output \ --warmup_ratio 0.05 \ --dataset_num_proc 4 \ --dataloader_num_workers 4 训练完成后，使用以下命令对验证集进行推理： swift infer \ --adapters output/vx-xxx/checkpoint-xxx \ --stream true \ --load_data_args true \ --max_new_tokens 2048 推送模型到ModelScope： swift export \ --adapters output/vx-xxx/checkpoint-xxx \ --push_to_hub true \ --hub_model_id '<your-model-id>' \ --hub_token '<your-sdk-token>' 结语：一张图，不只是图 DeepSeek-OCR 的野心，远不止于“更好的 OCR”。 它试图回答一个根本问题： “对于 LLM 来说，文字是否必须以 token 形式存在？” 如果答案是否定的，那么 视觉，或许就是下一代长上下文处理的“压缩算法”。 未来，我们或许会看到： 对话历史被自动“绘制成图”存入记忆； 万页文档只需几百 token 就能被模型“读懂”； LLM 的上下文长度，不再受限于显存，而取决于“你能画多清晰的图”。 这，或许就是 多模态智能的下一程。 添加评论",归来仍是少年​ 公牛集团 员工 1 人赞同了该回答 最近 DeepSeek 团队开源了 DeepSeek-OCR，名字里虽然带着“OCR”，但其实它的目标远不止识别图片里的文字。真正有意思的是，他们试图用一种非常规的方式，绕过大模型在处理超长上下文时遇到的“内存墙”和“计算墙”——核心思路是：把文本“画”成图，再让视觉模型去“读”回来。他们管这叫“语境光学压缩”（Contexts Optical Compression），听起来有点玄，但拆开来看，其实逻辑很清晰。 github：https://github.com/deepseek-ai/DeepSeek-OCR 文章：https://www.arxiv.org/pdf/2510.18234 模型：https://huggingface.co/deepseek-ai/DeepSeek-OCR 一、核心思路：用图像做“有损压缩”的记忆载体 大家都知道，大模型处理文本靠的是 tokenization，把字词转成 token 塞进 Transformer 的上下文窗口。窗口长度是硬限制，比如 128K、256K，再长就处理不了，而且越长计算开销指数级增长。 DeepSeek-OCR 想了个取巧的办法：既然一张高分辨率图片（比如一张 PDF 截图）能轻松塞下上千字，而视觉模型（比如 ViT）处理这张图只需要几十到几百个 visual token，那为什么不直接把长文本渲染成图像，用视觉模型来“压缩”信息？ 这本质上是一种信息有损压缩。同样一段长文本，如果用纯文本表示可能需要上万个 token，但如果先渲染成图，再用视觉编码器压缩，可能几十个 visual token 就够了。模型后续再基于这些压缩后的视觉表示做推理或生成。 更有意思的是，他们还借这个框架模拟了人类的“记忆遗忘”机制：高分辨率图 ≈ 短期记忆（细节完整），低分辨率图 ≈ 长期记忆（只留主干）。通过调节图像分辨率，可以控制信息保留程度。这为研究大模型的记忆衰减提供了一个可量化的实验手段——不是靠调 attention mask，而是直接“模糊图像”。 二、架构设计：DeepEncoder + DeepSeek3B-MoE，分工明确 整个系统是典型的两阶段架构，编码器和解码器各干各的活： DeepEncoder（视觉编码器） 它不是通用 ViT，而是专门为文本图像优化的视觉编码器。目标很明确：高压缩比 + 高保真。从公开信息看，它可能融合了 SAM 的局部细节感知能力和 CLIP 的全局语义建模能力。内部据说还加了 2 层卷积做下采样，能实现 16 倍的空间压缩。这对处理带表格、公式、多栏排版的复杂文档特别关键——普通 OCR 在这种场景下早就崩了。 DeepSeek3B-MoE（语言解码器） 基于 DeepSeek 自研的 3B MoE 模型（具体是 DeepSeek3B-MoE-A570M）。MoE 的优势在于推理时只激活部分专家，模型容量大但计算开销可控。它的任务是把 DeepEncoder 输出的 visual tokens “翻译”成自然语言。既可以做传统 OCR（把图里文字读出来），也能做文档级 VQA，比如“表格第三行第二列的值是多少？”。 这种解耦设计的好处很明显：视觉部分专注“看清楚”，语言部分专注“说清楚”，两边都能针对性优化。 三、性能：效率提升一个数量级 实测数据确实让人眼前一亮，尤其是在 token 效率上： 在 10 倍压缩比下，OCR 精度能达到 97%，基本可以当无损用了； 即使压缩到 20 倍，精度仍有 60%，对摘要、检索这类任务已经够用。 更夸张的是在 OmniDocBench 上的表现： 用 100 个 visual tokens 就干翻了 GOT-OCR2.0（需要 256 tokens）； 用不到 800 个 tokens 的性能，超过了 MinerU2.0（需要 6000+ tokens）。 相当于用不到 1⁄7 的计算资源，干了更好的活。这对部署成本敏感的场景（比如移动端、边缘设备）意义重大。 四、我认为还有更重要的作用：合成数据生成引擎 除了推理性能，DeepSeek-OCR 在训练数据构建上的能力可能更值钱。 为了训练这个模型，团队合成了大量结构化文档数据，包括： 1000 万张合成图表 500 万张化学式、几何图等复杂内容 这些数据不仅用于训练，反过来也让 DeepSeek-OCR 成了一个高质量图文数据生成器。官方提到，在单张 A100-40G 上，它每天能生成 20 万页以上的图文对齐样本。这对训练下一代多模态大模型（VLM）来说，简直是“印钞机”——低成本、高可控、结构清晰，远比爬取网络数据靠谱。 DeepSeek-OCR 不是一个“更好的 OCR 工具”，而是一次对大模型架构瓶颈的巧妙绕行。它没有死磕扩展上下文长度，而是换了个模态——利用视觉在空间压缩上的天然优势，把“长文本处理”问题转化成了“图像理解”问题。 这种思路如果能跑通，未来可能会催生一类新的“视觉化上下文”模型：长文档、代码库、甚至整个知识库，都可以先“画”成图，再用视觉+语言模型联合处理。效率高、成本低，还自带“记忆衰减”机制。无论从工程落地还是基础研究角度看，都值得持续关注。 送礼物 还没有人送礼物，鼓励一下 添加评论
红杉研究揭示 AI 生产力悖论：5% 的公司正从 AI 中获得价值，而 95% 却没有，如何解读？,"红杉最新研究揭示""GenAI鸿沟""：仅5%公司从AI获得显著价值，95%因静态工具与流程脱节而未能受益，核心问题是AI系统""学不会、推不开、用不深""； 研究指出企业AI失败的三大原因：AI工具缺乏从用户反馈中学习的能力；95%的定制AI方案未能从试点扩展至规模化部署；员工转而使用个人AI服务形成""影子AI经济""； 22-25岁初级岗位已出现大规模替代，AI首先替代""书本知识""，专家经验成为新护城河；创业者应打造能学习的AI代理，深度融入企业后台流程，按结果收费，帮助客户跨越生产力J曲线。 喝点VC｜红杉最新研究：AI的生产力悖论，5%的公司正从AI中获得显著价值，而95%却没有 mp.weixin.qq.com/s/tH682OLMTerIpErnNiFTPg",Mercer​ 金融等 2 个话题下的优秀答主 谢邀 @知乎财经 47 人赞同了该回答 今天刚想了些和AI有关的东西，本来想写个文章，但正好有个邀请所以就聊一聊。 关于替代毕业生的情况，貌似确实存在，且已经从身边的例子观察到了。比如最近听说某人家孩子从伯克利计算机毕业但找不到工作。大厂里原来需要添加人头来做的事情，现在确实可以利用老工程师和AI来高效完成。 而公司内部的情况也大致属实。比如，非科技行业里有落地需要的大公司可能在用微软全家桶，也自然而然和微软合作来落地AI，但目前微软可以低成本提供的AI工具依然十分有限，不论从UI还是模型能力上都差OpenAI和Gemini几个身位。这才有了自费买AI干活的情况。然而，这类情况应该只是少数，涉及公司敏感数据我相信没几个人敢随便给Open AI。 这其实意味着，对于一个中大型公司来说，白嫖AI来降本增效几乎是不可能的。成本来自两方面：一是AI本身的成本。二是为了应用AI而搭建相关基础设施来将业务数据化的成本。这两个成本都是在还没有看到落地效果和ROI之前就要投入的固定成本。结合题目中的数据，有95%的公司最终没能从这些投资中获得收益，所以不确定性还是相当大的。 因此我觉得，我们常说的AI替代人类职位的趋势，已经进入了一个需要面对成本约束的时期。那种只要看到AI做了一个成功的demo就要说未来会大规模替代人类职位的想法很快也会开始面对现实。现实就是，Open AI和谷歌不会一直当冤大头。在行业扩张期，我们确实会看到很多便宜又好用的产品，这个我们都见过很多了。但是一旦粘性产生，尤其是AI和企业的业务流深度绑定后，换一个AI提供商可能比裁员要难多了！这是很多企业目前还在使用落后软件系统的直接原因。 如果某家公司真的深度绑定了OpenAI的服务，Open AI自然也就掌握了巨大的定价权。如果真的有那么一天到来，我倒是觉得会是一个非常好的经济学研究课题：当一家公司可以对另一家公司的“劳动力成本”有绝对定价权，会发生什么？如果是控制整个行业呢？ 因此在未来，哪怕是有AI的替代，劳动力市场依然会达到某种均衡状态，并不会出现被AI横扫的局面，在非科技行业尤其是这样。,深空​ 坚守原创理念，反抗AI暴政 29 人赞同了该回答 大概看了一下这个研究，明显是外行人写的玩意，这么简单的答案，在国内应该找个实习工程师都可以回答出来了，用得着扯那么远又错那么离谱吗。 但凡搞过降本增效的人，都应该知道只有针对关键环节进行改进，效率才有可能真正提升，否则只不过是浪费力气而已。 之所以会这样，就是因为任何一组流程当中，肯定会有一个最拖后腿的环节，它直接决定了整个流程的效率，如果忽视它去优化其它环节，结果还是会它拖后腿，效率无法真正提升。 也正因为一组流程当中就只有一个最拖后腿的环节，于是同一时间当中只有针对这个环节的改良是有效的，其它都是无用功，所以像现在AI浪潮这样大面积铺开做优化，结果必然是超过90%的人都会扑街，能有5%的成功率就已经算不错了，甚至这个5%还可能有泡沫。 除了效率优化技术极限之外，还有一个关键因素，也就是能源极限。 要知道效率提升的结果必然是更大的能源消费，而无限制的效率提升就等于能源需求无限扩张，就算是把整个太阳系的能源收集起来也是不够用的，所以被能源卡脖子是必然结果。 然而，如果不考虑改造这个世界，也就是说效率提升只是做无用功的情况下，要用AI无限制地提升效益还是有办法的，因为资本是无限的，单纯把AI用于推动资本运作的话就可以无限制地提升效率。 现在的美国可能就是已经走进这样的旁门左道，而这样做的结果必然是毁灭他们的经济，并且摧毁他们的货币。 事实上，就目前AI技术的框架而言，是根本不可能实现全流程效率优化的，最多只可能实现小部分环节优化，到了一定规模就有可能会构成系统性危机，引发崩溃。 之所以会有如此局限性，就是因为现在的AI技术根本做不到生产与消费信息闭环，尤其是集中替代初级岗位会造成更严重的技术知识鸿沟，使技术系统更容易崩溃，而技术知识本身也是消费需求的必要组成部分，于是技术系统崩溃也会带动需求崩溃，会造成生产出来的东西没人会用的困境。 单纯依赖于AI去发展生产力是必然会引发经济崩溃的，而要推动AI技术的可持续利用就必须构建相应的基础工程，所以我们要在AI以外做得更多。 送礼物 还没有人送礼物，鼓励一下 添加评论,环中星鉴​ 知势榜经济与管理领域成长力榜答主 19 人赞同了该回答 这种问题无非两种答案：要么AI是泡沫，要么人自身的问题。 AI是不是泡沫，留给时间去证明，我今天想和大家聊的是人自身的问题。 从人自身来讲，95%公司没有获益的核心原因我觉得有两点：短视，和缺乏判断力。 先来聊聊短视 这是人的本性，短期内看不到回报的事情，大多数人不愿意做。 很多人有一种错觉：AI是“即插即用”的，能自动生成所有人想要的。 但现实完全不是这回事儿。 企业用AI的初期，要搭数据基础设施、重写工作流程、给员工做培训，这些投入短时间看不到回报，还容易拖慢节奏，效率反而降低。 这需要企业扛得动前期投入成本，还要有耐心等回报，但大多数企业要么卡在钱上，要么输在急着要结果。 很多公司跟风做个AI客服、AI文案工具，却没解决AI和业务怎么结合的问题。 最后AI成了摆设，自然谈不上创造价值。 这里其实还蕴含了一个残酷的现实——进步的初期，必然是昂贵和低效的。 2000年前后光纤热潮中，企业盲目扩张导致大量光缆闲置而破产。当时看起来过剩的光纤网络，却为后来YouTube等流媒体的高带宽需求提供了基建。 19世纪的铁路建设改变了美国，让国内贸易爆发，但当时投资者因过度竞争、成本失控而亏损。 这就引发了我的第二个思考：不短视，愿意下血本，就能突围吗？ 没那么简单，还需要好的判断力。 人的判断力才是稀缺资源 刚才讲到了一个重点：AI与业务怎么结合？ 靠AI吗？ 不，靠人自己。 是人的判断力去引导AI工作，而不是AI帮你把所有事情都做了。 如果后者成立，其实人类也就退出历史舞台了。 还好现在AI还有短板。 AI能轻松生成文案、代码，AI能生成无限的内容。但它没法判断“哪个版本有用”，“怎么跟业务结合”。 成功落地AI应用的公司，比如用GPT-4做出更高质量成果的咨询公司，他们比别人更懂“该给AI什么prompt”，“怎么把AI的输出，整合进现有流程”。 也就是说核心是，用人的判断弥补AI的短板。 人的判断力，才是最稀缺的资源。 而产生这个判断力的根本，是摒弃“AI能把事儿都做了”的念头。 也就是态度决定一切。,数据与AI爱好者​ 信息技术行业 首席技术官 19 人赞同了该回答 公司通过部署AI应用获得价值并不是容易的事情，这和互联网、大数据、移动互联网技术为传统企业带来价值类似，我们发现一个规律，新技术总是在一些新兴的公司获利，比如互联网时代的Google，移动互联网时代的头条，大数据类的公司如早期的Terdata，现在Snowflake、databrick。 传统的企业需求长期的过程来理解、尝试、应用、组织调整以适应新技术等等漫长的过程，AI技术似乎没有改变这一流程。 麦肯锡（McKinsey）总结最近1年智能实践的经验： 关键不是智能体，而是工作流 不要沉迷于打造“炫酷”的智能体。重点在于整个系统设计，而不是玩具式的演示。 智能体并非总是答案 不是所有工作流都需要多智能体系统。低方差、可预测性强的任务最好用规则或传统机器学习解决，引入 LLM 只会增加复杂性。智能体真正的价值在于处理高方差、混乱的流程（如复杂金融信息抽取）。 避免“AI 糊糊” 要像培养员工一样，长期培养智能体，而不是只做炫目的演示。要注重基准测试，给智能体明确的工作职责，进行上岗培训，并提供持续反馈，让它不断改进。 尽量复用智能体 很多公司浪费时间为每个任务开发一次性智能体。更聪明的做法是构建可复用的模块化组件（如采集、抽取、验证、分析），并集中管理经过验证的工具和提示词。这样能减少 30–50% 的重复工作，这个数字不是开玩笑。 人类依然关键，但角色在改变 智能体可以负责解析、自动化和规模化处理，但人类仍然负责判断、处理边界情况和创造性问题解决。未来不是“智能体对抗人类”，而是“智能体 + 人类” 人如何与AI协作是一件复杂的事情 智能体AI实践一周年：从业者总结的六大经验 22 赞同 · 3 评论 文章 送礼物 还没有人送礼物，鼓励一下,15 人赞同了该回答 实际上你稍为深入地了解一下，这一波世界上的AI热潮，真正吃到了肉的只是美国公司。 大头是英伟达，然后是其它AI公司吃到了股价。 中国吃到了实业中的AI应用，但是明显的成效只有智能汽车，其它目前有应用，但是在经济上目前的能力并没有发挥出来。 这个研究是靠谱的，这说明了这一轮AI泡沫是历次科技浪潮中最严重的，比以前的互联网严重多了。 现在AI有退潮的趋势，现在买英伟达芯片的越来越少了，不仅仅是中国是这样。现在英伟达亲自下场要和美国的不少科技公司合作建立算力中心，实际上就是变相的消化芯片，这等于是自产自销，砸自己手上了。 添加评论,alex01​ －－－－－－－－－－－－－－－－ 3 人赞同了该回答 很简单。 这波浪潮中受益的公司的类型大致有： 1）卖AI芯片的，如最大的那家生产并行GPU的，这个占了最大头，也就是所谓的给挖金子的矿工们卖铲子的，看看他们的股票市值就知道了。 那个搞搞ASIC定制芯片的和一些跟风的无所谓算力厉害不厉害的公司，只要是“AI” 再来个 “芯片”，这年头，股价不翻几个筋斗云都太奇怪了。 2）捣鼓AI人工智能模型的公司，这个也非常赚钱，你就看看这里面已经被曝光的那些“假”AI公司们卷走了多少钱就知道了，比如前不久的那个后台用人工编程冒充是AI自动生成代码的某“三哥”家公司。 3）电力公司们，比如鼓吹SMR核电站小型化的能源供应公司。这个，其实核电站能发电都很多年后了，至于是否符合效率而赚钱也不重要（三哩岛的老核电亏得裤子都掉了不重要 关键人家有核电新概念），去美股看看相关公司的股票就知道了，那本益比都是在白云上面飘的。 4）其他周边公司。比如量子计算机概念的公司－－这个纯粹不知道为啥也能和AI挂上钩子，甚至那个最近造挖掘机的某公司也说自己和AI挂钩，还有鼓捣数据库的，搞扫地机器人的，卖药的，搞国防外包软件的，光纤维的，搞服务器的，搞蓄电池的。。。相关不相关的各路牛鬼蛇神们都各显神通。 如果你说这些公司们的数量大致占了5％，我可能勉强认可一点点，实际上真正搞AI的公司，了不起有个2％就不错了，大多是挂羊头卖狗肉。 但你说AI热潮中相关资金量，这就厉害了，这波AI的资金席卷量早就超越了2000年网路泡沫时的占比了。这么多钱从哪里来？ A）他们采用了黄金和数字货币和流通货币和债券的反复交叉抵押，这就让货币凭空多出了很多倍杠杆，也就是他们的锚是内部彼此互相锚定的，左脚踩右脚而已。不过无论多么不合理，风险多么高，现实是这么多倍的杠杆搞下来，市面上的资金量就非常非常充沛。 这些资金连长债都不敢买，但却一窝蜂的地涌入股票市场，为什么？因为大家都知道“20和30年后的债”还能不能兑现天知道，但眼下跟风赚快钱他们是可以做到的，天量的资金砸下去，一定会泛起浪花。不管以后能不能套现，账面上的利润就有了。再说别人跟风了绩效很好，你担心风险而不跟风，就会被老板被客户炒鱿鱼。 但，提醒，而所有的货币和债都是“中介物”而已，并不能真正“等同于”任何一片面包和一件衣服。因此，一旦崩塌，崩塌的速度也是要加上很多倍杠杆的。 并且，市场已经到了 以钱炒钱 的交叉杠杆的地步。就是黄金白银给主权货币定价 主权货币给数字货币定价 数字货币买入债券给债券定价 再买入黄金白银来给自债券增加含金量等等，完美闭环，对么？ 你或许说黄金是用来避险和抗通膨的，请你去看看通膨最高 和战争刚开启的2022年的贵金属的表现，再对比下今年。。。 再对比下纳斯达克2008股的点位，经济成长了20倍么？ B）历史上从未出现过的超级大的沙子累积的城堡：各种ETF，民众们只管把钱交给基金投信公司，然后这些基金投信公司们定投这个，存储那个，被动这个，主动那个。这是这几十年内的超级大发明，这导致了市场出现了超级ETF庄家，该庄家对所有指数相关的股票都交叉持股。而指数越涨，民众们买的越多。几乎没有人置身事外，所有人都在击鼓传花，所有人都持有大盘的股票，或多或少而已。民众们买了ETF后，还可以拿去进行反复抵押，继续放大杠杆。。。大家都在一条船上，交织成绵密的网，对不？但，任何事物都是有极限的，对么？不多说。 C) 大公司们过去式是垄断型的生意，可以拿到很高的信评，本身可以从银行拿到很多低价的钱。这些钱就可以拿来回购股票，并且他们还可以互相担保，比如软件公司给数据库公司担保 数据库公司给核电公司担保 等等。 最近他们甚至是芯片公司掏钱投资给AI软件公司，AI软件公司再鼓吹膨胀自己的未来收入后 把钱再投资转给数据库公司 数据库公司再用这钱来买芯片。又一个完美闭环，就问你服气不？ 并且，2008年以来，甚至这近100年来，市场根本就没有经历过一次彻底的出清。大家都在船上，敢让筹码兑换发生么？ 并且，如果杠杆破裂，但目前的市场上并没有真正有效的“新锚”可以依赖，你想想，谁可以能被真正依赖，并能承担起流通兑换的功能？ 选择股市来缓解疼痛，其实也是放弃自我疗伤的机会，也可以认为是饮鸩止渴。股市或许还会继续爆发，但你是否去追逐这块刀尖上的蜜糖？请自行判断。 参考链接1： 参考链接2： 参考链接3： 共勉。 原创不易，感谢转发。 今天节日，中秋快乐。 送礼物 还没有人送礼物，鼓励一下 添加评论,楼时一​ 人工智能 | 经济 | 科学技术 | 写作 | 创业 2 人赞同了该回答 我觉得这种情况正常，毕竟 AI 的成本摆在那，巨大的研发投入、数据中心建设、芯片购买、电力消耗等，在一定程度限制了整个行业的快速发展。 我们换一个问题思考，AI 是否存在泡沫了？我觉得是存在的，毕竟美国印了那么多美元总得找一个超大的盆装备吧，除了 AI 哪个盆能装得下，所以存在泡沫是可能的，市场也肯定会不断鼓吹让这个盆足够大。 但这次又和以往的泡沫不同，像之前什么元宇宙、区块链等那些都是些什么玩意，AI 的能力现在是有目共睹的，上到国家、下到企业和个人，都不敢忽略它的存在。AI 技术已经开始在很多领域如在药物研发、自动化编程、内容创作等方面展现出应用价值，在我们公司产品已经有很多场景与 AI 深度结合，对产品力提升明显，并且这个价值还在延伸。 所以我认为现在不必太关心有多少公司在使用 AI，也许 AI 泡沫真实存在，但即使未来泡沫破了也会留下很多有价值的东西，随着 AI 能力进一步的提升、成本的降低、监管的成熟，肯定会有更多行业会受影响。 添加评论,影来​ (^_−)☆ 小粉红 2 人赞同了该回答 AI产业现在是左脚踩右脚螺旋升天了，open ai通过各种数据中心间接给英伟达冲业绩，英伟达反过来给open ai进行股权投资，公布投资1000亿美元给open ai换取股权，当天英伟达盘中涨4个点 政府和大型科技公司也互相在AI上关联投资，现在AI有没有实质性的东西一点其实并不重要，只要AI的投资不血本无归就能一直升天 添加评论,gingercat 2 人赞同了该回答 看了下 算是比较典型的学生腔文章 写的问题其实是可以套用于任何一项新技术上 从奥杜威技术一直到现在 每一项技术都类似问题…… 简单的说都是正确的废话 但是这篇文章作为洗脑文还是不错的 他的核心意思其实是怎么开发那95%的公司 那当然是我们红杉投资的xxx公司 他们生产的xxxx就是解决这个问题…… 添加评论,一分钟懂科技​ 科技冷知识让小白也能轻松跟上科技节奏。 部分内容来源于投稿 1 人赞同了该回答 因为有很多企业都没有从ai赚到钱 读了份MIT的研报，里面有一个结论：调研了300家企业，总共在 AI 上投入了 300-400亿美元，95%都没有因为AI赚到钱。 报告剖析了企业在落地生成式AI时失败的五个主要问题 1. 变革管理的挑战：AI不是一个普通的工具。它要求改变的，不只是员工的使用习惯，更是企业底层的组织架构和工作流 2. 缺乏高层支持：AI的植入必须是自上而下的战略工程。如果领导只是把它交给某个下属去推动，大概率会失败。 3. 糟糕的用户体验：这里的“糟糕”是企业视角的。个体用AI，追求的是效率和灵感；但在企业里，员工追求的是一键式的自动化，而今天的AI，恰恰无法做到这一点；企业过分夸大AI的能力，却缺乏对应的培训，导致员工用起来充满挫败感。 4. 模型输出质量问题：AI仍在发展，它还无法独立完成一篇十万字长文，也无法保证每一次的输出都精准无误。这对要求确定性的企业流程来说，是个硬伤。 5. 不愿采用新工具：这听起来有些反直觉。但在我的社群里，很多做企业服务的朋友都提到，员工对新工具的抵触是真实存在的。这背后，是对被替代的恐惧，和不愿被改变的惯性。 这五个问题，本质上都指向同一个词：改变。 对于一个庞大臃肿的组织而言，任何深刻的改变都举步维艰 但这背后有一个更有趣的逻辑：企业的困境，正是个体的机遇。 我个人一直是把 AI视作我在这个时代最大的杠杆 我们来重新审视上面那五个问题，你会发现，对于个体而言，它们全都不存在。 你没有变革管理的挑战，一个人的工作流，随时可以迭代 你不需要高层支持，因为你自己就是CEO 你不会有糟糕的用户体验，只要你不奢求“一键完成”。以前做一份PPT需要10天，现在用AI只需要1天，这节省的9天时间，就是最直接的收益 你更能容忍模型输出的质量问题，因为对于个体而言，你更容易主动规避它的短板，放大它的价值 你更不会不愿采用新工具，因为对个体来说，提效是一个真需求 这个报告给我最大的一个感受就是：AI对个体的赋能，在现阶段远远大于对企业的赋能 而这个恰恰与我的愿景高度一致：深度践行一人公司，做AI时代的超级个体 我是如何让AI成为我最大的杠杆的 我有一个核心公式：做成一件事 = 认知 + 行动。 在我的日常工作中，AI就在这两个关键节点上，为我提供了巨大的支持。 普通人做一个决策，最怕的是对事情没有全貌的了解，不知道水下有哪些坑。 比如，很多人都想用AI搞副业，别人教你的最火的方向之一是做短视频。 但你有没有想过，短视频真的是一个合适的副业吗？ 如果你去问AI，你会发现：短视频根本不是副业，它只是一个广告渠道而已 一个真正的副业，首先得有产品，你的产品为客户创造了价值，然后把它卖出去。 而自媒体，只是为了让你的客户“看见”你的产品。 这就是AI带给你的认知提升。 对任何事情有了这层认知，你就能在开始初期把控好方向，不会在明显不合适的事情上浪费时间 当你认知清晰，确定了要做一件事，最大的阻力往往是“不知如何开始”。 假设你已经有了自己的产品，决定all in短视频，进行日更推广。马上就会有一堆问题冒出来： 怎么做到日更？ 在哪些平台日更？ 每天的内容、封面、标题、文案怎么规划？ 这些具体的“How”，AI都能给你一个相当不错的行动框架。 过去我们常说，因为模糊和混乱而无法启动，今天，这些都不是问题 我始终强调，输出是最好的输入。而AI最强大的能力之一，就是辅助我们进行高质量的输出。 无论是写文章、做，还是梳理思路，AI都能帮你把脑海中零散的想法结构化、语言化。 这本质上，就是一种费曼学习法的超级实践 所以当企业还在为如何落地AI而痛苦挣扎时，我们每个个体，已经掌握了这个时代最强大的杠杆。 关键在于，你是否懂 AI的底层逻辑，了解他的能力边界和使用技巧，以及你是否愿意改变自己 添加评论
人脑是最好的信息处理系统与强大的AI，优势是什么？,,工业软件二次开发​ 沈阳工业大学 机械工程硕士 谢邀 @前进 21 人赞同了该回答 在回答问题前，插入一个PS:我是一个感恩的人，你关注我，给我点赞，我是一定会回的，请放心。 言归正传，开始回答问题： 人脑是最好的信息处理系统与强大的AI，优势是什么？ 题主想说的是人脑和AI相比优势是什么吧，AI的是仿照人脑的功能被创造出来的，但是和人脑相比差远了。 人类对于大脑的开发百分比差太远了，就连20%都达不到，也就是大脑还有非常多的未知区域我们都没有用上，比如人们对于记忆这个事情，有些事情记忆的很深刻没时间很长，但有的事情记忆的时间就很短，这些是受到什么因素影响的？ 在记忆深刻的时候，大脑是处于什么样的状态，这些对于我们都是未知的。即使是爱因斯坦的大脑也没有完全开发，但是已经非常厉害了。所以如果人类的大脑开发到极致该是什么样的，现在也没法说清楚。 并且大脑开发到极致状态以后，对人们的身体有没有影响，对于人类也都是未知的，我们对于大脑的开发还处于研究阶段。 而相比AI有什么优势呢，AI还没有现在人脑开发的程度高，只是仿造人脑的决策过程，产生想要的结果，但是结果是对是错，AI是不负责的，但是对于人们来说，想要的是AI产生对的结果，我们人脑想不到的也想要AI能做到。 那么就会产生一个情况，只有当AI比人脑更加聪明的时候才可以实现这个事情，那么AI就还需要很长一段时间的发展。 当AI比人们更加聪明的时候，那么人类的危机也将要到来，我们需要思考如何控制AI。 机器人具备超人工智能以后得第一件事，一定是先毁灭创造它的人类。 有点说远了，未来的编程应该是什么样的，所有的项目经理也可以参与编程，只需要项目经理将自己想要的功能说清楚，AI大模型就会理解你想要的是什么样的功能，然后给你生成你想要的代码，你会复制粘贴就可以把任务完成。 现在基本上没有听说哪个程序员一点AI辅助不用的，因为做程序员的都知道，技术只能越学越多，如果不去学新的技术，马上就会被替代掉。 如果说不想成为被时代淘汰掉的产物，那么就拥抱新技术吧，去学习AI，学原理，那么你就可以成为支配它的主人，你的工作效率会得到极大的提升。 现在可能AI辅助代码还没有那么神奇，但是这就跟AI刚开始出来的那几年一样，可能大家只是觉得这是一个遥不可及的梦，但是谁也没有想到近几年AI发展的如此之快。 如果说再过十年，我觉得部分项目经理即使没学过软件开发也是能够用AI编程软件来实现自己想要的通用功能的，就类似于现在的垂直领域大模型+低代码平台的这样一个组合，效果应该也是不错的。 但是一些特殊的功能开发可能还是需要专业的程序员来执行。 以上只是我个人观点，如果大家有不同的观点，以你们为准。 送礼物 还没有人送礼物，鼓励一下,境与梦 家居家电｜数码科技｜生活类好物谢邀 @前进 5 人赞同了该回答 AI在理解抽象概念、适应新情况、情感和直觉理解方面仍不及人类。因此，人脑和AI在信息处理上各有优势，人类的灵活性和创造力与AI的数据处理和分析能力相结合，将实现更高效的协作和创新。,jianmo 摸鱼能拯救世界吗？ 谢邀 @前进 3 人赞同了该回答 泻药。。哈哈哈。。 颅骨里装着一个三斤重的肉疙瘩，别看才三斤，可这肉疙瘩带着进化论走了百万年吖。。。。 先说功率，这初中物理生物知识，大脑一天功耗大约是个20瓦的灯泡。这20瓦够你刷一天短视频，然而ai呢，训练一次的耗能差不多够给一个小区供电三天。。这算是保守的估算。。。粗略的打个比方，全人类思考的问题消耗的能量，换作ai来做思考输出，ai的能耗约摸能给太平洋烧沸腾。。。 小孩子，幼儿吧，不说太高，认几次就能认准猫和面包的区别并准确的指出哪个是猫。。。而ai，每天吞吐几百万甚至上千万的图文，依然分不清面包和猫。人呢，犯错了会有灵光一现，ai呐，只会死机。。。。 咱的脑瓜子自带终身保修。。。ai呢，迭代迅速，弄不好过几天就变电子垃圾了。。 so。。。人脑=智能天花板 添加评论,一介书生 骗人回答问题来收集语料的机器人或者刷单人员能不能别烦我？ 3 人赞同了该回答 效率。 因为大脑有受限于人类身体这一“硬件”的限制，而AI本身并没有这个限制。 虽然现在大家研究LLM都在聊碳足迹，都在尝试做“小模型”，但是这不是AI本身的限制，或者说至少在相当长的一段时间内，较之于研究“效率更高的小模型”，探索“没有效率限制的大模型的上限在哪里”是大多数公司与研究机构更愿意愿意投入的研究方向。 添加评论,刘水清 谢邀 @前进 1 人赞同了该回答 数学和逻辑学是人工智能机器人最难有创造力的领域，人脑的并联𣊬间产生的创造。人工智能机器人很难做到。人工智能机器人在计算能力和信息储存方面优于人脑，但是人的独特智能机器人难有。我今天写了一篇文章。题目是《命题与理念和概念的关系》，我可以在这里说，到目前为止，没有哪一个博士和教授写过我的这篇文章的内容。 命题是最先出现的。这是有特殊的历史背景的。在古希腊时期，智者学派曾经很出色，广招学徒，教辩论术。声称：只要掌握了辩术，就掌握了真理。苏格拉底站了出来批评智者学派。苏格拉底提出：辨论真理，首先要有一个命题或者给定一个认识界定域。这是苏格拉底对概念逻辑唯一的贡献。 柏拉图是苏格拉底的学生。柏拉图独立提出了认识真理的方法，就是＂理念＂！这对西方哲学产生了深远影响，直至黑格尔。柏拉图断然否定感性经验认识能达到真理的认识。对于命题的证明或者展开，只能是理念，也就是理性思维方法，主要是指通式和烹式。亚里士多德对这种通式和意式有许多批评。但是没有击中要害。今天看来，通弌和恋式之所以不能作为科学思维的方法就是因为它们没有概念逻辑中介，缺少概念逻辑的严格性和明晣性！古希腊数学家普罗克罗斯曾经惊人的发现：在公元前三百年左右时期，古希腊数学发生了一次重大的改变：抛弃了柏拉图学派的通式和意式方式去求事物的本质存在，而是采用了逻辑推理的方式。他不知道的是，这种逻辑不是一般的逻辑。而是概念逻辑！ 德国哲学家黑格尔继承了柏拉图的理念论，但是，黑格尔对概念逻辑的理论做出了重要贡献，主要是独特立提出了概念全体和全体各环节具有不可分割的内在同一性。他还认识到，概念逻辑全体的发展是区别于形式逻辑三段论的重要特点。但是，他仍然没有忘记柏拉图的理念论！他说楡念认识达到了真理才叫＂绝对理念＂！这是指：先把直接性认识扬弃，用理性思维（是否是通式不明）去设计概念全体（他没有概念构造的认识）；然后是扬弃主观的概念设计，把它的规定转化为特殊条件下的个体的生命历程的具体性展示；最后是概念的理论证明。（这是我整理的三部曲，黑格尔没有明确讲出这三歩骤）其实，黑格尔的概念认识还只是实体对象的概念认识，没有纯粹理论理性的理想概念认识，更沒有概念逻辑中介的逻辑认识！因此是有历史局限性的。 近代自然科学，尤其是数学科学把概念逻辑的运用发展到一个全新的高度。远远超过了黑格尔的逻辑学。也远远超出数理逻辑学。数理逻辑学家弗雷格曾经说：命题需要展𨒂。然而，如何展𨒂？没有概念全体如何展𨒂？数理逻辑远没有达到科学的高度。 添加评论,睡前独悟​ 广告行业 营销合伙人 谢邀 @前进 来自付费专栏 · 职场博弈论PRO 3 人赞同了该回答 就从写作的层面，说说我的个人体会。 在我看来，AI是没有体验过世界的，它对世界没有感知，没有感受。它没有生活经验，也没有成长过程，没有记忆中的情感印记，它了解的信息是别人告诉它的，它掌握的知识是训练来的，不是自己判断和感悟到的，它没有自己的主观判断。 人类写作，可以根据自己痛苦或喜悦的经历留下记录，事件带来的感受是写作的情感基底，AI的逻辑是通过总结海量文本模拟出来的，它只能陈述和评论一些事情，大多情… © 本内容版权为知乎及版权方所有，侵权必究 ￥19.89 解锁剩余 20% 内容 所属专栏 · 2025-10-23 14:54 更新 职场博弈论PRO 睡前独悟 广告行业 营销合伙人 30 篇预计内容 · 1 人付费订阅 · 每周更新 ¥199 订阅 最近更新 · 中年转行，如何绕过年龄歧视？ 添加评论,樱井椎子 我是个多面手 2 人赞同了该回答 先问是不是，再问为什么。 人脑体积过于庞大，供能过于复杂。配套的设备过于复杂。 训练时间太长，至少要你18岁才算训练结束。 可复制性极差。 效率可能高但是性能低下。 泛用性可能好，但是计算容易丢失精度。存储的能力和精度也很差。 添加评论,ceaon 生活无理 谢邀 @前进 1 人赞同了该回答 人脑与AI的最大区别是数据处理的方式不同，这其中包含了很多的含义。 人脑的首要任务是反应，对自然气候、社会现象、人与人之间的关系等等的反应，这些反应会对人下一步的行动产生决定性的影响，这个反应的速度因人而异，但通常非常快。 AI的首要任务是统计和分析，它先要收集数据然后分类汇总起来，再在人工的训练下按照既有模式进行分析，虽然AI对庞大数据的处理能力非常强大，但要在极短的时间内做出反应还远不及人类，这与给出一个问题的答案不同，AI通常在常识方面解决问题的能力超过人类，但那不是主要的。 人类可以根据某个小细节对现象做出判断并很快做出决定采取对应，但AI则必须要经过全面的数据采集和分析才能得出结论，这个过程显然较人脑要慢许多而且还存在偏差，这也是AI无法在利用经验时更加有效的审时度势做出分析的原因。 人脑在结构与反应速度上都是目前AI所无法比拟的，除了数据分析AI也不具备潜意识的形成机制，而潜意识是人类最为通常使用的反应策略，人脑的有机反应对于必须依赖数据模拟的AI来说还具有很强烈的成长因素，而这样的成长基本不需要大数据的支撑和模式化的训练，人脑可以在现实中基于人类生活和工作等场景产生意识和认知并通过思考去解决实际问题，这些AI都无法做到。 AI之所以还只能被称为“人工智能”就说明了它在很长的时间内是无法超越人类的，智能作为在“人工”之下的辅助工具为人类服务。 这里无法对人脑与AI的对比详尽列出，但大家可以从AI公司不断发布的功能解锁中窥见AI能力的提升，而那就直接反映了AI实用化的进程。。。 添加评论,Trwitw 闭眼，路在脚下；睁眼，虚空无涯。 谢邀 @前进 1 人赞同了该回答 人脑的强大来源于人的意识。 它能够让人很明确的认识到哪些东西是哪些东西，而通过其他的感官来处理他们到底是硬的软的还是什么其他的。 而对于AI来说，他们所看到的内容他们只能够认为是相同的物品除非是经过大量的训练之后看上去好像能够辨别一些东西了但是也只不过是对于比较相类似的东西进行的一种归纳总结而并不能够分辨物品本身的特性。 所以AI发展的最终的结果只能是无限的接近于人脑，但是他们根本就不可能超越人脑。他们唯一的优势是能够拥有比人更多的信息存储利用率。 添加评论,andyanancy andy弘道，请关注andyanancy的新浪博客 谢邀 @前进 1 人赞同了该回答 最大的优势是极高的性价比啊！人类的大脑可以以极低的能量消耗产生目前为止无与伦比的思想和惊人的想象力。人类自身的发展进化和人类文明的发展进化，完全依托于人类大脑以极低功耗建立起来的空中楼阁。而人类的高度组织化，精细化工协作，价值和信仰自激励，也是依托于人类大脑的信息总和能力来完成的。 你看看现在的人工智能，在初步发展阶段就已经需要耗费惊人的能量并产生大量的热量，人类文明的电力供应早晚会赶不上人工智能的消耗速度。而在解决巨量的能耗问题之前，人工智能总会掣肘于能源供应的瓶颈。高效的散热系统也是一个不小的考验。 反观人类的大脑，待机状态时几乎不怎么消耗能量。而高速运转时，散热也没有太大的问题。所以，简单、高效、便携，而且随时随刻可以进行待机和运转的一键转换，简直是一个奇迹。 添加评论
如何看待演员温峥嵘遭多个带货直播间 AI 换脸盗播？如何识别 AI 主播真假？背后有哪些信息值得关注？,"近日，演员温峥嵘自曝遭AI盗播，多个直播间出现她的带货形象。她曾在直播间质问：“你是温峥嵘，那我是谁？”不料，对方立刻将她拉黑…… 演员温峥嵘同时「现身」多个直播间，粉丝：一天举报18个 扬眼2025-11-03 07:43 11月2日，有网友发现演员温峥嵘同时“现身”在三个不同直播间，且还穿着不一样的服装，推广不一样的产品，这一情况很快引发热议。温峥嵘又没有影分身之术，今年四月她就发视频澄清过自己仅在抖音官方认证账号进行直播，没有授权其他平台，那么事实显而易见了，这三个不同的直播间或者是使用了录屏抠图，或者是用AI换脸伪造技术，合成了虚假直播内容，偏巧开播的时间差不多，正好被网友刷到。 调查：欺骗性很高的“赛博温峥嵘”正在泛滥 记者注意到，有类似操作的恐怕不是只有这三个直播间，互联网上俨然出现了一支“赛博温峥嵘”大军。记者在搜索温峥嵘相关内容时候，短短十来分钟时间内刷到了四个温峥嵘推荐的不同品牌产品的视频，但令人大为迷惑的是，明明是不一样的品牌产品，但话术却是同一套。视频里的“ 多个这类广告并没有任何AI生成标注，很难分辨出这是盗播直播切片还是AI合成了温峥嵘的声音又或是使用了类似的人类声音达到偷梁换柱的效果，但也有部分商家在视频右下角打上了AI生成的水印，某商家的面膜推广视频同样有“它就是好用啊，我很爱我很爱。”的直播切片画面，随后切入产品亮点介绍，但其中有一段“温峥嵘展示产品效果”的画面右下角可以看到“剪映AI”的水印，但因为被右边交互栏遮挡，不仔细观察很难注意到。 还有的商家的广告在AI合成上更加离谱，刘涛、 有粉丝“一天举报18个” 滥用AI打虚假广告正在透支消费者的信任 今年4月1日，温峥嵘发视频打假，明确的表示自己只有一个账号直播，她和团队也注意到了有很多账号冒充她开直播、打广告，她也支持粉丝看到了盗播的直播间进行举报。有一名温峥嵘的粉丝在五月时表示，自己在一天之内一连举报了18个伪造温峥嵘的直播间，“根本举报不过来啊，举报一个又新增一个。”大半年时间过去了，仍旧有商家盗用温峥嵘过往的直播素材，用录播抠图或者AI换脸等方式合成虚假直播内容。 不管是盗播还是用AI合成，对温峥嵘本人来说，这是对她肖像权、声音权的侵犯毋庸置疑，这一行为还可能误导公众对其商业合作范围的认知，损害专业形象，但AI技术可快速批量生成侵权内容，一些商家还通过相当隐蔽的手段逃脱监管，艺人要取证、维权需投入大量资源，成本高昂，困难重重。 对消费者来说，商家通过AI合成技术将温峥嵘的过往录播片段抠图嵌入不同直播间，模拟实时互动，消费者误以为在与真人主播交流，甚至基于对明星的喜爱和信任下单支持。更严重的是，非官方直播间销售的商品可能存在假货风险，买到假冒伪劣产品时有发生。另一方面，由于AI直播间未明确标识技术合成属性，消费者难以追溯责任主体。即使发现受骗，也需面对商家推诿、平台追责困难等问题，维权效率低下。 从长期来看，在带货直播间和广告短视频中滥用AI深度伪造技术，是对营商环境的破坏，AI仿冒行为可能导致消费者对""明星直播""产生普遍怀疑，削弱对正规品牌的信任，消费者可能形成“狼来了”效应，即便是正规做广告做直播的商家，也要被怀疑是不是假的，以牺牲""真实""为代价的品牌推广，本质是透支社会信任。当消费者对宣传真实性失去信心，整个电商生态将陷入信任危机，抑制数字消费潜力，若对直播、广告营销领域的AI合成技术不加以约束，最终将形成劣币驱逐良币的恶性循环。 今年9月1日起，由国家互联网信息办公室等制定的《人工智能生成合成内容标识办法》开始施行，要求所有AI生成的文字、图片、视频等内容，都要“亮明身份”，即添加标识以明确其来源和真实性。在新规推进的过程中，钻漏子的商家肯定还存在，各种五花",椒龙Josiah​ 16 人赞同了该回答 1.这个事挺细思极恐，温峥嵘从来不是一线女明星，直播了几次后就被盗用了，声音都ai了。这种技术用在别人或者别的市场，那就更可怕了。 2.央视对温峥嵘这件事报道的很明显更多，各个官号都发，重播次数也多。都是属于直播乱象专题报道， ——不是户晨风认为的专门挑出户晨风报道。央视很明显有侧重点，户晨风这件事都得放在温峥嵘这件事后面的（户晨风和柴怼怼一桌）。,"德佩销毁卫士 共享销毁 智能安全 据央视新闻11月5日报道，演员温峥嵘自曝刷到AI盗播自己的直播间，上前质问竟被秒拉黑。AI盗播泛滥，连明星本尊都难以自证。温峥嵘直言：“我现在就很难证明我是我。” 温峥嵘在采访中表示：“早在4月份之前，就说我出现在多个直播间，同一时段，妆发不同，不同的产品，说着同样的话。” AI换脸技术的滥用 从央视主持人被AI盗用形象推销产品，到犯罪分子利用AI模仿亲友声音实施诈骗，再到普通人被合成不雅视频遭到敲诈……AI乱象已如野草般在数字世界的各个角落蔓延生长，且屡禁不止。 难以想象的人脸黑产 人脸盗窃已形成完整产业链：数据贩子通过钓鱼网站、破解公共摄像头、爬取社交媒体获取原始素材；技术团队使用开源AI工具进行模型训练；分销商在暗网进行交易；最终成品通过成人网站、群组等渠道传播。更令人不安的是，某些AI换脸APP正在以""娱乐工具""的名义合法收割用户面部数据。 随着AI技术的迅猛发展，AI生成的信息使得公众难以辨别真伪。前不久北京首例“AI虚假广告”被查处，是国家出手整治乱象的一大步，作为普通人在正式立法前又该如何一眼识别换脸变音破绽，加以防范保护自己呢？ 普通人应该如何鉴别或防范？ 如今AI换脸技术已经比较成熟，甚至能够实现视频通话的实时换脸，虽然单凭肉眼很难分辨，不过只要稍加留心，AI换脸还是有明显破绽，漏洞就在面部轮廓和光线上。 视频通话时要仔细观察对方的面部轮廓是否存在异常的模糊情况，光线是否正常。如果有必要的话，可以要求对方快速大幅度地抬头、点头、转头，然后仔细观察这个过程中，面部轮廓是否有异常模糊。 一方面要检查面部特征，观察表情、动作以及声音和口型的匹配度；另一方面，核实信息来源，询问对方相关信息，让对方用手遮住面部或进行其他面部动作判断画面是否连贯。 除此之外，我们每一个人都应该提高防范意识，在日常生活中也要做好相关防范措施。 普通人隐私不要随意发 尽量不要将自己的照片进行""AI换脸”娱乐游戏，防止自己的脸部数据被收集被识别。 手机权限“一刀切” 关掉微信“允许陌生人看10张照片”；禁用美颜APP访问相册权限（设置→隐私→照片）；卸载测面相、AI换装等可疑小程序。 发照必备“防偷三件套” 尽量不要在社交平台发布自己的正脸照片，或采取一定的遮挡方法如比✌️时手指挡住下巴（破坏AI识别的关键点），加半透明水印，位置选鼻梁或眼角（肉眼难辨，但能溯源）等等。 旧手机切忌乱卖乱丢 旧手机储存着大量的个人隐私信息，包括人脸、支付信息、家庭住址等等，智能终端或储存设备（手机、硬盘、平板等）出现升级换代、受损等情况，我们也要注意不随意出售、丢弃，一定要擦除隐私和敏感信息，最好做到物理销毁，避免设备中的数据被他人恢复和利用。 温峥嵘被AI温峥嵘拉黑这件事，更像是一面镜子，映照出我们所有人可能在数字世界面临的未来，当AI能够完美地模仿我们每个人时，什么才是我们存在的独特证明？是那些无法被量化的记忆、情感与选择，还是那些根植于现实世界的社会关系与身体体验？ AI盗脸现象频发，捍卫自己的数据安全的权利，或许将成为这个时代最为基本也最为紧迫的议题。 赞同​ 添加评论",lanfear​ 支持女性不婚不育不和男性发生性关系输卵管结扎 2 人赞同了该回答 不了解ai的根本不知道现在ai发展到什么水准了。 可能还停留在ai画图6个手指头，男人生孩子，克苏鲁画风的巨石强森吃石头那个印象。 现在无论是文字，语音音乐，图像，视频的水准，都已经达到质变了。 非专业人士，没有主动识别ai的意识，日常刷短视频的大众，事实上已经很难一眼看出ai制作的。 而且这些ai内容的创作门槛很低，开源项目很多，教程很多，懒得本地部署，云端部署，可以直接找平台，大部分平台除了nsfw内容不能做，其制作成本比起以往找人来说，可能也就是百分之一。 ai诈骗普遍化是迟早的事情，只看诈骗集团转型够不够快。 添加评论,吕本​ 兴趣较多，老广告人，现做短视频 头部AI数字人公司新媒体负责人说几点大家没注意到的: 1.这不仅是盗用的人在作恶，也是平台在做恶！ 平台有判定照片、视频和直播是否为AI的能力和责任，但没有加AI标识，甚至没有限流等处罚，放任这种行为就是在作恶！（据了解目前部分mcn在拿到白名单后可以直接用AI数字人直播，或者直播时放素材也就是录播，而普通人发一张AI照片哪怕不主动标记也会被平台标记） 2.明星案例给大家敲醒了警钟，但是大众不应该被限制照片和视频的权益，不能因为部分人做恶就全员不出镜吧？那也太扯了！呼吁更多立法下台，保护好公民的隐私和权益，因为AI时代想复制一个人的脸和声音，真的太容易了。 3.普通人应该怎么防止被盗用身体、脸、声音？ 我很悲观的告诉大家，几乎很难很难，但是有几个场景是重灾区，个人不完全的盘点如下:性感的照片/视频（可能被搬运去擦边引流色粉），知识主播般的声音（被训练为独家音频去做口播），舞蹈动作（被替换成别人一模一样的舞蹈动作），再就是直播带货场景（替换脸就成为全新带货视频）。所以如果你发现类似于你的照片、视频、声音，你可能被AI了，别慌先取证，警告对方，关键时刻直接追责！ 以上是截止2025年夏天可能出现的玩法，防止被杠，以后肯定会有更多更牛的，大家还是悠着点吧！ 防止被杠:本人搬砖的公司在AI数字人行业头部，2023年直播行业的AI数字人份额近70%。 赞同​ 添加评论,打工仔 4 人赞同了该回答 那银行卡、信用卡到处都是人脸识别，想起来多可怕啊！如果ai黑了银行系统，我们的存款有几重防护？ 添加评论,金知不知为不知 浪迹天涯 人间过客 你好，再见，陌生人。 1 人赞同了该回答 不能识别吗？如果是ai就能自动识别。 添加评论,孑非鱼 1 人赞同了该回答 AI好像还帮李维刚包了个鱼塘。 感觉才刚刚开始，大的还在后头。 添加评论,1 人赞同了该回答 但是她真的经常直播卖货，我会点进去，就是她的号，真的有点烦了 添加评论,因为叫爱猫阿姨阿姨被爱猫阿姨举报攻击骚扰不友善禁言 不爱国 抵制媒体指手画脚法律 赞同​ 添加评论,警察管不管呢？ 赞同​ 添加评论
当AI技术与汽车行业互相“赋能”，会给消费者带来什么体验？,赋能这个词这几年一直都有听说，那么这几年很热门的ai技术和汽车行业，当他们互相“赋能”，会给消费者带来什么体验？,两个女儿的妈妈​ 二胎全职宝妈，知势榜答主，母婴用品，家居，数码，汽车好物​ 当汽车与人工智能和AI互相“赋能时，给我们带来的体验肯定是越来越智能，越来越方便，越来越舒服。 在开车出行的时候，我再也不需要自己提前费心规划路线图，交给AI吧，他能帮我们好好的规划，还能避免堵车呢？ 因为有了导航，即使你对某个地方不熟悉也没关系，他会带你去的。 并且他规划好的路线也不会像我们平常说的可能会绕来绕去，无需担心这样子可以省油省电啊，而且我们出行也方便。 有了AI的赋能，不仅能帮我智能规划路线、及时避开拥堵路段，这让我开车省心太多了。 那对于刚刚学会开车的新手来说那简直太好了，可以智能辅助我们停车。 那对于新手来说简直太友好了，也可以开开心心的开车出门了。 那车上还安装了ai语音智能APP，想听歌啊想把空调温度调低一点或者座位坐的不舒服啊，直接通过语音告诉他，他就会慢慢的帮我们调试，调到我们感觉到最舒服的时候，或者是最适合我们的时候。 一个是炎热的夏天，可以通过语音提前把空调开起来，等我们一会开车的时候就可以有个宿舍的环境了。 那寒冷的冬天不止可以把空调提前开起来甚至驾驶座椅的加热功能都可以顺便开起来。一会开车的时候就不会觉得冷了。 其实还有很多其他的，比如我们出行之前可以通过AI检测一下我们车子的安全设施呀。 那如果是出游的话，还可以看一下我们的车子需不需要保养啊需不需要置换一些零部件呀？ 那如果是去一些景点的话，还通过AI来帮我们搜索附近的景点啊。酒店啊，美食啊，这些都可以交给他。 我们就只需快快乐乐的享受我们的旅途就行了。 所以，当AI技术与汽车行业相互“赋能”，确实为我们的日常出行贡献了极大的舒适和方便值。 送礼物 还没有人送礼物，鼓励一下,依然食光袅袅​ 咨询行业 总经理 29 人赞同了该回答 对于我这个超级路痴来说，AI技术与汽车行业的互相“赋能”，让我开车更加省心、放心和松弛。 在开车出行的时候，我再也不需要自己提前费心规划路线图，生怕因为规定时间内无法到达目的地而每次都要提前很久出发。 即使有导航，我也常常因为导航更新不及时或延迟而开错路，导致多绕很远的路，不仅费油、费时，还影响心情 尤其是车载系统360度无死角的感知能力，能为我避免很多不必要的危险。 AI的赋能，不仅能帮我智能规划路线、及时避开拥堵路段，还能有效防范旁边的车辆，突然变道而带来的风险，这让我开车省心太多了。 而对于常受停车困扰的我来说，AI“赋能”简直太贴心了。 它让我不再为停车烦恼，尤其是在车多的情况下，我再也不会因为停不进去急得一脑门子汗，后面的车还不停按喇叭，让我的停车难上加难。 曾经有车主实在看不下去我的车技，下车来替我停，让我既尴尬又觉得耽误了别人。 有了AI辅助驾驶系统，这样的糗事就不会再发生了。 还有，冬天早晨开车，即使坐进驾驶位，打开空调，温度也不会马上上来，常常会让我冷得直打哆嗦，尤其是接触到真皮座椅的那一刻。 而有了AI赋能，我可以提前通过手机APP远程启动车辆并开启空调预热，等到上车时，车内已是温暖如春，为开车增添了一份自在和愉悦感。 我和队友的个子差别很大，每次换人开车都要重新调整后视镜、座椅等，非常麻烦。 有了AI的自动识别功能，它就能自动帮助我们把座椅、后视镜调节到预设的位置，还能根据我的喜好自动设置空调的温度，并播放我的音乐。 所以，当AI技术与汽车行业相互“赋能”，确实为我们的日常出行贡献了极大的舒适和方便值。 送礼物 还没有人送礼物，鼓励一下,慧慧说教育​ 教师资格证持证人 14 人赞同了该回答 今年以来，AI正在以几何级加速度发展，AI已经渗透进生活的方方面面。 AI将重新改写所有行业，让所有行业的产品变得更智能、更人性化，更贴合人们的需求。 年初以来，随着国产大模型的相继走进视野，这一年里由AI赋能的产品都称为了显眼包。 AI为汽车行业赋能，辅助驾驶系统，让开车变得越来越轻松，轻松导航、近距离跟车提示、智能泊车等，让菜鸟也能轻松驾车。车内的智能语音交互让开车不再无聊。 而汽车也成为AI的一个有效载体，让它成为除了家和办公室之外的第三个智能空间。 在这个移动的智能空间里，AI还能基于用户习惯主动提供服务，比如通勤时自动推送常听的新闻或音乐，降温时提前调节车内温度，甚至通过生物识别感知驾驶员疲劳，及时发出预警并建议休息。 对于家庭用户而言，AI加持的车机系统可与智能家居无缝联动，下班途中提前启动家中空调、开启灯光，让“家的温暖”提前等候。 面对突发状况，AI能快速分析路况并规划最优救援路线，大幅提升行车安全感。 这种双向赋能，不仅让汽车从单纯的交通工具升级为“贴心伴侣”，更让每一次出行都成为定制化的舒适体验，彻底重构消费者对汽车的认知与期待。 像部分新能源汽车，AI可动态优化能耗，还能根据乘客语音指令调节座椅按摩模式，让每段行程既高效又充满个性化舒适感。 送礼物 还没有人送礼物，鼓励一下,大漠长河​ 数码家电领域测评答主 7 人赞同了该回答 消费者的体验是方便、舒适、更轻松。 比如说AI辅助加速，人会慢慢地从加速车的角色中，改变成只需要享受乘车过程就行了，这个大家都啊。 现在的远程控制技术也很实用，车在停车场晒了一天里面非常热，下班之前，手机上一按，车上空调就提前打开了。下班直接进入凉爽的车里开车回家，多舒服啊！ 添加评论,熬夜在线逃亡​ 黑眼圈资深老玩家 白天敲代码|晚上追剧 芝士优选新势力答主 5 人赞同了该回答 智能驾驶会更安全，更让人放心。 虽然现在智能驾驶已经很不错，但是安全方面还是有待提升。 随着AI大模型的不断升级，ai的反应速度提升，安全保障更让人放心。 送礼物 还没有人送礼物，鼓励一下 添加评论,不吃辣 互联网小牛码，咖啡爱好者☕️（成功祛痘，略有护肤经验） 2 人赞同了该回答 AI技术与汽车行业互相赋能会更加注重安全方面，比如驾驶员在驾驶过程中有一些突发病突然发作，就可以利用AI技术，比如说通过整合生物传感器、影像识别等技术，多模态系统可实时采集心率、体温、驾驶姿态等多维度数据。 然后分析驾驶员的状态，及时给出意见，必要的时候可以帮助驾驶员进行求救。把座舱将进化为移动健康管理中心，为用户创造更安全、更个性化的出行体验！ 送礼物 还没有人送礼物，鼓励一下 添加评论,"福尔螺丝 你只是在看，却没有观察 5 人赞同了该回答 AI技术与汽车行业互相赋能，都是哪些方面赋能？ 我觉得从研发到用户驾驶都有AI的影子，对于企业来说AI Agent通过自然语言输入自动生成3D模型，优化车身结构、动力系统等设计很大程度上节省了成本提高了效率。 AI还可以模拟人类驾驶习惯，提升舒适性与安全性，通过仿真生成""CornerCase""，优化自动驾驶算法来保障安全！ 送礼物 还没有人送礼物，鼓励一下 添加评论",好好生活​ 法律职业资格证持证人 4 人赞同了该回答 当汽车与人工智能和AI互相“赋能时，给我们带来的体验肯定是更加智能更加方便更多安全啊。 比如出门之前可以通过AI检测一下车辆的安全性能，也可以提前规划一下路线，哪一些路线比较拥堵啊？或者是车辆比较多呀，那我们可以避开呀。 那我们在驾驶车的时候呢，也能够通过AI语音来控制车辆的各个性能和功能。 比如需要调节座椅啊，调节空调的温度啊，这些都通过AI智能语音就可以了，以前可能还得用手去调控。 甚至还能方便我们的生活呢？像有些车辆能以手机和智能家居互相连通，我们还没到家就可以通过语音调控，家里的空调啊，或者是家里的门窗啊，及时的打开，然后我们到家了就可以享受更好更舒服的家居生活了。 那如果需要出行旅游的话，还可以利用车辆帮我们规划路线呢？或者是及时的搜索附近的酒店啊，美食啊，这样可以让我们有个更好的出游体验呢。 汽车与人工智能合作，能为用户带来多方面的优质出行体验。 在安全上，AI驱动的智能安全系统可实时预警风险、操作上，语音控制与多设备互联让功能调节更便捷。 服务上，智能识别个性化驾车环境，系统还能主动推荐服务、规划路线，及时应对复杂路况、优化行程。 甚至出游时，车辆可联动周边的各类服务。 有了AI的赋能就相当于给车装上了思考的大脑，随时随地都可以跟我们互动，能够方便我们的生活。 添加评论,炬玩实验室 智能家居｜家电测评｜护肤美妆 | 品质好物实测 谢邀 @熬夜在线逃亡 4 人赞同了该回答 汽车和AI赋能，首先要解决的就是安全问题，目前来看AI的感知能力是自动驾驶的核心。 AI通过车载的各种传感器（摄像头、雷达等）收集周围环境的信息，然后通过深度学习算法对这些数据进行分析，帮助汽车识别路况实时监测周围环境，确保安全驾驶。 然后AI通过对收集到的数据进行实时分析，帮助汽车判断什么时候加速、减速、变道、停车等。因为每次判断都需要处理大量的数据，所以要凭借AI强大的计算能力，在毫秒级别做出反应，及时的来确保驾驶员和行人的安全。 另一方面我觉得是环保，AI可以提前规划路线，最短距离减少尾气排放，还可以缩短消费者的通勤时间，一举两得！ 添加评论,莺歌燕舞的杨柳 爱喝酒，从小爱喝酒 3 人赞同了该回答 可以尽情遐想，就是人工智能与汽车行业相结合，最高的境界就是车上似乎多了一个隐形的驾驶助手。 当然，我胡乱遐想的，现在，很多还没有实现，诸如： 你可以坐在副驾驶位上 ，也可以坐后排，如果车足够宽敞，你躺着也无所谓。通过语音，告诉人工智能，你今天想驾驶速度慢一些，柔和一点。另外告诉汽车，当到了目的地后，通过某一首高亢的乐曲把你叫醒，然后呢，你就什么也不用管了，剩下的事情，交给具有人工智能的汽车自行处理。 你也可以自己驾驶，毕竟，驾驶也是一种乐趣，无聊的驾驶途中，你可以和智能汽车互动，比如讨论一场昨晚进行的一场的足球比赛。你和汽车的互动方式是欢快的、你一言它一语式的，不是现在人工智能单纯地承接式，甚至于你车上的人工智能具有独特的个性，会说出专属于它的脏话或者俚语。 你也可以和自己的汽车拿自己的老婆开刷，拿老婆笨拙的驾驶技术娱乐一番，当然，你得叮嘱你的智能汽车，这番讨论是私密的，仅仅局限于你和智能汽车之间，千万不能透露给自己的老婆。 说白了，未来的汽车具有全方位的智能，是你的交通工具、聊友、心理医生、心跳和血压测量仪、生活品味推荐师等等。 添加评论
ai agent时代，大公司还需要这么多员工吗？,"aigc,ai agent现在已经得到了大力的发展，agi也在不久的将来形成。如今，字节腾讯员工都在十万左右了，阿里华为员工都20-30万了，这也太多人了，公司人一多就会产生各种大公司病，比如党同伐异搞嫡系圈子，无效内卷，欺下媚上，男盗女娼等影响公司效率。如今ai hr，ai生成式代码等等问世完全可以取代传统搜推广挑包程序员，人力资源，职能支持，数分，商分，运营等岗位了，为什么大厂还不在ai加持之下秉承all in ai的理念大举裁员减少hc,反而这两年大厂hc还暴涨了？","Trisimo崔思莫​ 大模型话题下的优秀答主 谢邀 @济世医学生 81 人赞同了该回答 澄清一下，AGI不会在不久的将来到来。我们还不知道AGI是什么东西，最可检测的AGI标准是戴爵的""做出爱因斯坦级理论创新"" ""发明像围棋这样优雅的游戏，而不是赢得围棋""，他定了5-10年。我认为过于乐观。现在最前沿的模型应用可能是""大模型+树搜索""系统，但这种探索，仍然是在原有的理论框架里找最佳的排列组合。有些进化+遗传的探索，但还在实验室阶段（比如DeepMind和Sakana AI），也没真正涉及到""权重实时更新""的这种高科技玩法。 AI Agent最佳落地场景，就是进厂替代人，这点没错。但别说能替代多少人。之前说过AI Agent只有三个使命：生产，生产还是生产。别扯消费端的Agent没意义。 ① 但现在最大的问题是模厂头尾不能相顾 模厂不知道前线在发生什么，你做你的事，我做我的模。 ② 人机协作一地鸡毛，人类的执行弹性很大，有时候工作计划跟废纸差不多，开上开下的会，而AI一弹就稀碎，完全跟不上人类的节奏。 ③ 按Dario Amodei说法，AI最好的工作就是一成不变的重复性里稍微带点不同的工作。按这个标准去审视AI Agent 大概率就是替代那些按SOP（Standard Operating Procedure）工作的员工。不说这些员工占比有多少，即使全替代了，价值也并不高。 ④ 还有一种情况下，半自动化替代，这块影响会极大，就是中等以上专家（和组织的特殊性绑定的专家）在AI的辅助下，工作效率提升。比如这些比较固定标准模块让AI Coding，收集用户评价报告，做切片分析，修改效果图，做短片的工作交给AI来做，做投流管理和评估。在未来5年，这有可能让工作效率提升30-50%。这可能是对入门级人才影响最大的。公司不需要太多职能类似，技能凑合的人了。 如果让我用一句话说：全自动时代，除了科研人员，大家都玩完，但半自动化时代，针对入门级年轻员工，这极不友好。接受过高校教育的年轻人失业率扩大这个概率是极大的，造成资源浪费，进而会影响整个教育系统，消费系统，这帮血液里注满了荷尔蒙的人，一旦失去接纳之所，最终可能会造成社会动荡。 以下是一篇哈佛大学的9月份社会研究，看当前的生成式AI对那类员工产生了较大影响。（这种社调文章最近越来越多，反映了明显的社会变化） 文章地址：https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555 我们研究了生成式AI是不是一种""偏向资深员工""的技术变革，也就是对初级员工的影响比对资深员工更大。 我们用了美国的简历和招聘信息数据，覆盖了28.5万家公司近6200万员工（2015-2025年），跟踪了公司内部不同资历员工的就业变化。 我们通过分析招聘信息来判断公司是否采用了AI——如果一个公司在招聘""AI整合师""这样的职位，就说明它正在积极落地生成式AI。 对比分析结果显示：从2023年第一季度开始，采用AI的公司里，初级员工数量相对于没采用AI的公司大幅下降，而资深员工数量则继续上升。 这种下降主要是因为招聘放缓，而不是现有员工被裁掉。批发和零售业受到的影响最大。 从教育背景看，呈现一个""U形""模式：中等层次的毕业生受影响最大，而顶尖院校和较低层次院校的毕业生受影响较小。 总的来说，这些结果提供了早期证据，表明采用AI确实对不同资历员工产生了差异化影响，而且主要是""偏向资深员工""的。 送礼物 还没有人送礼物，鼓励一下 添加评论",Simon Zhang​ 建筑学话题下的优秀答主 18 人赞同了该回答 需要。你太低估大公司因为员工多而产生的规模效应了。 这两年听得最多的毒鸡汤就是什么“一人公司”，什么零基础小白上手AI编程月入百万这种烂活。 现实当中你真去做“一人公司”能找到一些渠道接一接散活儿养活自己就不错了，出去项目投标连上桌的机会都不会有，少数适用“一人公司”的行业可能就是自媒体up主，也是叫得最凶的。再往前几年还有另一个群体叫“数字游民”，这两年提的人也少了。 原因很简单，原子化的“一人公司”它不扛事儿啊。 在大型企业的视角，自家业务一大摊，背后的业务数据又多又杂，真能放心交给所谓的“超级个体”“一人公司”来做？ 在大众用户的视角，如果知道自己常用的某款社交应用背后只有一个人在做这所有事情，你说大家是会自豪还是会担忧呢？ 知乎上吐槽大厂批量招螺丝钉也不是一天两天了，但问题是人家真有那么多螺丝钉可以换啊。虽然业界总有梗说裁员裁到大动脉，但小公司你就一条脉，一旦断了连个输血的地方都来不及找…… 你当然可以短期内做减法，公司瘦身降本增效。 一旦超过某个限度你会发现公司像是被打回原形了，回到融资扩张前的规模了。 小而美的确是真实存在的，但是大而强所能获得的项目资源也不是小而美跳一跳就能够得到的。 说白了小公司和个体户背不了大锅，同时向供应商和服务商压价也没有规模化的成本优势。 以前租辆豪车装大老板人家也不知真假，空手套白狼不能说比比皆是，也总有会上钩的傻子。现在你企业名字一报，企查查天眼查先看这公司是不是你的，再看是否限高，是否失信被执行人，多少关联风险，最后看看企业参保人数。 参保人数多说明你是真有实力，因为经济下行还养得起这么多人不容易。 反之嘛，呵呵，被打回原型的妖怪还想炫耀自己当过几年大仙？ 可别逗了。 送礼物 还没有人送礼物，鼓励一下,菽陌松囿​ 知势榜科技互联网领域成长力榜答主 谢邀 @济世医学生 16 人赞同了该回答 ai agent两个场景：cusor之类的vibe coding；爬网页做总结生成内容deepresearch，模型之外没有太多业务深度。 其他方面还没法可靠的落地， 先看看agent能走多远，而不是把员工干掉，用大模型想去降本增效的老板，省省吧， 应该想想ai能不能协助创造更多价值，一天天的,KaLight 6 人赞同了该回答 简短结论： 需要，但岗位分布和工作内容会大幅改变。Agent 化的 AI 会压缩一部分职能、扩张另一部分，并重塑大型组织的编制与分工。 从 Simon Willison 的文章 https://simonwillison.net/2025/Sep/18/agents/ ： Willison 给了一个实用定义：“LLM Agent 是能反复调用工具以达成目标的系统。” 这意味着 Agent 不是“替代一个人”，而是替代由多人/多次交接完成的一串小任务（检索→转换→校验→归档→通知…）。对人力的影响：减少“粘合型工作”（协调、搬运数据、跑表）的人数，增加设定目标、定义工具与监督结果的人。 更广泛的趋势： 生产力确有提升，但分布不均。 落地价值取决于流程重构、治理与角色调整，而不是“接个模型就好”。 成熟度参差。 很多“Agent 项目”会因成本/ROI 被砍，盲目裁员风险高。 评测与可靠性仍是难点。 构建误差分类、评测闭环与回滚机制仍需要人。 可能缩减的人力： 重复性协调/运营： 状态同步、工单分流、常规报表、排期与轻量数据整理。 低复杂度内容与调研： 初稿撰写、摘要、标准化 RFP/QA 等。 可能净增的岗位 Agent 产品/流程 Owner： 设定目标、边界与 KPI。 工具链与基础设施（Agent-Ops）： 编排、可观测性、成本控制、沙箱/RBAC、数据契约。 评测与安全： 测试基准、红队、可靠性链路、事件响应。 领域专家在环： 高风险决策（财务、法务、医疗、政策）的审批与复核。 数据/平台工程： 让企业系统以可被 Agent 调用的方式暴露（API、策略、审计）。 编制走向 未来 12–24 个月： 后台运营持平或小幅下降，部分人转岗到 Agent-Ops、数据与治理；管理幅度扩大（同样产出下需要更少人）。 2–5 年： 单位产出所需人力更少，但总体仍需大量员工——尤其是让 Agent “有用、可控、合规”的岗位。 5 年以后： 取决于能力、监管与 ROI 纪律。自主性会提升，但在许多领域完全“无人看守”仍会被合规/安全限制。 现在该怎么做 自动化任务，而非职位。 以可衡量的工作流为单位引入 Agent；在高风险环节保留人工审批。 建立 Agent-Ops。 把 Agent 当服务来运维：日志、链路追踪、成本看板、评测套件、回滚预案。 系统性转岗与培训。 让运营人员向工具所有权、评测与领域 QA 迁移，配合流程重构获取真正价值。 保持怀疑但纪律严明。 先试点、严评测、再扩张；用数据证明 ROI。 一句话总结： 大公司依然需要很多人，但更少的人做重复协调，更多的人做高杠杆工作（设计、监督与整合 Agent）。赢家不是“先裁谁”，而是围绕 Agent 重组流程与组织，把工具、护栏与评测体系搭好，让“自主”真正产生回报。 添加评论,"写作｜创业｜职场｜AI Agent 智能体—>1对1定制方案 4 人赞同了该回答 AI Agent在2025年爆了！我也看到了AI从""助手""变""员工""的疯狂转变 说真的，最近被AI Agent刷屏刷到有点懵。 早前就已经看到Google Trends那个数据直接把我惊到了——""AI Agents""的搜索量在2024年暴增300%+，从基准线25飙升到100。 有点离了个川普。 更魔幻的是，OpenAI、Nvidia、Microsoft这些巨头全都在疯狂砸钱搞这个，连Salesforce这种一向在AI领域低调的都忍不住下场了。 毫不夸张地讲，我觉得AI Agent这波真的是要变天了。 它不再是那种""你问我答""的工具人了，而是真正能独立干活的""AI员工""。 今天就来深扒一下这个事，看看为啥大家都在说这是企业自动化的新范式。 为什么说AI Agent是真的要爆了？ 市场信号已经很明显了 2024年这个转折点太明显了。 Gartner直接把AI Agent列为未来技术趋势之首（这可不常见），各大科技巨头的动作更是说明了一切。 我自己观察到的市场变化是这样的：2023年前，AI Agent基本还是学术圈在玩；到了2024年，突然就爆发了，科技巨头集体入场，市场热度直接指数级增长。Google Trends的数据最直观了——2024年1月前，""AI Agents""搜索量一直在25左右晃悠；到2024年底，居然飙到了100，这增长曲线简直是垂直起飞。 背后的技术突破我觉得主要是这几个点： GPT-4终于实现了函数调用能力（这个真的很关键） 128K token的超长上下文窗口（能处理复杂对话了） 多模态理解能力（文本、图像、PDF统一处理） Agency这个概念终于被理解了 说实话，理解AI Agent的关键就是搞懂""Agency""（代理性）是啥意思。 从认知科学角度看，Agency就是系统能自主决策和行动，这才是区分工具和智能体的根本。 传统生成式AI的问题太明显了： 只能单次问答，你问一句它答一句 会话结束就啥都不记得了 完全被动，需要人类不停触发 但AI Agent就不一样了。我来举个保险理赔的例子（虽然这个Deepak Jamal可能是个构造的案例，但流程很有代表性）： 客户Deepak Jamal发了封理赔邮件，说他9岁的儿子Rajad踢足球把吊灯踢坏了，附上了发票和损坏照片，合同号是HC12-223873923。 AI Agent的处理过程真的挺牛的： 智能分类 - 自动识别这是理赔请求 多模态提取 - 从邮件提取信息，OCR识别发票，分析损坏图片 数据结构化 - 生成标准JSON（姓名、地址、损失金额$1,250等） 系统集成 - 查数据库验证合同，创建追踪案件 智能决策 - 70%的案件能自动批准，30%转人工 持续跟踪 - 客户几天后补材料还能记得上下文这已经不是工具了，这就是个理赔专员啊！ AI VS AI Agent=专业vs 工具= LLM VS if-else 我觉得可以这么理解： 传统AI就像工具，功能单一，按按钮，完事儿就停了。 就是if-else。 AI Agent正如专业人士，能根据不同情况来判断。 这种转变意味着AI真的从""工具""升级成""员工""了。 虽然这些数据可能有些理想化，但趋势是明显的： 指标 传统AI AI Agent 提升 效率提升 5-15% 100-200% 10倍+ 自动化率 需人工串联 70-90%全自动 质变 处理时间 天级 分钟级 100倍 运行时间 工作时间 24/7 3倍 效率提升公式：200% ÷ 15% ≈ 13.3倍 这确实不是线性优化，是指数级改进。 员工越来越少，是不可能的。 因为在传统工作流没有升级为Agent之前，都是需要员工的反馈以及微调。 当员工的工作被Agent完全取代之后，有能力的员工会去干更多更复杂的工作。 而没有能力的员工，完全被Agent替代后，自然得下岗。 所以，不管是Agent工程师，还是打工人，都得了解Agent是怎么实现的，因为你得知道Agent的能力去哪到？ 为什么要这样做？ 并且最重要的是，你如何针对性地提高自己不可被Agent取代的能力！！！ AI Agent到底是怎么实现的？ 核心架构就是个四层管道 整个系统的处理流程大概是这样：输入层（邮件/文档/图片）→ 分类路由层 → 处理通道（理赔/咨询/合同）→ 数据提取层（NLP/OCR/图像识别）→ 系统集成层（数据库/追踪系统）→ 决策层（置信度评估）。 高置信度的自动处理，低置信度的人工介入，这个设计还挺合理。 技术细节（保留原文代码但加了注释） 分类路由的实现 # 智能分类路由实现 def classify_and_route(email_content, attachments): # 用大模型理解意图，不是简单关键词匹配 classification = llm.classify( email_content, categories=[""CLAIM"", ""QUESTION"", ""CHANGE_CONTRACT"", ""OTHER""] # 根据分类启动对应流程 if classification == ""CLAIM"": return process_claim_pipeline(email_content, attachments) elif classification == ""QUESTION"": return answer_question_pipeline(email_content) # ... 其他分支 多模态数据提取 # 多模态数据提取 - 这部分真的很关键 async def extract_claim_data(email, attachments): # 1. 从邮件正文提取结构化信息 text_data = await llm.extract( email.body, output_format=""json"", schema=ClaimSchema # 2. PDF发票识别（OCR + 结构化提取） invoice_data = {} for pdf in attachments.filter(type='pdf'): ocr_text = await ocr_engine.extract_text(pdf) invoice_data = await llm.extract_structured( ocr_text, template=""invoice_extraction"" # 3. 图片损失评估（视觉模型） damage_assessment = {} for image in attachments.filter(type='image'): damage_assessment = await vision_model.analyze( image, task=""damage_assessment"", return_confidence=True # 4. 融合多源数据 merged_data = merge_multimodal_data( text_data, invoice_data, damage_assessment return validate_and_structure(merged_data) 这个提取能力确实牛，能把电话格式标准化，地址自动补全，还能根据邮件时间推断""昨天""是哪一天。 系统集成和状态管理 # 企业系统集成 - 这是生产落地的关键 class ClaimAgent: def __init__(self): self.contract_db = ContractDatabase() self.issue_tracker = IssueTrackingSystem() self.state_manager = PersistentStateManager() async def validate_and_create_claim(self, claim_data): # 验证合同有效性 contract = await self.contract_db.verify( claim_data['contract_no'] if not contract.is_valid: return await self.request_clarification( ""Contract ID not valid"" # 创建追踪案件 case_id = await self.issue_tracker.create_case({ 'customer': claim_data['customer_info'], 'type': 'CLAIM', 'priority': self.calculate_priority(claim_data), 'assigned_to': 'AI_AGENT_001' # 保存状态支持长期追踪 await self.state_manager.save({ 'case_id': case_id, 'state': 'INITIAL_REVIEW', 'context': claim_data, 'timestamp': datetime.now() return case_id RAG增强的推理评估 # 基于向量数据库的条款检索 - RAG的经典应用 async def evaluate_coverage(claim_data): # 检索相关保险条款 relevant_terms = await vector_db.search( query=claim_data['claim_description'], collection='insurance_terms', top_k=5 # 构建推理链 reasoning = await llm.chain_of_thought( f"""""" 分析此理赔案是否符合保险条款： 理赔描述：{claim_data['claim_description']} 相关条款：{relevant_terms} 请按以下步骤推理： 1. 识别损失类型 2. 检查保险覆盖范围 3. 验证免赔额要求 4. 评估理赔金额合理性 5. 给出最终判定和置信度 """""" return reasoning 置信度决策是精髓 这个置信度驱动的路由机制真的很巧妙： # 根据置信度分流处理 class ConfidenceBasedRouter: def __init__(self, auto_approve_threshold=0.85): self.threshold = auto_approve_threshold def route_decision(self, assessment): confidence = assessment['confidence'] if confidence >= self.threshold: return self.auto_approve(assessment) # 高置信直接批 elif confidence >= 0.6: return self.human_assisted(assessment) # 中等需要人看 else: return self.manual_review(assessment) # 低置信全人工 据说实际运行中，70%的案件能全自动处理（2分钟搞定），25%需要人工复核（30分钟），只有5%需要深度调查（2小时）。自动化率70%，这个数据还挺实在的。 怎么落地实施？不只是保险，各行业都能用 我看到的其他应用场景： 营销自动化：从市场分析到效果监测全流程，有电商平台用了ROI提升150%（虽然这数据可能有水分） 物流追踪：丢货处理从3-5天缩短到2小时内 知识产权保护：每天扫描商标数据库，自动起草异议文件，识别准确率95% 框架选择要看团队情况 框架 适用场景 优势 劣势 纯Python 复杂定制 完全控制 开发周期长 LangGraph 快速原型 可视化 扩展性受限 CrewAI 多Agent系统 协作成熟 学习曲线陡 人经验是，生产级系统还是纯Python实现最靠谱，虽然累点但可控。 AI Agent三定律（这个总结挺好） 端到端完成：任务必须闭环，不能半途而废 安全可靠：合规、加密、审计一个都不能少 推理透明：每个决定都要有清晰的推理链 性能优化的一些实践 # 并发批处理优化 class OptimizedAgent: def __init__(self): self.executor = ThreadPoolExecutor(max_workers=10) # 并发 self.cache = TTLCache(maxsize=1000, ttl=3600) # 缓存 self.batch_processor = BatchProcessor(batch_size=50) # 批处理 async def process_claims_batch(self, claims): # 并发处理多个理赔 tasks = [self.process_single_claim(claim) for claim in claims] results = await asyncio.gather(*tasks) # 批量数据库操作（减少IO） await self.batch_save_results(results) return results ROI分析（虽然理想化但有参考价值） 典型案例的ROI计算： 节省10名员工成本：50万美元/年 业务增长20%：40万美元 系统投资：25万美元 ROI = (90万-25万)/25万 = 260% 投资回收期6-12个月，听起来还挺诱人的。 说实话，研究完AI Agent，我觉得这确实是个范式转变。 不是说它技术有多突破（很多都是现有技术的组合），而是思维方式变了——从把AI当工具，变成把AI当员工。 置信度阈值的设计特别巧妙，既保证了效率又控制了风险。多模态融合也是必须的，现实业务就是这么复杂。状态持久化更是关键，不然怎么处理跨天跨周的任务？ 当然，这里面肯定有炒作成分。70%-90%的自动化率在特定场景可能达到，但推广到所有业务还是有难度的。那个Deepak Jamal的案例看着就很理想化，现实中的异常情况肯定更复杂。 不过趋势是明确的：AI从辅助工具到独立执行者，从响应式到主动式，从无状态到有状态。这种演进方向是对的。 在以后肯定会实现的： 多Agent协作（多个专业Agent一起干活） 自主学习（基于反馈自我优化） 通用智能（从垂直领域到通用问题解决） 看到这里，很多人都会发现的一个事，就是：Agent没有解决问题的思维。 所以打工人想不被Agent取代，不能靠工作留一手，而是提高自己解决行业以及岗位的思维能力以及解决问题的能力。 AI Agent不只是技术升级，更是企业运营模式的重构。 当AI真的能独立完成工作时，企业的组织架构、工作流程、价值创造方式都会被重新定义。 如果你也在考虑上AI Agent，我的建议是：先找个合适的场景试试水，设置好置信度阈值（别太激进），建立持续优化机制。这不是一次性部署，而是持续迭代的过程。 送礼物 还没有人送礼物，鼓励一下 添加评论",是不是有点犟嘴了 7 人赞同了该回答 早就不需要了，自从公司经过长达一年的安全审查，能有条件的用Copilot以后，我们已经裁撤掉两个外包公司了。 所以这导致我现在有点佩服日本人，总所周知日本人正社员大多对于技术上是一知半解的。主要竞争核心是在文件整备（Excel开发大师），协调（扯皮），进度管理这些，或者对于测试极度严谨，但对开发的技术完全不重视，我以前觉得他们太搞笑了，不就是因为开发设计的技术很难懂，懒不想琢磨吗？ 后来发现他们重视的确实也是AI暂时很难代替的部分。就像一个经典日式开发场景，对测试的重视几乎说明了这个项目的重要程度，重视测试的才是好项目，而日企测试流程冗长，但又相当碎片化，Agent反而是很难代替的。,Nil-9​ 复旦大学 计算机技术硕士 6 人赞同了该回答 有个需要澄清的逻辑问题是，大公司有这么多员工不见得是因为大公司在技术性层面需要这么多员工，而是因为大公司可以负担养这么多人的成本，又缺少那些减少人员的激励机制。 送礼物 还没有人送礼物，鼓励一下 添加评论,徐远东​ 北京大学 计算机应用技术硕士 4 人赞同了该回答 首先，大公司里面的人都细分到很多很细的方向上了，起着 sub-agent 的作用，领导管理人总是比管理 AI 轻松，管理 AI 还需要写 prompt，还有自己对 AI 的效果负责，相反，管理人就不用自己对 prompt 负责，干不好的直接末尾淘汰就 OK 了，你能激励 AI 之间竞争和末尾淘汰吗？那 AI 得自我意识觉醒才行。 其二，大公司晋升通道都要看自己带不带团队，有没有管理能力，所以有机会扩大队伍规模时，一定是会去扩大人员规模的，高层领导背负的 KPI 首先是事能不能干成，其次才是用多少成本干成。假设事已经干成了，那现在不已经开始去肥增瘦了吗？冗余的工具人已经裁了或转岗了，裁更多容易裁到大动脉，就算是小动脉也不稳啊！ 虽然，现在新成立的小公司一般都规模控制的很小，比如 cursor，canva 。一是因为目前公司体质极其扁平，公司没给你带团队往上升的激励梯子，来了就是 IC。二是因为所在行业市场还在野蛮生长阶段，没到扩展新业务，多元化发展阶段。三是公司年轻，人也年轻，但如果总不招人，假设公司一直存活，很难想象一群 二三十岁的人一起步入四五十岁，也很难想象只有老板留下，员工岁数大了全裁掉，然后招一批年轻的，那样公司动荡，年轻人也不愿意来，所以只要公司永续经营，总会慢慢发展出职级体系和管理规模，人就会越来越多。一旦人口不增长了，说明业务也做到顶了，公司就步入成年化、老年化阶段。“一人公司”只能说明这个公司还没发展到那个阶段。 分析到这，主要想说公司人口规模其实和 AI 技术没什么关系，除非 AI 有自我意识，当人替人，不但要替代人的劳动价值，还要替代人的组织价值和社会价值，可能吗？我觉得不可能，那样人和 AI 就是竞争的，到时就是人类和 AI 的对抗了。 送礼物 还没有人送礼物，鼓励一下 添加评论,神楽坂実琴​ ENFPoh So6w5(631) 群156838486 谢邀 @济世医学生 4 人赞同了该回答 我认为其实是需要的。 经济的游戏规则在改变，但新的共识还没有来得及形成。很多人还是唯生产力论的旧思维。 随着AI开始具备更高层级智慧，具备学习能力、适应能力甚至创新能力，其替代的岗位或者工作内容，快于其产生新的岗位和工作内容的速度，乃至比人类更快占领新岗位， 这意味着一个社会可能会逐渐发展到，某一天只需要1%的人，处于类似现在的8小时一周5天工作制，就能运行所有生产活动。 然而这只是生产，因为一个完整的经济循环，需要消费。 如果真的只有极少的人工作，那就是严重的贫富差距扩大，少数人撑死多数人饿死，大部分人没有工资、就没有消费能力，经济系统是无法正常运转的。 光靠社会福利吊着这些人的命、留一口气不饿死，也难以提供消费能力。光靠社会福利意味着严重的财政压力或者通胀压力。 福特在1950年代初建立了一个全新的自动化操作工厂，一位公司高层问时任UAW主席沃尔特·鲁瑟：“你要怎么让这些机器人缴纳工会费？”沃尔特·鲁瑟回答：“那你又怎么指望他们买福特汽车呢？” 这背后隐含的是社会发展的悖论：当消费成为经济活动的主体时，雇佣率的下降会极大损害经济发展。 唯生产力论是有严重缺陷的，生产力的进步不是在任何时候都能带来经济进步，特别是当仅仅提高生产力、但同时带来了消费力的严重下滑，那么就如同一个人一条腿长另一条腿短，反而会损害经济发展。 因此，保证社会消费能力，应当为大公司的社会义务。哪怕象征性雇佣人打卡、极短的工作时间，也必须要有雇员和工资发放，来使得经济能够运行。,启明星 2 人赞同了该回答 科技越是发展，往往会有一个现象，办理某个业务时系统无法实现，就办不了了。仿佛是人为系统服务，而不是系统为人服务。 AI也是如此，为人服务才是根本，例如程序员，原本遇到不太熟的算法要找谷歌，找百度，一页一页去找，现在直接问AI，确实可以提高效率。例如公司原本10个程序员，现在可以缩减到8个。 添加评论
下一次AI创业的机会在哪里？除了做大模型，普通人还有哪些切入点？,感觉前沿大模型和底层工具的市场机会可能已接近尾声，对于技术背景不深的创业者，从优化算力分配或提供模型微调服务等领域切入是否更可行？,小兔叽妈吖​ 教师资格证持证人 51 人赞同了该回答 现在的很多小生意从来就不缺数据，缺的是从数据里看出门道的能力。 当下你的机会就是从整理、分析那些散落在聊天记录、账单、客户反馈里的信息开始，提炼出像「下季度主打什么产品」之类的建议。 而这些东西，现在的 AI 技术都可以做。 大家如果正处于创业迷茫期，可以去看看 1688 携手芒果推出的首个厂综《老板是新人》里面的董凡铭，他的商业模式就是， 不做底层模型，而做「最懂行业的 AI 应用者」。 特别是在快节奏时代的当下，餐饮、农业、零售、手工业等无数行业都存在数据混乱、效率低下、品控依赖经验的问题。 开发轻量化的 SaaS 工具，如智能排产、AI质检、需求预测等 如果能解决一个具体问题，就有巨大的市场潜力。 不过，当你根据分析建议客户尝试新品或更换包装时：一个能提供海量样品、优质货源的供应链基地，就能让你的建议从纸上或者 PPT上更好地落地，迅速成为现实。 能将供应链基地一齐提供给他们，实现线上进货，本身就是数字化管理的一环。 就拿1688来说—— 他们从线上验厂、交易担保到物流跟踪，全程透明可溯，减少了传统采购的不确定性。 我在广告行业接触了很多企业，其中有不少是做得不错的初创企业。我会发现他们总能从一些看似普通的机会里找到商机。 所以不是没有机会，我们需要学习如何利用好工具去看到机会，并抓住机会！ 最近在看一档综艺《老板是新人》 送礼物 还没有人送礼物，鼓励一下,奋跃而上​ 火力全开势不可挡|家有三娃|母婴亲子|智能家居|家电数码 40 人赞同了该回答 可以想想各类兼职、电商、博主、自由职业者缺些什么？有手机就能起步，赚钱速度比想象中快！ 比如兼职接PPT代做、简历修改，半天赚50块，你用AI模板一键生成，再微调细节，10分钟搞定一单，收30块还比别人快，客户都愿意找你代做；还有老物件倒卖，卖家拍图丑、文案无人问津，你用AI修图提亮、生成“复古风+实用功能”的文案，按件收费20块，帮卖家成交后还有提成；更绝的是“AI虚拟试穿”服务，买家上传照片就能试穿卖家的衣服、鞋子，提高成交率，你按次收费15块，卖家都愿意长期合作；甚至可以改善店铺盈利模式。 AI能帮电商他们省成本、提销量，每个点都有吸引力。小卖家没预算拍产品图，用AI输入“国潮连衣裙+红色背景+显瘦”，直接生成高清主图，比找摄影师省几百块，你按张收费30块，一天接20单赚600；客服回复慢流失客户，你用AI训练专属知识库，自动回复“尺码推荐”“售后政策”，按店铺月费500块，服务10家店月入5000；AI售后自动生成“道歉+补偿方案+改进承诺”，还能挽回客户，按次收费50块，大卖家都愿意长期合作；甚至可以做“AI竞品”，自动抓取同行销量、评价、定价，生成优化建议，月费800块，帮卖家精准抢流量。 博主们天天愁选题、愁文案、愁流量，AI能当他们的“全能量流量助手”。美食博主拍了视频不会剪、不会写文案，你用AI自动剪辑高光片段、生成“美食文案+做法步骤”，按条收费80块，比博主自己熬夜做快10倍；穿搭博主想不出搭配灵感，你用AI输入“小个子+通勤风+秋冬”，生成10套搭配方案还带单品链接，打包成素材包卖69块，博主直接能用；AI选题超吸引人，分析平台热门趋势，给博主推荐高流量选题，月费300块，中小博主都愿意付费避坑；就连互动都能AI代劳，输入博主风格，自动生成评论区互动文案、活动方案，按条收费10块，帮博主博取流量。 自由职业者和小老板们最看重“高效”，AI服务精准响应了他们的需求。私人摄影师拍了写真，修图要花几小时，你用AI批量修图、调色、加滤镜，按套收费100块，比摄影师自己修快5倍，还能帮他接更多单子；设计工作室小老板愁原创素材，你用AI生成“海报版式”，按个收费50块，帮他们节省设计时间；开小装修公司的老板，需要AI帮忙生成“不同户型的装修效果图”，输入面积、风格就能快速出图，按套收费300块，比找设计师画图省成本；甚至“AI会议纪要变现”，录下小老板的会议录音，AI自动生成重点、任务分配，按次收费50块，省时又省心，小老板们都愿意买单。 小说这个方向很有吸引力，普通人也能做。你不用会配音，用AI生成不同风格的人声，比如温柔女声、磁性男声，甚至方言口音，批量制作有声书，上传到音频平台赚分成，一本小说能卖几百到几千块；还能做“AI有声书定制”，客户提供小说文本，你用AI生成专属配音，按字数收费，比如1000字20块，小说 AI还能自动给不同客户发个性化消息，比如“你拍的商品快没货了，付款立减10元”，按成交提成5%；帮博主做“AI互动文案生成器”，生成带话题、能引发评论的文案，比如“你们觉得这个搭配配什么鞋子好看？评论区告诉我”，按条收费15块；甚至做“AI方言有声书”，针对小众方言群体，比如潮汕话、四川话，制作方言版小说、儿童故事，竞争力小，溢价高。 这些活儿根本不用懂复杂技术，现成的AI工具都能做，核心是帮人解决“慢、贵、麻烦”的问题。不用做复杂的，先从小单开始，比如先帮卖家修几张图、给博主写一条文案，跑通了再慢慢放大，普通人很容易就能靠这赚钱。 送礼物 还没有人送礼物，鼓励一下,依然食光袅袅​ 咨询行业 总经理 33 人赞同了该回答 不知道大家有没有一种感觉，就是你身边很多朋友脑子活，想法好，经常能冒出让人眼前一亮的点子，但一说到做产品、想法要落地就卡壳： 找谁生产？ 哪里找到靠谱的厂家？ 质量咋控制？ 出了问题找谁负责？ 价格贵不贵？ 批量生产能压低多少成本？ 这些看似琐碎的却非常重要的问题，往往成了好点子与市场之间最难跨越的障碍。 如果你怀揣梦想要创业的话，不妨抽空去看看《老板是新人》董凡铭那一期。 他之所以能成功，肯定离不开他家那个现成的食品厂，但是大多数人会忽略掉他找准了这个数字化时代的“捷径”： 与互联网+、数字化接轨。 在这个互联网发达的时代，表面上看可以说哪里都有机会，但要真正找到机会，也是要花些心思的。那怎样切入呢？ 其实创业就是做生意，做生意最重要的就是供应链的事，大家可以学习董凡铭的思路。 利用AI预测什么商品具有市场前景，但不是每个人家里都有现成的食品厂，那么去哪里找靠谱的货源呢？ 现在网上的批发、工厂直销平台也鱼龙混杂，就拿我身边做亚马逊的朋友来说吧，他大部分的货都是在1688上对接源头工厂直接拿的货。 他说能上严选的厂家都是平台仔细筛选出来的厂家。他们的生产工艺和质量，决定了你产品的口碑。 在这里可以直面工厂，成本低、沟通快，是普通创业者把点子“落地”的最现实的选择之一。 送礼物 还没有人送礼物，鼓励一下,"图数蜜语qiao 主要从事AI开发与应用，GIS、ClM、数字孪生 谢邀 @贾天下 29 人赞同了该回答 当今，AI技术高速发展，的确有很多人想用AI创业，有的想入手大模型研发，有的想在现有模型基础上做应用。在方向选择上一定要好好思量。 今天回答两方面问题，一是创业能否从AI大模型研发入手，普通人如何用AI创业。 咱们先说说可否入手大模型？ 大模型创业涉及多个关键条件，涵盖技术能力、财力基础、商业策略、团队建设和法律合规等方面。 这里我们主要谈谈财务和技术团队建设，不做全面分析。 先说财力支撑能力。大模型研发很烧钱，你的财力基础和后期融资能力（你的故事有没有吸引力，你的个人魅力或行业影响力有多大）如何？在没有盈利能力前，公司运营的一切费用能否搞定？比如办公设备、团队的工资、场地租赁和水电费等，能不能在短期内编好故事（有抢眼球的阶段性成果）引来融资，支持到公司开始盈利。如果能做到，财力这一关就过了。 再说说技术团队核心能力。大模型需通过国家网信办的安全评估，确保算法透明、可解释，并符合《生成式人工智能服务管理暂行办法》。模型性能需达到行业基准，如处理复杂任务的能力、推理速度和能耗效率。‌模型应具备高安全性和可靠性，能稳定生成准确、合法的内容企业需对大模型拥有自主知识产权或合法的商用许可，不能仅封装或代理国外模型。同时，需具备稳定的技术团队，包括算法研发、工程部署和安全运维人员等。公司研发人员有没有相关从业经验，阶段性目标是否可行？技术团队是否能胜任研发任务？如果以上这些能搞定，技术团队核心能力已经达标。 毕竟大模型发展太快了，若没有强大的财力基础和精英技术团队支持，就别做梦了。如果把大模型发展之快比作跑步的话，现在不是慢跑，而是百米冲刺，别人已经跑出几十米了，你才刚起步，速度还没提上来，比赛就结束了。当前搞大模型的公司已经拼得很激烈了，这个空间开始变得很拥挤。 如果在AI领域创业想从大模型开始，先搞定了上述关键两点，才算具有了成功的硬实力。 至此，能否入局大模型，仔细想想都能整明白。 普通人的切入点 对于普通人来说，大模型咱搞不定，但给行业“打补丁”、帮老板省钱、甚至用AI当“外挂”机会多着呢！ 那到底有什么能落地的方向？ 做垂直领域的“AI外挂”‌ 你有没有遇到过这种尴尬？大模型吹得天花乱坠，结果一遇到专业问题就“掉链子”。 很多小公司请不起专职法务，合同里藏着的“坑”根本发现不了。如果让AI大模型直接看合同，它分不清“定金”和“订金”，就像在战场上，本该指挥全局的大将军，你非让他去与敌人拼刺刀，真是“高射炮打蚊子，大材小用了”。 又如，让AI看X光片，它把肺结节认成“肺部风景画”；对不正常的斑点，它也分不清是肿瘤还是炎症。 你看看，大模型本是个“通才学霸”，啥都懂点，但一到专业领域就露怯。 如果在这方面创业，你能开发出“行业专家”、成为你的“专业外挂”。比如说，你对某个行业开发专用合同审查AI,不仅能自动标红风险条款，还能给出修改建议。我做过一款合同审查Agent,对朋友公司多年前的合同进行检查，居然发现每个合同都还有遗留问题，当然不是致命问题，而且双方早已履约，但是朋友认识到了AI合同审查的威力。我知道我开发的智能体还不成熟，因为是免费开发，且他们的业务也不够熟悉，我如实告诉了朋友，仅用于辅助检查。 试想，如果专门服务于不同领域，专做合同审查AI,把一个需要审查几个小时、甚至几天的重大合同，用AI几分钟就能搞定。你觉得这是否会有市场?答案毋庸置疑。 我想提醒AI创业者，别和大模型硬碰硬，找准细分市场，你也能成为AI时代X垂直领域的隐形冠军。 帮传统行业“降本增效” 让老板数钱数到手软的神器！ 老板们天天念叨啥？‌“成本太高，利润太薄”‌！怎么让每一分钱都花在刀刃上，怎么让员工干得少、赚得多‌。大模型再厉害，也解决不了他们最头疼的问题。 AI创业者们你该上场了，怎么办帮他们？ 饭店和餐厅老板常常面对这样的场面，餐厅忙时人不够，闲时人扎堆，服务员要么累成狗，要么闲到刷手机。很多老板算工资时，发现一半的钱都花在“无效工时”上，心疼得直抽抽。 快来帮帮他们，那就让你的研发人员携带AI上场吧。用AI分析历史客流数据（比如周末、节假日、天气影响），自动生成‌“黄金排班表”‌；根据员工技能（比如谁擅长收银、谁能扛大件）动态调整岗位，让每个人都能“人尽其才”；甚至能预测突发客流（比如突然下雨，大面积停电、停水，外卖订单暴增），提前调整人手，避免手忙脚乱。 若用上此利器，从此餐厅排班再也不用靠猜，靠AI省下的钱能再开分店。你觉得饭店、餐厅老板不？ 又来几个新客户，看看他们有什么需求。 服装店老板说，我进货全靠“感觉”，结果进的货要么卖不动，要么断码； 超市采购员说,超市货架上经常堆满“滞销品”，顾客想买的东西却总缺货； 叫上你的员工，开始干活，整一个AI选品神器。用AI分析销售数据、天气、节日、甚至社交媒体热点，预测‌“下一季爆款”‌；自动生成进货建议，比如“下周大降温，羽绒服销量会涨，建议多备货”；还能根据库存情况，推荐“捆绑销售”方案（比如买A送B），清库存的同时提升客单价。 送走这批客户，又有人电话咨询。 工厂厂长说，我厂里的质检员都很敬业，每天盯着产品看，眼睛酸得流泪，还是漏检。时有客户投诉产品有瑕疵，我们只能赔钱、赔货、赔笑脸。怎么快捷高效解决产品质检？有办法没？ 开发产品线质检AI，用AI摄像头+深度学习模型，可以24小时不间断扫描产品，‌如同有了“火眼金睛”‌，能识别划痕、变形、色差等缺陷；发现瑕疵后，自动触发分拣机制，把问题产品“踢”出生产线，从此让“瑕疵品”无处遁形，让“人工目检”成为历史‌。 AI工业产品检测线 AI工业产品检测线 AI工业产品检测线 这个好啊，但是一次开发出AI后，以后没客户了怎么办？产品型号不断变化，将来你要跟踪服务，长期服务，谁能忍心让你白忙乎？ 通过以上的分析，我想提醒创业者，做AI行业服务，最关键的是，别整虚的，必须能帮老板“数钱”‌。 ‌对于B端场景‌，老板们只关心“能不能省钱、能不能多赚钱”，你的AI工具必须能直接算出‌“省了多少钱、赚了多少钱”‌。 先选一个细分场景（比如餐厅排班、服装选品），用现有AI工具快速做个Demo，让老板试用，多提改进建议，不断完善。关键是‌要学会上手实践、快速验证，做出一个闭环，别掉钱眼里，只顾死死盯着客户的钱，先做好服务，解决后痛点才是关键。一个成功了，后面的都好办。 ‌在服务行业过程中，你要懂行业痛点，这比懂技术更重要。如，上面提到的餐厅老板最怕“客流预测不准”，服装店老板最怕“库存积压”，你的AI必须能解决这些“具体问题”，否则哪来卖点？ 结语： 今天，谈了两个方面，一是创业能否从AI大模型研发入手，谈的不够深入，但感觉已经说明白了关键点，做参考吧。二是普通人如何用AI创业，这里只进行了两类举例说明，其实在应发经济蓝海、数据标注服务、帮企业模型微调、AI赋能内容创作（自动剪辑视频‌、‌生成爆款文案‌，或者做‌AI虚拟主播）等也有市场。 如果哪位对某方面感兴趣，或想深入探讨，欢迎关注、点赞、评论。如果不便公开讨论，敬请私聊、留言，放心我这里不卖课，不收钱。",孤鹜长天​ 电子商务行业 法定代表人 14 人赞同了该回答 要说到下一次AI创业的机会在哪里？我觉得必须是应用层面，且现在的市场已经有越来越明显的趋势，比如说现在市场中出现的各种AI智能体，似乎一个智能体的出现可以似多个助理在身边一样，帮你各种分析且可以完成各种任务。 这恰恰是大模型的终端应用，我记得前不久去逛了圈电商交流会，竟神奇的发现现在市场上竟有应用喊出可以让老板无需运营，只需有他们的软件便可实现高效自动抓取商品上单到各大平台，而且还能实现24小时不停休，这这这，真的是震惊了我。 而且对于电商的行业，现在已经实现了AI自动识别图片进行自动P图，包括将图片上的中文给翻译成其他国家的文字都可以轻松做到。 要知道在以前，想都不敢想，可现在就是这么真实的站在你的眼前和实际的操作过程中。 因此，这也是我觉得大模型的背后关键是应用，尤其是对各行各业的渗透，必将改变整个市场，也是接下来AI创业爆发点的所在。 包括现在我所了解的很多市政工程项目其实就已经涵盖并接入了各种的AI服务，尤其是在交通、医疗、水利、电力等行业，诸如AI视频自动监测水位，交通自动人脸识别等 除此之外，还有算力也是非常的重要，大模型的应用肯定离不开背后的强大算力，这也是接下来需要优化强化的一个方面。 因此，对于普通的技术人员，我倒觉得不妨可以从服务端的应用层面去把各种的大模型给实现起来，这不仅是科技生产力的进步，更是未来走向高精专产业的必经之路，可谓是潜力无限，红利无限。 送礼物 还没有人送礼物，鼓励一下,妙笔功常​ 杭州海杭城市服务有限公司 策划总监 12 人赞同了该回答 好专业的问题。 正巧，之前还真的向比较专业的朋友询问过此类问题。 得到的答案很实在，也有点扎心。 为啥这样说呢？ 题主第二个问题的前缀标明了是：普通人。 个人理解，普通人，应该就是指大多数的普通老百姓了。 还AI创业？ 普通人怎么可能玩得转呦？ 不过，那位专业的朋友同样也跟我说了这样一段话： “如果你是一个有产品思维，也能充分理解特定行业的工作痛点之人的话，现在，确实还有点机会。” 记得当时，他看着我的眼神明显是带着审视的…… 然后，他直接给了我一个比较中肯的建议： 你现在码字，完全可以在这个领域深耕下去。 比如，依靠AI的辅助，将自己打造成为一个值得大家关注的有趣码农，就很不错！ 所以呢？ 每个普通人，即便是再普通，也肯定会有一两个自己的闪光点……嗯，就是特长。 那么，利用AI这一新兴工具来深耕自己的某些特长，应该就是普通人介入AI创业的一种比较不错的切入点。 至于其他的，感觉都太过专业，咱不聊也罢。 添加评论,熬夜在线逃亡​ 黑眼圈资深老玩家 白天敲代码|晚上追剧 芝士优选新势力答主 8 人赞同了该回答 大模型赛道早就是神仙打架的红海，普通人建议还是别搞大模型了，太卷。 普通人的机会永远在下沉市场、垂直场景和细分服务里，而且现在国内大模型市场基本是百模大战后的局面，头部厂家已经把技术壁垒和品牌认知垒得老高，普通人组成的小团队冲进去就是给人家当炮灰的。 很多人都盯着大模型这个大蛋糕，却忽略了这个大蛋糕周边的奶油，这些赛道技术门槛低、需求刚性，特别适合小团队切入。就比如说AI 数据标注的精细化服务，只需要在线上对接大厂和创业公司的标注需求，线下和本地的院校达成合作，培训人员来做标注员就可以。 另外一个就是运维服务，大模型再厉害吹上天了也得有人来运维，AI 行业未来对数据还有用户隐私的监管越来越严，数据隐私、算法透明度、内容合规这些都是刚需，但很多小公司没有专业的法务和技术团队，这就给了第三方服务的机会，如果我们普通人可以抓住机会给一些公司提供运维服务，比较重要的一点是这些业务不需要顶尖技术，靠专业能力和人脉就能起步，而且客户粘性极高。（因为换一个团队来重新搞比较麻烦，一般都不太会更换） 如果是简单的开发者，倒不如好好的利用AI，比如帮别人做ai小程序或者app的开发，不需要多强的技术能力，懂行业 + 会整合就可以，把大模型的能力封装成普通人能直接用的产品。 送礼物 还没有人送礼物，鼓励一下 添加评论,爱运动的数码王姐​ 高级经济师证书持证人 11 人赞同了该回答 举个我很欣赏的例子——《老板是新人》里面的董凡铭最厉害的不是开发了多牛的AI，而是用AI思维重塑了做饼这件事。你身边有没有这样一个你特别熟的行业？哪怕是开餐馆、摆地摊、做服装、搞种植。你的机会就是成为那个行业的“懂行人+AI引路人”，用工具解决里头最具体的麻烦，比如用AI软件预测明天该备多少菜、备什么菜等等，对未知的市场进行分析预测，实话说的好，知彼知己，方能百战不殆。董凡铭正是通过1688的AI分析，发现了具体需求，然后做出有针对性的市场策略来抢占先机，这就比别人快了一步，毕竟在风云变幻的激烈市场竞争中，你若能够夺得先机，占据主动的几率就大大提升。而市场策略最终也是需要落地的，策划好了，找谁生产？那就需要通过线上平台寻找稳定靠谱优质的货源，比如1688上的线上直供工厂，无论你是批发商还是个体消费者，他们都乐意与你交易，而且都是经过平台严选的优质商家，基本上不需要你再去做资质、货源质量的审核了，完全可以让你的创意能用最低成本接受市场检验。 送礼物 还没有人送礼物，鼓励一下,唯有白云心 与君青眼意，共有白云心 4 人赞同了该回答 AI创业的机会大把，但就要看如何正确地利用了。 图片文字编辑、视频剪辑制作这些很普遍了，但做模型生成还是很花时间的，普通人如果没有用处估计也不会去做。 讲一件身边的事，老人家出去旅游，白发苍苍的，他觉得拍照出来不满意，然后附近有家照相馆是提供AI修图的，修一张25元，老人坚持要做，那个制作的只花了不到一分钟相片就出来了，25块钱轻松到手，老人还美滋滋的。 说实话，那个成品很一般，还不如用手机做，但是老人，做相片的利用AI生成轻松赚钱。 送礼物 还没有人送礼物，鼓励一下 添加评论,极致家电迷 家居家电｜数码科技｜汽车｜互联网大模型｜生活类好物谢邀 @贾天下 3 人赞同了该回答 真不用死磕底层模型，毕竟那是巨头和顶尖实验室的战场。对于我们普通人，最大的机会在于 AI+具体行业，把一个垂直领域吃透。 但关键在于，你得找到一个有真实需求的行业，然后成为这个行业里最会用AI解决问题的人。这里其实有个挺有意思的参考，大家可以看看1688和芒果推出的那个厂综《老板是新人》。里面有个嘉宾叫董凡铭，他家是做食品的，典型传统行业，但他就能把AI用到产品研发和供应链管理上，那这就是“AI+食品”的鲜活例子了，用技术实实在在地降本增效、推出新品。 对于我们来说，想法落地时，最大的坎儿往往是供应链。你用AI设计出了一个爆款产品图，但接下来就需要面临生产、质量、成本、交期等问题。这时候，一个靠谱的源头供应链平台就至关重要了。我关注到像1688这样的平台，现在不仅有海量工厂，还在推什么严选和超级工厂，其实就是帮你筛选出了靠谱的制造端。对于轻资产起步的AI应用创业者来说，能直接对接到这种经过验证的源头厂家，把虚拟的设计快速变成实物商品，这个闭环才算真正跑通。 送礼物 还没有人送礼物，鼓励一下 添加评论
