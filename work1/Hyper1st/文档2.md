# 大语言模型训练流程

## 自监督学习

1. 从各种信息来源（爬虫等，主要是爬虫爬来的数据）获取庞大的训练数据；
1. 得到的数据（只筛去了少量的有害，无用的内容，基本没有人工干预）投喂给训练的模型，以确定模型参数的值；
1. 训练完成的模型回答问题的准确率并不高，因为它只学会了“文字接龙”，但不知道接的字正不正确。

## 监督式学习

1. 与自监督学习相似，该步骤也是投喂训练数据给模型，教会其"文字接龙"；
2. 但与之不同的是，该阶段投喂的数据是**人工精细筛选、标注**的数据，以保证训练后模型能更准确地输出答案；
3. 另一个不同点在于**角色标注**：即在投喂的数据中加入对对话角色的标注，以帮助模型对不同对话情况进行区分；
4. 为防止第二阶段对之前第一阶段已产生的参数修改过多，引入了**适配器**，如LoRA，这可以保证原先的参数不变，只生成少量的新参数（来源于第二阶段的训练）；
5. 以监督式学习训练模型有两种方向：打造通才和专才。无论哪种方向，在经过监督式学习后，模型的准确率都能得到显著提升；

## 人类反馈强化学习

1. 虽然监督式学习提高了模型的准确率，但其输出未必符合人类的偏好，于是就需要进行**人类反馈强化学习**；
2. 这一阶段和前两个阶段有显著不同：前两个阶段属于是“文字接龙”，是单纯的训练模型“预测下一个词”的能力，而这一阶段则是训练模型要知道“什么答案是更好的”；
3. 该阶段的训练，不再关注训练的中间过程，只关注模型的输出结果：如果结果符合人类的偏好，则给予正向反馈，反之亦然；
4. 该阶段为了能进行大规模的训练，引入了**奖励模型**：一个收集了大量人类偏好，能给训练模型输出打分的模型；
5. 但奖励模型也有局限：奖励模型本身并非人类，它协助训练得出的模型可能会过度迎合奖励模型的偏好而非人类的偏好。为了结决此类问题，业界也在探索RLHF的别的新方法。