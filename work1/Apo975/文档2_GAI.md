# 大语言模型训练过程

### **第一阶段：自监督学习（Pre-training）**

主要任务是让机器根据提供的数据进行**自我训练**，做文字接龙，让语言模型掌握两类知识：**语言知识**和**世界知识**

该过程不需要人工介入，所以称为**自监督学习**

训练的数据主要来自于**网络**（需要对不适合的数据进行适当清理）

经过第一阶段训练的模型只是拥有了知识，但还不会使用



### **第二阶段：监督式学习（Supervised Fine-tuning）**

在第一阶段语言模型学会大量知识的基础上，让**人类指导**语言模型，使其**学会使用**知识回答问题，本质上还是对**文字接龙**的训练

需要将数据整理成AI文字接龙的格式（需要明确标注哪部分是用户的发言，哪部分是AI的发言），构建这种数据要花费大量人力，被称为**数据标注**，用这种数据集来训练模型的过程叫**指令微调**

指令微调的方向有两种：一是打造精通一个任务的**专才**，二是打造能解决多种任务的**通才**

这种通过人工标注数据训练模型的方法叫做**监督式学习**

使用**适配器**技巧（保持原有参数不变，新增少量参数进行训练）可以减小模型最终参数与原始参数的差异



### **第三阶段：人类反馈强化学习（RLHF）**

让模型开始与**用户**进行互动，在实战中不断优化自生能力，这个阶段被称为**人类反馈强化学习**

**人类反馈**即用户在使用过程中的各类反馈

此过程中，模型训练的重点不再是局部的文字接龙，而是优化**答案整体质量**，只关注结果，不关注过程，这种训练方式属于**强化学习**

当人类指出某个答案的好坏时，模型就会通过“**近端策略优化**”对自己的参数进行微调



**奖励模型**可以模仿人类偏好，它由人类偏好数据训练而成，既可以对语言模型的各种答案进行评分，又可以在生成过程中评分，动态调整语言模型的生成策略。

但这种方式也有**局限性**：奖励模型不能完全模拟人类，同时人类也无法判断某些问题答案的好坏
