# 第一阶段：自监督学习（Pre-training）
这个阶段可以看作是让模型学习海量的文本数据。模型的任务很简单，就是不断地预测一段文字中下一个会出现的词是什么。通过无数次这样的练习，模型逐渐学会了词汇之间的关联、语法规则和一些基本的事实知识，形成了一个具有基础语言能力的“基座模型”。这个阶段消耗的计算资源和数据量都是最大的。

# 第二阶段：监督式微调（Supervised Fine-tuning）
经过第一阶段，模型虽然有了知识，但还不懂得如何像一个对话助手那样听从人类的指令来回答问题。它可能会自顾自地长篇大论，或者答非所问。监督微调的目的就是教会模型“听话”。

我们会准备一个由人工精心编写的数据集，里面包含许多“指令-答案”对。然后，我们用这些高质量的数据集去进一步训练基座模型。模型通过这个过程学习到，当接收到人类的某个指令时，应该生成什么样格式和内容的回答才是符合期待的。

# 第三阶段：基于人类反馈的强化学习（RLHF）
即使经过了监督微调，模型的回答可能仍然不够理想数据集。RLHF的目标就是进一步精细化调整模型，让它的回答更符合人类的偏好和价值观。

这个过程通常分为三步：

## 收集人类反馈：
让模型对同一个问题生成多个不同的答案，然后由人类标注员对这些答案的质量进行排序，指出哪个更好，哪个较差。

## 训练奖励模型：
利用上一步得到的人类排序数据，训练一个专门的、用来打分的模型（奖励模型）。这个奖励模型的学习目标是：当输入一个问题和模型给出的答案时，它能打出与人类偏好一致的分数，即好的答案得分高，差的答案得分低。

## 强化学习微调：
现在，让第二阶段得到的模型（演员模型）去生成答案，同时由训练好的奖励模型为它生成的答案打分。模型的目标是尽可能获得高分，它会根据得分情况不断微调自己内部的参数。为了防止模型为了高分而“钻空子”（比如生成一些毫无意义但恰好符合奖励模型偏好的内容），通常会加入一个约束，要求模型的输出不能偏离第二阶段模型太远。通过这种方式，模型在奖励模型的引导下，逐渐学会生成更受人类欢迎的回答。
