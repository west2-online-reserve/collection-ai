# <center>AI笔记</center>
## 一.自监督学习（Pre-training）
自监督学习（Pre-training）的核心是不用人工标注数据，让模型从海量无标签数据中，通过设计“自导自演”的学习任务（如填空、预测下一句），来学习数据内部的通用规律，最终学到通用的特征表示，为后续的具体任务打下基础。
## 二.监督式学习（Supervised Fine-tuning）
监督式学习（Supervised Fine-tuning）的核心是用带 “标准答案” 的标注数据对模型进行进一步的训练。即将一批由人类精心编写的、高质量的“提问-回答”数据（有标签数据）“喂”给模型，让模型微调它内部的参数，使模型的行为与我们的意图“对齐”。模型的学习目标从“预测下一个词”（填空）变成了“生成符合人类偏好和特定任务要求的回答”，避免答非所问。
## 三.人类反馈强化学习（RLHF）
人类反馈强化学习（RLHF）是一种结合强化学习与人类主观评价的 AI 训练技术。核心思想是把人类的 "喜欢" 和 "不喜欢" 转化为可计算的“分数”，让 AI 模型通过不断尝试，依据“分数评价”调整参数，生成更符合人类价值观的内容。
### 训练过程
1.完成监督式学习（Supervised Fine-tuning）
2.构建奖励模型（RM）：模型对同一个问题生成多个不同的回答（比如A、B、C、D），然后由人类对这些回答的质量进行排序（例如，D > B > A > C）。利用这些排序数据，我们可以训练一个“奖励模型”。这个模型的作用就像一个“评分器”，能自动给新回答打分。
3.强化学习优化（PPO 算法）：模型生成多个回答，奖励模型给出分数。模型为实现“高分”，不断调整参数，实现从有限标注数据中归纳出通用规则


