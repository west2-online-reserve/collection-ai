## 背景知识：文字接龙



#### 文字接龙



​                                            <img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110205456132.png" alt="image-20251110205456132" style="zoom: 33%;" />                                 



#### 机器怎么学会做文字接龙？ 



​                                          <img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110205539347.png" alt="image-20251110205539347" style="zoom:33%;" />                             



过程一：通过训练资料，机器学习可以把数十亿个参数找出来，这个过程叫做训练，training（学习，learning）

过程二：使用这个模型来得到下一个token，这个过程叫做测试，testing（推论，inference）



<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110205724631.png" alt="image-20251110205724631" style="zoom:33%;" />



大语言模型训练的三个阶段都是在学文字接龙，只是训练资料不同



#### 找参数的挑战

##### 挑战一



<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110212750666.png" alt="image-20251110212750666" style="zoom:33%;" />



找参数的过程叫最佳化（Optimization），超参数（hyperparameter）决定了最佳化的方式

挑战一：训练可能失败，即找到的参数没有符合训练资料

怎么办？换一组超参数再上一次

 

##### 挑战二



<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110213115421.png" alt="image-20251110213115421" style="zoom:33%;" />



挑战二：训练成功了，但测试失败（Overfitting）



#### 如何让机器找到比较合理的参数



​                                            <img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110213241713.png" alt="image-20251110213241713" style="zoom:33%;" />             

<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110213512776.png" alt="image-20251110213512776" style="zoom:33%;" />



如何让机器找到比较合理的参数？

增加训练资料的多样性

找一组比较好的初始参数，这个好的参数就叫先验知识



## 大语言模型训练的第一阶段 - Pre-train



<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110214151342.png" alt="image-20251110214151342" style="zoom:33%;" />

学会文字接龙，需要语言知识和世界知识



<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110214409767.png" alt="image-20251110214409767" style="zoom:33%;" />

<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110215017632.png" alt="image-20251110215017632" style="zoom:33%;" />

从网络上可以获得大量的文字资料，从而获得大量的训练资料，这个过程人工介入很少，这个过程就叫做自督导式学习



<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110215635080.png" alt="image-20251110215635080" style="zoom:33%;" />

<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110215850695.png" alt="image-20251110215850695" style="zoom:33%;" />

<img src="C:\Users\17278\AppData\Roaming\Typora\typora-user-images\image-20251110220835499.png" alt="image-20251110220835499" style="zoom:33%;" />

第一阶段，语言模型学了很多东西，但不知道使用方法



## 大语言模型训练第二阶段 - Instruction Fine-tuning



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113110228946.png" alt="image-20251113110228946" style="zoom:33%;" />

耗费大量人力产生训练资料的过程，叫做资料标注

通过这个过程训练模型，叫做督导式学习(Supervised Learning)



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113112121869.png" alt="image-20251113112121869" style="zoom:33%;" />

将第一阶段找出的参数作为第二阶段的初始参数（Pre-train，预训练）

第二阶段找出的参数不会和第一阶段的参数差太多（Instruction Fine-tuning，微调）



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113112515817.png" alt="image-20251113112515817" style="zoom:33%;" />

如果想第二阶段找到的参数和第一阶段非常接近，就可以不要动第一阶段找到的参数，在第二阶段再增加少量参数，第二阶段找到这些少量参数就行，这叫做Adapter



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113113409187.png" alt="image-20251113113409187" style="zoom:33%;" />

<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113113720267.png" alt="image-20251113113720267" style="zoom: 33%;" />

## 大语言模型训练第三阶段 - RLHF

Reinforcement Learning from Human Feedback(RLHF)



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113161556902.png" alt="image-20251113161556902" style="zoom:33%;" />

<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113162315107.png" alt="image-20251113162315107" style="zoom:33%;" />

通过人类回馈来学习的这种方法，叫做增强式学习（Reinforcement Learning）

通过微调参数，提高人类觉得好的答案的生成概率，降低人类觉得不好的答案的生成概率



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113162644369.png" alt="image-20251113162644369" style="zoom:33%;" />

从人类生成资料的角度看：

Instruction Fine-tuning需要人类自己想出问题和答案

RLHF只需要人类判断生成答案的好坏

<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113163034301.png" alt="image-20251113163034301" style="zoom:33%;" />

从模型学习的角度看：

Instruction Fine-tuning只问过程，不问结果

RLHF只问结果，不问过程



<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113164232060.png" alt="image-20251113164232060" style="zoom:33%;" />

<img src="C:\Users\17278\Desktop\projects\collection-ai\work1\Ning-creation\生成式人工智慧.assets\image-20251113164506700.png" alt="image-20251113164506700" style="zoom:33%;" />

用人类的喜好训练出回馈模型，之后机器就可以直接和回馈模型学习了

过度的和回馈模型学习，不一定提高模型的表现