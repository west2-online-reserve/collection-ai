---
date: 2025-10-22
tags:
  - 作业
  - python
  - West2-OnLineAI
  - GAI
---
## 前言
- 由于近几年AI的 **爆炸式** 的发展，我们就有必要去学习一下AI的知识
- 而对于 ~~GAI~~ 来说，==目的是让机器产生复杂且有结构的结果== 

## 大语言模型训练[^1]
[^1]: 这里就只进行非常粗略的介绍并且本文档的内容大部分取自 **李宏毅生成式人工智能教程LeeGenAI Tutoria** 
[github链接](https://kkgithub.com/datawhalechina/leegenai-tutorial.git)
### 第一步 预训练（pretrain）

- 训练语言模型的第一阶段，是机器自学做“文字接龙”。语言模型要正确地生成下一个词元，需要两类知识：**语言知识和世界知识** 。语言知识包括语法结构的理解，例如“这个人突然就”之后可以接“跑”或“飞”，但不能接“的”，接“的”不符合语法。
- 所有对于模型来说，要想训练它需要极其大量且丰富的数据
- 而这些数据最主要的来源就是 **网络** 。通过爬虫程序，可以抓取大量网页上的文字信息，构造成训练数据
### 第二步  指令微调（instructionfinetuning）

- 语言模型修炼的第一阶段，其通过自我学习累积了丰富的能力，但仅拥有强大的“内功”却不知道如何将其应用。因此在第二阶段，**我们需要让语言模型接受人类教师的指导，发挥其真正的潜力**
- 语言模型如何接受人类教师的教导？
- 人类教师需要 **为语言模型准备教材** 。这些教材的结构是：先思考人类可能会向语言模型提出的问题，并为每一个问题准备一个正确的答案。然后，将这些问题与答案整理成适合语言模型训练的“文字接龙”格式
### 第三步  RLHF
- 即让模型开始与用户进行互动，并在实战中不断优化自身能力。这个阶段被称为 **人类反馈强化学习**（Reinforcement Learning from HumanFeedback，RLHF）
- 在这个阶段，模型不再直接预测下一个词，而是学习“哪个答案更好“
- 强化学习的核心在于优化输出结果的整体质量。人类指出哪个答案好，模型就调整参数，提升产生好答案的概率，降低差答案的概率。在 **RLHF** 中，模型的参数微调是通过一种叫做 ==近端策略优化（Proximal Policy Optimization，PPO）== 的方法完成的