{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def5aa31",
   "metadata": {},
   "source": [
    "1.自监督学习（Pre-training\n",
    "\n",
    "​目标：​​ 让模型学会“语言”本身，打下坚实的知识基础。\n",
    "​过程：​​ 给模型喂食海量的、来自互联网的文本数据（如书籍、网页、文章等）。训练任务很简单：​预测下一个词。比如，给模型一句话“今天天气很好，我们去…”，模型的任务就是猜出最可能的下一个词，比如“公园”。\n",
    "​结果：​​ 通过这个“完形填空”式的训练，模型不仅学会了语法、句法，还掌握了海量的事实知识、逻辑推理能力和文风。这时它就像一个“饱读诗书但未经世事的大学毕业生”，知识渊博，但还不知道如何与人有效协作，回答可能冗长、机械或不安全。我们得到的这个模型通常被称为 ​基座模型。\n",
    "\n",
    "第二阶段：监督式微调（Supervised Fine-Tuning, SFT）\n",
    "\n",
    "​目标：​​ 教会模型如何按照人类的指令和偏好来回答问题，让它从一个“知识库”变成一个“助手”。\n",
    "​过程：​​ 雇佣人类标注员，编写大量高质量的“指令-回答”对（例如，问：“用莎士比亚的风格写一首关于咖啡的诗”，并提供相应的优秀回答）。然后用这些精心准备的数据去微调第一阶段得到的基座模型。\n",
    "​结果：​​ 模型学会了“听话”，理解了人类指令的意图，并开始模仿人类助手的回答风格。它的回答变得更有用、更相关。这时它就像一个“刚结束岗前培训的新员工”，知道该怎么做事了，但水平还不够稳定，有时可能还是会给出奇怪或不恰当的答案。\n",
    "\n",
    "第三阶段：人类反馈强化学习（RLHF）\n",
    "\n",
    "​目标：​​ 进一步打磨模型，让它的回答不仅正确，还要更安全、无害、符合人类价值观和偏好​（比如，更简洁、更乐于助人）。\n",
    "​过程：​​ 这个阶段最复杂，可以简化为两步：\n",
    "\n",
    "​训练一个“偏好模型”：​​ 让人类标注员对同一个问题的多个不同回答进行排序（比如，哪个回答更好/更差）。用这些排序数据训练出一个能判断回答好坏的“裁判模型”。\n",
    "​用强化学习微调助手：​​ 让第二阶段的SFT模型（“助手”）针对问题生成回答，然后由第一步训练的“裁判模型”给这个回答打分（好或坏）。这个分数作为奖励信号，通过强化学习算法（如PPO）来进一步微调“助手”模型，鼓励它生成能获得高分的回答。\n",
    "\n",
    "\n",
    "​结果：​​ 经过这个“优胜劣汰”的打磨过程，模型的回答质量、安全性和有用性都得到了显著提升。它变得更像一个“经验丰富、处事圆滑的优秀员工”，懂得如何提供最令人满意的服务。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
