- 第一阶段，预训练。对于大语言的学习，主要是**文字接龙**，总体是在说基于一个Transformer（模型）下，通过寻找参数training的过程，再进行一次testing（推论，inference），在预训练中通过足够的训练资料来给大语言提供**举一反三**的能力，就可以让大模型有两条路线，“专才”和“通才”。
- 在基于一堆训练资料中找（寻找）参数，训练前也要设置**初始参数**（设定初始参数越“好”得到的参数越接近初始化参数），使得最终找的参数与初始参数接近，再设定（人设定）超参数来寻找方法，来实现**最佳化**。
- 训练资料一定要足够的多样化，才能使得机器学习过程中不会出现**overfitting**，机器学习时只会管找到的参数是否符合训练资料。
> 自督导式学习
- 在机器学习过程中通过网络等资料来源可以不断进行爬取资料学习，不必人的过多的介入。
> 资料清理
- 在爬取资料学习中，需要有用且“好的”资料，要进行**资料清理**，如过滤有害内容（黄赌毒），去除HTML项目符号或者低品质的资料重复资料等...
- 在第一阶段的学习之中，机器学习只能在从中爬取不一样的数据进行找到参数进行学习，但没有学习的方法，会出现答非所问或者用问句来回答问句。
---
- 第二阶段。提供Instruction Fine-tuning。**资料标注**也就是**督导式学习**(Supervised learning)。人类提供标注的少量资料学习，使用**Adapter**，在之后的最佳化的过程中只增加了少量参数（和初始参数保持比较接近，减少训练计算量）。在训练大语言模型的过程中，可以通过渠道来获取用户所问的问题来提供更为高质量的训练资料（chatgpt训练）
- 也可以对chatgpt等大型语言模型做逆向工程，让chatgpt想一些可能的输入与答案，来作为自己训练的资料来做instruction fine-tuning的资料。
---
- 第三阶段。RLHF增强式学习（Reinforcement Learning）。通过微调语言模型的参数来提高“好答案”出现的概率。只问结果不问过程。让语言模型提问人群来评判语言模型的答案好坏，好的概率不断提高，坏的不断降低。
- 通过**回馈模型**来模仿人类喜好（虚拟人类），回答多个答案，通过回馈模型给答案打分。（但是过度依靠虚拟人类学习有害）
- 当然增强式学习也会遇到不同的难题，比如说对于“好”的评判标准，例如火药的制作上，若回答“我不能教你”这在安全性上是好答案，却在帮助人上还是坏的答案。