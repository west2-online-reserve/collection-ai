> 其实感觉现在听到的所有都只是很片面的，真要系统的学完并讲述对于现在的我还是困难，只能以自己的说法大致讲一讲。
- 简单来说，大语言模型的训练就像让一台机器读无数书并学会写话：首先把句子拆成小块（“词片段”，像拼图块），然后用一种叫Transformer的结构（可以理解为一种会“注意”不同词之间关系的读书方式）让模型预测下一块或填空，模型里有很多可调的“旋钮”（参数），每次预测错误都会计算出一个“错多少”（损失），训练就像教练根据错多少回头调整旋钮（反向传播和梯度下降），不断重复直到错误变小；因为数据和旋钮都很多，需要很多电脑一起训练（分布式训练），训练完成后可以用少量新数据微调到特定任务，或者用人类评分来教它更符合人类期望的回答（RLHF）。